{"cells":[{"cell_type":"markdown","id":"o2PCim9NBE0Q","metadata":{"id":"o2PCim9NBE0Q"},"source":["## [HW4] Hush, the seagulls are purring: Autoregressive Language Modeleling\n","\n","CS 4740 + crosslists, Spring 2025\n","\n","\u003cbr/\u003e\n","\n","\u003cdiv align=\"center\"\u003e\n","    \u003cimg src=\"https://media.newyorker.com/cartoons/6438b7e6a22e87ce2f4d36d5/master/w_1280,c_limit/230424_a27699.jpg\" width=\"500\"/\u003e\n","    \u003cbr/\u003e\n","    Source: \u003ca href=\"https://www.newyorker.com/cartoons/issue-cartoons/cartoons-from-the-april-24-and-may-1-2023-issue\"\u003e\n","\"\u003ci\u003eTo think this all began with letting autocomplete finish our sentences.\u003c/i\u003e\" by Robert Leighton\u003c/a\u003e\n","    \u003cbr/\u003e\n","    (Licensed from the New Yorker Cartoon Bank.)\n","\u003c/div\u003e\n","\n","\u003cbr/\u003e\n","\n","No part (code, documentation, comments, etc.) of this notebook or any assignment-related artefacts were generated/created, refined, or modified using generative AI tools such as ChatGPT. Cite this notebook as:\n","\u003e Tushaar Gangavarapu, Darren Key\u003csup\u003e\u0026#129433;\u003c/sup\u003e, Logan Kraver\u003csup\u003e\u0026#129433;\u003c/sup\u003e, Lionel Tan\u003csup\u003e\u0026#129433;\u003c/sup\u003e, Pun Chaixanien\u003csup\u003e\u0026#129436;\u003c/sup\u003e, Kai Horstmann\u003csup\u003e\u0026#129436;\u003c/sup\u003e, Dave Jung\u003csup\u003e\u0026#129436;\u003c/sup\u003e, Aaishi Uppuluri\u003csup\u003e\u0026#129436;\u003c/sup\u003e. 2023. [CS 4740 Fa'23 HW4] Hush, the seagulls are purring: On generating humorous captions from scene descriptions. GitHub. \u003cbr/\u003e\n","\u003e \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003csup\u003e\u0026#129433;\u003c/sup\u003eequal contribution, software creators, ordered alphabetically\u003cbr/\u003e\n","\u003e \u0026nbsp;\u0026nbsp;\u0026nbsp;\u0026nbsp;\u003csup\u003e\u0026#129436;\u003c/sup\u003eequal contribution, software testers, ordered alphabetically\u003cbr/\u003e\n"]},{"cell_type":"markdown","id":"8c67f7b6-1992-41d1-a747-b9c9504b4102","metadata":{"id":"8c67f7b6-1992-41d1-a747-b9c9504b4102"},"source":["---\n","\n","\n","__\u003cfont color=\"red\"\u003eSpecial Instructions for this homework\u003c/font\u003e__: This assignments builds on top of the code functionalities you already implemented in HW3. Many of the files provided for this homework are identical to those from HW3 but contain `raise NotImplementedError` in key functions. This is not a bug! These placeholders are intentional, and we expect you to copy over **your own working implementations from HW3** into these sections to complete HW4.\n","\n","\n"," \u003cfont color=\"red\"\u003e**The milestone submission for this homework checks that all your relevant code functionalities from HW3 work as intended.**\u003c/font\u003e You do not implement anything extra for the milestone. We will provide you code for the specific functions that are incorrect in your implementation.\n","\n","**How will the milestone submission work?**\n","\n","*  You will follow the instructions in this ipynb to create your milestone submission. Submit it to the milestone submission site ``hw4-milestone`` on gradescope.\n","* On Thursday, April 24th 11.59 p.m., we will make the autograder results visible to you. You will be able to see which, if any, of your function implementations are incorrect. Note that this time is chosen to be after the last possible to submit to ``hw3-programming``.\n","* You can comment (either anonymously or not) on this Ed post (https://edstem.org/us/courses/73802/discussion/6571140) to request correct code for *only* your incorrectly implemented functions. Our course staff will email the correct code to you. There is no penalty for hw4 associated with this.\n","\n","Note:\n","* Parts 1-4 of this assignment can be completed without relying on any code implemented in HW3. Don't wait for Thursday to start your assignment!\n","* ``hw4-milestone`` does not check that *all* your code from HW3 is correct, only the relevant functionalities needed for this HW."]},{"cell_type":"markdown","id":"1d96bd27","metadata":{"id":"1d96bd27"},"source":["__Deadlines__\n","\n","* Milestone deadline: \u003cfont color=\"red\"\u003eApril 28, 2025\u003c/font\u003e (Monday), 11.59pm on the submission site(s).\n","* Assignment submission deadline: \u003cfont color=\"red\"\u003eMay 6, 2025\u003c/font\u003e (Tuesday), 11.59pm on the submission site(s). We will accept submissions with no penalty upto \u003cfont color=\"red\"\u003eMay 8, 2025\u003c/font\u003e, 11.59pm. You are allowed to use your remaining slip days (maximum of 2) after this point. The last day you may submit this assignment is \u003cfont color=\"red\"\u003eMay 10, 2025\u003c/font\u003e.\n","\n","\n","__Policies.__ All the policies described on the course website are applicable as is (including the policy on academic integrity and the use of generative AI tools)."]},{"cell_type":"markdown","id":"614c9d2c-cd32-48bc-ae49-c0b4ab5aee0f","metadata":{"id":"614c9d2c-cd32-48bc-ae49-c0b4ab5aee0f"},"source":["---\n","\n","\u003ca name=\"outline\"\u003e\u003c/a\u003e__Assignment outline__\n","\n","* [[$\\ast$] Attributions](#attr)\n","* [[0] Imports and installs!](#sec0)\n","* [[1] Assignment overview](#sec1)\n","* [[2] Milestone submission](#sec2)\n","* [[3] Data processing, tokenization, padding](#sec3)\n","* [[4] Sampling Tokens given Logits](#sec4)\n","* [[5] Finetuning the Seagull to \"understand\" humor](#sec5)\n","* [[6] Model Evaluation](#sec6)\n","* [[7] Final Submission!](#sec7)"]},{"cell_type":"markdown","id":"c33c83ae-b60f-47bc-ade0-edc3c2a2c2b4","metadata":{"id":"c33c83ae-b60f-47bc-ade0-edc3c2a2c2b4"},"source":["---\n","\u003ca name=\"attr\"\u003e\u003c/a\u003e\n","### [$\\ast$] Attributions [↩︎](#outline)\n","Please use the space provided below to acknowledge (by name/source) all help you received (this includes generative AI tools); you're welcome to cite references in line when answering the \"written\" questions. If you wish to submit answers to the written questions in a separate file, please be sure to note your attributions there.\n"]},{"cell_type":"markdown","id":"o1onuJowYyBk","metadata":{"id":"o1onuJowYyBk"},"source":["_Attributions (if any) go here._"]},{"cell_type":"code","execution_count":1,"id":"F0nPQJb2KLAU","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":3642,"status":"ok","timestamp":1746583240593,"user":{"displayName":"Aileen Huang","userId":"01843079669161655601"},"user_tz":240},"id":"F0nPQJb2KLAU","outputId":"9a8ac553-547b-4ff5-bc65-05872c507cb8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/', force_remount = True)"]},{"cell_type":"markdown","id":"7202fd73-d1ea-4c7a-a136-a9e75c6bc0ab","metadata":{"id":"7202fd73-d1ea-4c7a-a136-a9e75c6bc0ab"},"source":["---\n","\u003ca name=\"sec0\"\u003e\u003c/a\u003e\n","### [0] Imports and installs! [↩︎](#outline)\n","\n","Run the following code to install any external libraries and needed packages to run HW3 assignment. Before proceeding, be sure to run the second code cell to ensure that the installation is successful.\n","\n"]},{"cell_type":"code","execution_count":2,"id":"397e2602-d003-4528-8223-04a00ddbeb8a","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":3307,"status":"ok","timestamp":1746583243902,"user":{"displayName":"Aileen Huang","userId":"01843079669161655601"},"user_tz":240},"id":"397e2602-d003-4528-8223-04a00ddbeb8a","outputId":"b4bae74c-b88c-42ba-95da-3bd1c4078c46"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/cs4740_programming_assignments/hw4-release\n","Requirement already satisfied: numpy~=1.25.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (1.25.2)\n","Requirement already satisfied: torch~=2.0.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (2.0.1)\n","Requirement already satisfied: torchinfo~=1.8.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (1.8.0)\n","Requirement already satisfied: setuptools==65.6.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (65.6.3)\n","Requirement already satisfied: prettytable~=3.8.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (3.8.0)\n","Requirement already satisfied: datasets~=2.14.4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (2.14.7)\n","Requirement already satisfied: PyYAML~=6.0.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 7)) (6.0.2)\n","Requirement already satisfied: urllib3==1.26.15 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 8)) (1.26.15)\n","Requirement already satisfied: notebook~=6.5.5 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (6.5.7)\n","Requirement already satisfied: jupyter~=1.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (1.0.0)\n","Requirement already satisfied: jupyter-client~=7.3.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (7.3.5)\n","Requirement already satisfied: jupyter-console~=6.6.3 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (6.6.3)\n","Requirement already satisfied: jupyter-core~=5.3.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 13)) (5.3.2)\n","Requirement already satisfied: ipython~=7.34.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 14)) (7.34.0)\n","Requirement already satisfied: ipython-genutils==0.2.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 15)) (0.2.0)\n","Requirement already satisfied: ipywidgets==8.1.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 16)) (8.1.0)\n","Requirement already satisfied: matplotlib~=3.7.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 17)) (3.7.5)\n","Requirement already satisfied: seaborn~=0.12.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 18)) (0.12.2)\n","Requirement already satisfied: rich~=13.5.2 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 19)) (13.5.3)\n","Requirement already satisfied: jsonlines~=3.1.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 20)) (3.1.0)\n","Requirement already satisfied: scikit-learn~=1.3.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 21)) (1.3.2)\n","Requirement already satisfied: dataclasses~=0.6 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 22)) (0.6)\n","Requirement already satisfied: einops~=0.7.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 23)) (0.7.0)\n","Requirement already satisfied: tokenizers~=0.14.1 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 24)) (0.14.1)\n","Requirement already satisfied: transformers~=4.35.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 25)) (4.35.2)\n","Requirement already satisfied: huggingface-hub~=0.16.4 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 26)) (0.16.4)\n","Requirement already satisfied: rouge-score in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 27)) (0.1.2)\n","Requirement already satisfied: comm\u003e=0.1.3 in /usr/local/lib/python3.11/dist-packages (from ipywidgets==8.1.0-\u003e-r requirements.txt (line 16)) (0.2.2)\n","Requirement already satisfied: traitlets\u003e=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets==8.1.0-\u003e-r requirements.txt (line 16)) (5.7.1)\n","Requirement already satisfied: widgetsnbextension~=4.0.7 in /usr/local/lib/python3.11/dist-packages (from ipywidgets==8.1.0-\u003e-r requirements.txt (line 16)) (4.0.14)\n","Requirement already satisfied: jupyterlab-widgets~=3.0.7 in /usr/local/lib/python3.11/dist-packages (from ipywidgets==8.1.0-\u003e-r requirements.txt (line 16)) (3.0.14)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch~=2.0.1-\u003e-r requirements.txt (line 2)) (3.18.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch~=2.0.1-\u003e-r requirements.txt (line 2)) (4.13.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch~=2.0.1-\u003e-r requirements.txt (line 2)) (1.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch~=2.0.1-\u003e-r requirements.txt (line 2)) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch~=2.0.1-\u003e-r requirements.txt (line 2)) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.11/dist-packages (from torch~=2.0.1-\u003e-r requirements.txt (line 2)) (11.7.99)\n","Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.11/dist-packages (from torch~=2.0.1-\u003e-r requirements.txt (line 2)) (11.7.99)\n","Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.11/dist-packages (from torch~=2.0.1-\u003e-r requirements.txt (line 2)) (11.7.101)\n","Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.11/dist-packages (from torch~=2.0.1-\u003e-r requirements.txt (line 2)) (8.5.0.96)\n","Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.11/dist-packages (from torch~=2.0.1-\u003e-r requirements.txt (line 2)) (11.10.3.66)\n","Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch~=2.0.1-\u003e-r requirements.txt (line 2)) (10.9.0.58)\n","Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.11/dist-packages (from torch~=2.0.1-\u003e-r requirements.txt (line 2)) (10.2.10.91)\n","Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.11/dist-packages (from torch~=2.0.1-\u003e-r requirements.txt (line 2)) (11.4.0.1)\n","Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.11/dist-packages (from torch~=2.0.1-\u003e-r requirements.txt (line 2)) (11.7.4.91)\n","Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.11/dist-packages (from torch~=2.0.1-\u003e-r requirements.txt (line 2)) (2.14.3)\n","Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.11/dist-packages (from torch~=2.0.1-\u003e-r requirements.txt (line 2)) (11.7.91)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.11/dist-packages (from torch~=2.0.1-\u003e-r requirements.txt (line 2)) (2.0.0)\n","Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66-\u003etorch~=2.0.1-\u003e-r requirements.txt (line 2)) (0.45.1)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0-\u003etorch~=2.0.1-\u003e-r requirements.txt (line 2)) (3.31.6)\n","Requirement already satisfied: lit in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0-\u003etorch~=2.0.1-\u003e-r requirements.txt (line 2)) (18.1.8)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prettytable~=3.8.0-\u003e-r requirements.txt (line 5)) (0.2.13)\n","Requirement already satisfied: pyarrow\u003e=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets~=2.14.4-\u003e-r requirements.txt (line 6)) (18.1.0)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.11/dist-packages (from datasets~=2.14.4-\u003e-r requirements.txt (line 6)) (0.7)\n","Requirement already satisfied: dill\u003c0.3.8,\u003e=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets~=2.14.4-\u003e-r requirements.txt (line 6)) (0.3.7)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets~=2.14.4-\u003e-r requirements.txt (line 6)) (2.2.2)\n","Requirement already satisfied: requests\u003e=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets~=2.14.4-\u003e-r requirements.txt (line 6)) (2.32.3)\n","Requirement already satisfied: tqdm\u003e=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets~=2.14.4-\u003e-r requirements.txt (line 6)) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets~=2.14.4-\u003e-r requirements.txt (line 6)) (3.5.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets~=2.14.4-\u003e-r requirements.txt (line 6)) (0.70.15)\n","Requirement already satisfied: fsspec\u003c=2023.10.0,\u003e=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]\u003c=2023.10.0,\u003e=2023.1.0-\u003edatasets~=2.14.4-\u003e-r requirements.txt (line 6)) (2023.10.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets~=2.14.4-\u003e-r requirements.txt (line 6)) (3.11.15)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets~=2.14.4-\u003e-r requirements.txt (line 6)) (24.2)\n","Requirement already satisfied: tornado\u003e=6.1 in /usr/local/lib/python3.11/dist-packages (from notebook~=6.5.5-\u003e-r requirements.txt (line 9)) (6.4.2)\n","Requirement already satisfied: pyzmq\u003e=17 in /usr/local/lib/python3.11/dist-packages (from notebook~=6.5.5-\u003e-r requirements.txt (line 9)) (24.0.1)\n","Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook~=6.5.5-\u003e-r requirements.txt (line 9)) (23.1.0)\n","Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from notebook~=6.5.5-\u003e-r requirements.txt (line 9)) (5.10.4)\n","Requirement already satisfied: nbconvert\u003e=5 in /usr/local/lib/python3.11/dist-packages (from notebook~=6.5.5-\u003e-r requirements.txt (line 9)) (7.16.6)\n","Requirement already satisfied: nest-asyncio\u003e=1.5 in /usr/local/lib/python3.11/dist-packages (from notebook~=6.5.5-\u003e-r requirements.txt (line 9)) (1.6.0)\n","Requirement already satisfied: ipykernel in /usr/local/lib/python3.11/dist-packages (from notebook~=6.5.5-\u003e-r requirements.txt (line 9)) (6.17.1)\n","Requirement already satisfied: Send2Trash\u003e=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook~=6.5.5-\u003e-r requirements.txt (line 9)) (1.8.3)\n","Requirement already satisfied: terminado\u003e=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook~=6.5.5-\u003e-r requirements.txt (line 9)) (0.18.1)\n","Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook~=6.5.5-\u003e-r requirements.txt (line 9)) (0.21.1)\n","Requirement already satisfied: nbclassic\u003e=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook~=6.5.5-\u003e-r requirements.txt (line 9)) (1.3.0)\n","Requirement already satisfied: qtconsole in /usr/local/lib/python3.11/dist-packages (from jupyter~=1.0.0-\u003e-r requirements.txt (line 10)) (5.6.1)\n","Requirement already satisfied: entrypoints in /usr/local/lib/python3.11/dist-packages (from jupyter-client~=7.3.1-\u003e-r requirements.txt (line 11)) (0.4)\n","Requirement already satisfied: python-dateutil\u003e=2.8.2 in /usr/local/lib/python3.11/dist-packages (from jupyter-client~=7.3.1-\u003e-r requirements.txt (line 11)) (2.9.0.post0)\n","Requirement already satisfied: prompt-toolkit\u003e=3.0.30 in /usr/local/lib/python3.11/dist-packages (from jupyter-console~=6.6.3-\u003e-r requirements.txt (line 12)) (3.0.51)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from jupyter-console~=6.6.3-\u003e-r requirements.txt (line 12)) (2.19.1)\n","Requirement already satisfied: platformdirs\u003e=2.5 in /usr/local/lib/python3.11/dist-packages (from jupyter-core~=5.3.1-\u003e-r requirements.txt (line 13)) (4.3.7)\n","Requirement already satisfied: jedi\u003e=0.16 in /usr/local/lib/python3.11/dist-packages (from ipython~=7.34.0-\u003e-r requirements.txt (line 14)) (0.19.2)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython~=7.34.0-\u003e-r requirements.txt (line 14)) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython~=7.34.0-\u003e-r requirements.txt (line 14)) (0.7.5)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython~=7.34.0-\u003e-r requirements.txt (line 14)) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython~=7.34.0-\u003e-r requirements.txt (line 14)) (0.1.7)\n","Requirement already satisfied: pexpect\u003e4.3 in /usr/local/lib/python3.11/dist-packages (from ipython~=7.34.0-\u003e-r requirements.txt (line 14)) (4.9.0)\n","Requirement already satisfied: contourpy\u003e=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.7.2-\u003e-r requirements.txt (line 17)) (1.3.2)\n","Requirement already satisfied: cycler\u003e=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.7.2-\u003e-r requirements.txt (line 17)) (0.12.1)\n","Requirement already satisfied: fonttools\u003e=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.7.2-\u003e-r requirements.txt (line 17)) (4.57.0)\n","Requirement already satisfied: kiwisolver\u003e=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.7.2-\u003e-r requirements.txt (line 17)) (1.4.8)\n","Requirement already satisfied: pillow\u003e=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.7.2-\u003e-r requirements.txt (line 17)) (11.2.1)\n","Requirement already satisfied: pyparsing\u003e=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib~=3.7.2-\u003e-r requirements.txt (line 17)) (3.2.3)\n","Requirement already satisfied: markdown-it-py\u003e=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich~=13.5.2-\u003e-r requirements.txt (line 19)) (3.0.0)\n","Requirement already satisfied: attrs\u003e=19.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonlines~=3.1.0-\u003e-r requirements.txt (line 20)) (25.3.0)\n","Requirement already satisfied: scipy\u003e=1.5.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn~=1.3.0-\u003e-r requirements.txt (line 21)) (1.15.2)\n","Requirement already satisfied: joblib\u003e=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn~=1.3.0-\u003e-r requirements.txt (line 21)) (1.4.2)\n","Requirement already satisfied: threadpoolctl\u003e=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn~=1.3.0-\u003e-r requirements.txt (line 21)) (3.6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers~=4.35.0-\u003e-r requirements.txt (line 25)) (2024.11.6)\n","Requirement already satisfied: safetensors\u003e=0.3.1 in /usr/local/lib/python3.11/dist-packages (from transformers~=4.35.0-\u003e-r requirements.txt (line 25)) (0.5.3)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score-\u003e-r requirements.txt (line 27)) (1.4.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score-\u003e-r requirements.txt (line 27)) (3.9.1)\n","Requirement already satisfied: six\u003e=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score-\u003e-r requirements.txt (line 27)) (1.17.0)\n","Requirement already satisfied: aiohappyeyeballs\u003e=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp-\u003edatasets~=2.14.4-\u003e-r requirements.txt (line 6)) (2.6.1)\n","Requirement already satisfied: aiosignal\u003e=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp-\u003edatasets~=2.14.4-\u003e-r requirements.txt (line 6)) (1.3.2)\n","Requirement already satisfied: frozenlist\u003e=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp-\u003edatasets~=2.14.4-\u003e-r requirements.txt (line 6)) (1.6.0)\n","Requirement already satisfied: multidict\u003c7.0,\u003e=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp-\u003edatasets~=2.14.4-\u003e-r requirements.txt (line 6)) (6.4.3)\n","Requirement already satisfied: propcache\u003e=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp-\u003edatasets~=2.14.4-\u003e-r requirements.txt (line 6)) (0.3.1)\n","Requirement already satisfied: yarl\u003c2.0,\u003e=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp-\u003edatasets~=2.14.4-\u003e-r requirements.txt (line 6)) (1.20.0)\n","Requirement already satisfied: debugpy\u003e=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel-\u003enotebook~=6.5.5-\u003e-r requirements.txt (line 9)) (1.8.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel-\u003enotebook~=6.5.5-\u003e-r requirements.txt (line 9)) (5.9.5)\n","Requirement already satisfied: parso\u003c0.9.0,\u003e=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi\u003e=0.16-\u003eipython~=7.34.0-\u003e-r requirements.txt (line 14)) (0.8.4)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py\u003e=2.2.0-\u003erich~=13.5.2-\u003e-r requirements.txt (line 19)) (0.1.2)\n","Requirement already satisfied: notebook-shim\u003e=0.2.3 in /usr/local/lib/python3.11/dist-packages (from nbclassic\u003e=0.4.7-\u003enotebook~=6.5.5-\u003e-r requirements.txt (line 9)) (0.2.4)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from nbconvert\u003e=5-\u003enotebook~=6.5.5-\u003e-r requirements.txt (line 9)) (4.13.4)\n","Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0-\u003enbconvert\u003e=5-\u003enotebook~=6.5.5-\u003e-r requirements.txt (line 9)) (6.2.0)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert\u003e=5-\u003enotebook~=6.5.5-\u003e-r requirements.txt (line 9)) (0.7.1)\n","Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert\u003e=5-\u003enotebook~=6.5.5-\u003e-r requirements.txt (line 9)) (0.3.0)\n","Requirement already satisfied: markupsafe\u003e=2.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert\u003e=5-\u003enotebook~=6.5.5-\u003e-r requirements.txt (line 9)) (3.0.2)\n","Requirement already satisfied: mistune\u003c4,\u003e=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert\u003e=5-\u003enotebook~=6.5.5-\u003e-r requirements.txt (line 9)) (3.1.3)\n","Requirement already satisfied: nbclient\u003e=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert\u003e=5-\u003enotebook~=6.5.5-\u003e-r requirements.txt (line 9)) (0.10.2)\n","Requirement already satisfied: pandocfilters\u003e=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert\u003e=5-\u003enotebook~=6.5.5-\u003e-r requirements.txt (line 9)) (1.5.1)\n","Requirement already satisfied: fastjsonschema\u003e=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat-\u003enotebook~=6.5.5-\u003e-r requirements.txt (line 9)) (2.21.1)\n","Requirement already satisfied: jsonschema\u003e=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat-\u003enotebook~=6.5.5-\u003e-r requirements.txt (line 9)) (4.23.0)\n","Requirement already satisfied: pytz\u003e=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas-\u003edatasets~=2.14.4-\u003e-r requirements.txt (line 6)) (2025.2)\n","Requirement already satisfied: tzdata\u003e=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas-\u003edatasets~=2.14.4-\u003e-r requirements.txt (line 6)) (2025.2)\n","Requirement already satisfied: ptyprocess\u003e=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect\u003e4.3-\u003eipython~=7.34.0-\u003e-r requirements.txt (line 14)) (0.7.0)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.11/dist-packages (from requests\u003e=2.19.0-\u003edatasets~=2.14.4-\u003e-r requirements.txt (line 6)) (3.4.1)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.11/dist-packages (from requests\u003e=2.19.0-\u003edatasets~=2.14.4-\u003e-r requirements.txt (line 6)) (3.10)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests\u003e=2.19.0-\u003edatasets~=2.14.4-\u003e-r requirements.txt (line 6)) (2025.4.26)\n","Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-\u003enotebook~=6.5.5-\u003e-r requirements.txt (line 9)) (21.2.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk-\u003erouge-score-\u003e-r requirements.txt (line 27)) (8.1.8)\n","Requirement already satisfied: qtpy\u003e=2.4.0 in /usr/local/lib/python3.11/dist-packages (from qtconsole-\u003ejupyter~=1.0.0-\u003e-r requirements.txt (line 10)) (2.4.3)\n","Requirement already satisfied: mpmath\u003c1.4,\u003e=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy-\u003etorch~=2.0.1-\u003e-r requirements.txt (line 2)) (1.3.0)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0-\u003ebleach[css]!=5.0.0-\u003enbconvert\u003e=5-\u003enotebook~=6.5.5-\u003e-r requirements.txt (line 9)) (0.5.1)\n","Requirement already satisfied: tinycss2\u003c1.5,\u003e=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0-\u003enbconvert\u003e=5-\u003enotebook~=6.5.5-\u003e-r requirements.txt (line 9)) (1.4.0)\n","Requirement already satisfied: jsonschema-specifications\u003e=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema\u003e=2.6-\u003enbformat-\u003enotebook~=6.5.5-\u003e-r requirements.txt (line 9)) (2025.4.1)\n","Requirement already satisfied: referencing\u003e=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema\u003e=2.6-\u003enbformat-\u003enotebook~=6.5.5-\u003e-r requirements.txt (line 9)) (0.36.2)\n","Requirement already satisfied: rpds-py\u003e=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema\u003e=2.6-\u003enbformat-\u003enotebook~=6.5.5-\u003e-r requirements.txt (line 9)) (0.24.0)\n","Requirement already satisfied: jupyter-server\u003c3,\u003e=1.8 in /usr/local/lib/python3.11/dist-packages (from notebook-shim\u003e=0.2.3-\u003enbclassic\u003e=0.4.7-\u003enotebook~=6.5.5-\u003e-r requirements.txt (line 9)) (1.16.0)\n","Requirement already satisfied: cffi\u003e=1.0.1 in /usr/local/lib/python3.11/dist-packages (from argon2-cffi-bindings-\u003eargon2-cffi-\u003enotebook~=6.5.5-\u003e-r requirements.txt (line 9)) (1.17.1)\n","Requirement already satisfied: soupsieve\u003e1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4-\u003enbconvert\u003e=5-\u003enotebook~=6.5.5-\u003e-r requirements.txt (line 9)) (2.7)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi\u003e=1.0.1-\u003eargon2-cffi-bindings-\u003eargon2-cffi-\u003enotebook~=6.5.5-\u003e-r requirements.txt (line 9)) (2.22)\n","Requirement already satisfied: anyio\u003e=3.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server\u003c3,\u003e=1.8-\u003enotebook-shim\u003e=0.2.3-\u003enbclassic\u003e=0.4.7-\u003enotebook~=6.5.5-\u003e-r requirements.txt (line 9)) (4.9.0)\n","Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server\u003c3,\u003e=1.8-\u003enotebook-shim\u003e=0.2.3-\u003enbclassic\u003e=0.4.7-\u003enotebook~=6.5.5-\u003e-r requirements.txt (line 9)) (1.8.0)\n","Requirement already satisfied: sniffio\u003e=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio\u003e=3.1.0-\u003ejupyter-server\u003c3,\u003e=1.8-\u003enotebook-shim\u003e=0.2.3-\u003enbclassic\u003e=0.4.7-\u003enotebook~=6.5.5-\u003e-r requirements.txt (line 9)) (1.3.1)\n"]}],"source":["\n","# set to location where you uploaded directory\n","%cd \"/content/drive/MyDrive/cs4740_programming_assignments/hw4-release\"\n","%pip install -r requirements.txt\n","\n","import IPython\n","\n","ipython = IPython.get_ipython()\n","#ipython.run_line_magic(\"sx\", f\"chmod +x scripts/*.py\")\n","\n","ipython.run_line_magic(\"load_ext\", \"autoreload\")\n","ipython.run_line_magic(\"autoreload\", \"2\")"]},{"cell_type":"code","execution_count":3,"id":"4cv1iDDe33zW","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":279},"executionInfo":{"elapsed":28,"status":"ok","timestamp":1746583243932,"user":{"displayName":"Aileen Huang","userId":"01843079669161655601"},"user_tz":240},"id":"4cv1iDDe33zW","outputId":"d238401a-84c6-4c2c-d07e-e8fc107c38ee"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[92mSuccess!\n","\u001b[0m"]},{"data":{"text/html":["\u003cvideo alt=\"success, happy puppy!\" width=\"400\" height=\"240\" controls autoplay=1\u003e\n","                \u003csource src=\"https://openpuppies.com/mp4/bDYdPSV.mp4\" type=\"video/mp4\"/\u003e \n","            \u003c/video\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"}],"source":["from IPython.display import display\n","\n","try:\n","    from seagull.utils.utils import success, colored\n","    display(success())\n","except ImportError:\n","    print(\"\\033[31mInstallation failed, please retrace your steps ...\")"]},{"cell_type":"markdown","id":"9fAOFM8UB5-1","metadata":{"id":"9fAOFM8UB5-1"},"source":["Let's import a few packages and methods that are used throughout this notebook; in this notebook, you are free to import and/or install packages (a lot of the packages you may need should already be available) as you see fit. \u003cfont color=\"red\"\u003eThat said, you are __not__ allowed to modify the imports in any of the Python source files; further, please do not modify (delete lines, change method signatures, etc.) above or below the `TODO` placeholders within the Python source files.\u003c/font\u003e"]},{"cell_type":"code","execution_count":4,"id":"nXlqoSiZ4Xmo","metadata":{"executionInfo":{"elapsed":41104,"status":"ok","timestamp":1746583285038,"user":{"displayName":"Aileen Huang","userId":"01843079669161655601"},"user_tz":240},"id":"nXlqoSiZ4Xmo"},"outputs":[],"source":["import os\n","import random\n","import textwrap\n","from collections import Counter\n","from itertools import chain\n","from time import process_time\n","\n","import datasets\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import torch\n","import torch.nn.functional as F\n","import yaml\n","from IPython.display import display\n","\n","from seagull.data_processing.bbpe import BBPETokenizer\n","from seagull.data_processing.sequence_sampler import SequenceSamplingDataset\n","from seagull.model.components.embedding import Embedding\n","from seagull.data_processing.constants import SCENE_TOKEN, UNCANNY_TOKEN, CAPTION_TOKEN, END_OF_CAPTION_TOKEN\n","from seagull.model.components.transformer_layer import TransformerLayer\n","from seagull.model.heads.seagull_lm import SeagullLM, greedy_decode, top_p_decode, top_k_decode, sampling_decode, make_seagull_talk\n","from seagull.model.seagull_transformer import Seagull\n","from seagull.nn.transformer.ffn import FFN\n","from seagull.nn.transformer.mha import MultiHeadAttention\n","from seagull.utils.utils import colored, success\n","\n","\n","%matplotlib inline\n","%config InlineBackend.figure_format=\"retina\"\n","plt.rcParams[\"font.size\"] = 8\n","\n","text_wrapper = textwrap.TextWrapper(width=120)"]},{"cell_type":"markdown","id":"LtXDn6yzZ1zg","metadata":{"id":"LtXDn6yzZ1zg"},"source":["Same as HW3, let's set up a few filepaths: for convenience, we will redirect all the output artefacts to `artefacts` folder — this includes the processed/tokenized datasets, experimental artefacts (e.g., model checkpoints, configs, etc.), submission .zip files, and others."]},{"cell_type":"code","execution_count":5,"id":"OA8hYWt7Z0e_","metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1746583285040,"user":{"displayName":"Aileen Huang","userId":"01843079669161655601"},"user_tz":240},"id":"OA8hYWt7Z0e_"},"outputs":[],"source":["BASE_DIR = os.path.abspath(\".\")\n","\n","PRETRAINED_ARTEFACTS_DIR = os.path.join(BASE_DIR, \"pretrained_artefacts\")\n","ARTEFACTS_DIR = os.path.join(BASE_DIR, \"artefacts\")\n","SCRIPTS_DIR = os.path.join(BASE_DIR, \"scripts\")\n","CONFIGS_DIR = os.path.join(BASE_DIR, \"scripts/configs\")"]},{"cell_type":"markdown","id":"vlpXBGpwb501","metadata":{"id":"vlpXBGpwb501"},"source":["Finally, set your and your partner's net IDs as a comma-separated string (e.g., \"`tg352,ljl2`\"); we'll use the `net_ids` variable to auto-populate any required information while making the submission. Please __don't__ include any spaces or special characters (e.g., \"`\u003ctg352\u003e`\" or \"`tg352, ljl2`\" will result in processing errors)."]},{"cell_type":"code","execution_count":6,"id":"C4efW6odcv-B","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"collapsed":true,"executionInfo":{"elapsed":7,"status":"ok","timestamp":1746583285044,"user":{"displayName":"Aileen Huang","userId":"01843079669161655601"},"user_tz":240},"id":"C4efW6odcv-B","outputId":"bce851df-f3e0-4dd8-e0f5-d0e2ef564e45"},"outputs":[{"name":"stdout","output_type":"stream","text":["net_ids='aeh245' set\n"]}],"source":["# Add your net IDs as a comma-separated string (e.g., \"\u003cnet-id-1\u003e,\u003cnet-id-2\u003e\").\n","net_ids = \"aeh245\"\n","\n","if net_ids is None:\n","    raise ValueError(\"net-IDs not set; set them above\")\n","print(f\"{net_ids=} set\")"]},{"cell_type":"markdown","id":"JsUwZ7LBdJMI","metadata":{"id":"JsUwZ7LBdJMI"},"source":["---\n","\n","\u003ca name=\"sec1\"\u003e\u003c/a\u003e\n","### [1] Assignment overview \u003csmall\u003e[↩︎](#outline)\u003c/small\u003e\n","\n","In this assignment, we will use Seagull, the transformer-based language model you implemented in HW3, to generate humorous captions for a given scene and uncanny description; inspired from what can be described as a natural language adaptation of the New Yorker Cartoon Caption Contest: https://www.newyorker.com/cartoons/contest. Here's an example (taken from the validation data):\n","\n","\u003e __Scene__: Two explorers are hiking through nature. They come across two subway cars in the brush. \u003cfont color=\"blue\"\u003e← input to the model\u003c/font\u003e \u003cbr/\u003e\n","\u003e __Uncanny description__: Subway cars wouldn't be way out in the middle of nowhere. \u003cfont color=\"blue\"\u003e← input to the model\u003c/font\u003e \u003cbr/\u003e\n","\u003e __Caption__: Of all the things we packed, I didn't think to bring my MetroCard. \u003cfont color=\"green\"\u003e← to be generated by the model!\u003c/font\u003e\n","\n","Our objective is to train our Seagull to generate the caption text, hopefully a humorous one; two key things to note here: we want the model to 1) \"generate\" and 2) utilize the given scene and uncanny description to generate a suitable caption. For the \"generate\" part, let's formulate the task as an autoregressive problem: for the given sequence, predict the next token; use the predicted token to predict the next one, and so on.\n","\n","For your reference, this was the Seagull model you already implemented in HW3:\n","\n","\u003cdiv align=\"center\"\u003e\u003cbr/\u003e\n","    \u003cimg src=\"https://i.imgur.com/3K75QZG.png\" width=780px/\u003e\n","\u003cbr/\u003e\u003c/div\u003e"]},{"cell_type":"markdown","id":"afe2eac7","metadata":{"id":"afe2eac7"},"source":["\u003ca name=\"sec2\"\u003e\u003c/a\u003e\n","### [2] Milestone Submission \u003csmall\u003e[↩︎](#outline)\u003c/small\u003e\n","\n","Let us make the milestone submission right away. We will let you know if all the relevant functions from hw3 are correct on Thursday, April 24, 11.59 p.m. You will be able to request correct implementations of incorrect functions (see instructions above) without penalty.\n","\n","**Note: Please do not delay working on the assignment while you results of this check and the correct implementations (if needed)! You can proceed upto section 4 of this assignment without waiting to verify that your milestone code is correct.**\n","\n","\n","You will need to copy-paste the following functions from your HW3 implementations:\n","* `__getitem__` function of `SequenceSamplingDataset` in `seagull/data_processing/sequence_sampler.py`\n","* `forward()` method of the `Embedding` class `seagull/model/components/embedding.py`\n","* `attention()` function in `seagull/nn/transformer/mha.py`\n","* `masked_attention_probs()` in `seagull/nn/transformer/mha.py`\n","* `forward()` function of `FFN` class in `seagull/nn/transformer/ffn.py`\n","* `forward()` function of `TransformerLayer` in `seagull/model/components/transformer_layer.py`\n","* `forward()` function of `SeagullLM` in `seagull/model/heads/seagull_lm.py`"]},{"cell_type":"markdown","id":"1075d8e4","metadata":{"id":"1075d8e4"},"source":["After you are done, run the cell below to make a milestone submission and check your code functinalities. The `make_submission.py` command when run with `--milestone-submission` flag creates a `milestone_submission.zip` file in `artefacts` folder, which is to be submitted on the submission site(s).  \u003cfont color=\"red\"\u003eCaution: the script will overwrite any existing file named `milestone_submission.zip` in `artefacts` folder.\u003c/font\u003e\n","\n","The `milestone_submission.zip` is all that you will need to submit for the milestone (no need to submit anything else!)."]},{"cell_type":"code","execution_count":7,"id":"3bfcc053","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":296},"executionInfo":{"elapsed":17587,"status":"ok","timestamp":1746583302632,"user":{"displayName":"Aileen Huang","userId":"01843079669161655601"},"user_tz":240},"id":"3bfcc053","outputId":"a2a49cba-8102-4922-efff-36feb0b4a2e1"},"outputs":[{"name":"stdout","output_type":"stream","text":["submission stored at: /content/drive/MyDrive/cs4740_programming_assignments/hw4-release/artefacts/milestone_submission.zip\n","\u001b[92mSuccess!\n","\u001b[0m"]},{"data":{"text/html":["\u003cvideo alt=\"success, happy puppy!\" width=\"400\" height=\"240\" controls autoplay=1\u003e\n","                \u003csource src=\"https://openpuppies.com/mp4/MQCIwzT.mp4\" type=\"video/mp4\"/\u003e \n","            \u003c/video\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"}],"source":["submission_filepath = f\"{ARTEFACTS_DIR}\"\n","\n","!python3 -m scripts.make_submission \\\n","    --basepath-to-store-submission={os.path.join(ARTEFACTS_DIR, submission_filepath)} \\\n","    --net-ids={net_ids} \\\n","    --milestone-submission\n","\n","if os.path.isfile(f\"{os.path.join(ARTEFACTS_DIR, 'milestone_submission.zip')}\"):\n","    display(success())\n","else:\n","    print(colored(\"Oops, something went wrong!\", \"red\"))"]},{"cell_type":"markdown","id":"792bc969","metadata":{"id":"792bc969"},"source":["---\n","\n","\u003ca name=\"sec3\"\u003e\u003c/a\u003e\n","### [3] Data processing, tokenization, padding \u003csmall\u003e[↩︎](#outline)\u003c/small\u003e\n","Let's get back to our task of generating humorous captions from scene descriptions. The dataset for this task is located in the `dataset` folder and is processed in an [arrow format](https://huggingface.co/docs/datasets/about_arrow). Let's load the dataset and see what the dataset looks like:"]},{"cell_type":"code","execution_count":8,"id":"a19c3d2f","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":13428,"status":"ok","timestamp":1746583316062,"user":{"displayName":"Aileen Huang","userId":"01843079669161655601"},"user_tz":240},"id":"a19c3d2f","outputId":"34108d5f-d239-4b0e-a6f4-31bfeba64b5c"},"outputs":[{"name":"stdout","output_type":"stream","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['scene', 'uncanny', 'caption'],\n","        num_rows: 79180\n","    })\n","    val: Dataset({\n","        features: ['scene', 'uncanny', 'caption'],\n","        num_rows: 10783\n","    })\n","    test: Dataset({\n","        features: ['scene', 'uncanny', 'id'],\n","        num_rows: 986\n","    })\n","})\n"]}],"source":["dataset = datasets.load_from_disk(\"dataset/raw\")\n","print(dataset)"]},{"cell_type":"markdown","id":"1ef138c2","metadata":{"id":"1ef138c2"},"source":["The dataset has three fields for train/val splits—scene, uncanny, and caption; while, the test split lacks the caption entry. Our objective is to use our Seagull to caption those.\n","\n","Run the cell below to inspect a few train/test instances. Upon running the cell, you'll note that in the train/val set, there are cases where one scene and uncanny description can be captioned in multiple ways. This adds to the task complexity; not only does the model have to learn to understand humor, but also that there's no one \"right\" answer!"]},{"cell_type":"code","execution_count":9,"id":"ff0e489d","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":74,"status":"ok","timestamp":1746583316128,"user":{"displayName":"Aileen Huang","userId":"01843079669161655601"},"user_tz":240},"id":"ff0e489d","outputId":"f0a9d7ba-3d79-4450-fc13-b61769efa188"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m\u001b[4msplit='train'\u001b[0m\u001b[0m\u001b[0m\n","[10] scene: An alligator is coming out of the floor. Two people stare at it. A waiter points at it.\n","[10] uncanny: There is an alligator coming out of the floor.\n","[10] caption: \u001b[94mIt's our way of encouraging social distancing.\u001b[0m\n","\n","[11] scene: An alligator is coming out of the floor. Two people stare at it. A waiter points at it.\n","[11] uncanny: There is an alligator coming out of the floor.\n","[11] caption: \u001b[94m... Technically, yes, but he identifies as a crocodile.\u001b[0m\n","\n","[12] scene: An alligator is coming out of the floor. Two people stare at it. A waiter points at it.\n","[12] uncanny: There is an alligator coming out of the floor.\n","[12] caption: \u001b[94mComplaints go in the box behind the alligator.\u001b[0m\n","\n","\u001b[1m\u001b[4m\n","split='test'\u001b[0m\u001b[0m\u001b[0m\n","[104] scene: People are at a table playing games, reading, and working on the computer. A woman is reading a book and talking to a man.\n","[104] uncanny: People are doing many things at a table and are very busy.\n","\n","[105] scene: Men have captured a giant. The giant is in a wide net. There is a castle in the background.\n","[105] uncanny: Giants do not exist. Giants are hard to capture.\n","\n"]}],"source":["split, sample_idxs = \"train\", [10, 11, 12]\n","print(colored(f\"{split=}\", attrs=[\"underline\", \"bold\"]))\n","for sample_idx in sample_idxs:\n","    print(f\"[{sample_idx}] scene: {dataset[split][sample_idx]['scene']}\")\n","    print(f\"[{sample_idx}] uncanny: {dataset[split][sample_idx]['uncanny']}\")\n","    print(f\"[{sample_idx}] caption: {colored(dataset[split][sample_idx]['caption'], color='blue')}\\n\")\n","\n","split, sample_idxs = \"test\", [104, 105]\n","print(colored(f\"\\n{split=}\", attrs=[\"underline\", \"bold\"]))\n","for sample_idx in sample_idxs:\n","    print(f\"[{sample_idx}] scene: {dataset[split][sample_idx]['scene']}\")\n","    print(f\"[{sample_idx}] uncanny: {dataset[split][sample_idx]['uncanny']}\\n\")"]},{"cell_type":"markdown","id":"af2cd721","metadata":{"id":"af2cd721"},"source":["\u003ca name=\"sec31\"\u003e\u003c/a\u003e\n","#### [3.1] Preprocessing for autoregressive modeling \u003csmall\u003e[↩︎](#outline)\u003c/small\u003e\n","\n","Now that we have a concrete idea of what our dataset looks like, let's see what the preprocessed version of this dataset, suited for our autoregressive language modeling task, i.e. predicting the next token, looks like. Specifically, we need our model to generate captions, token-by-token, using the scene and uncanny description. A simple way of doing this to perform template processing via control codes to facilitate in-context learning.\n","\n","We use the following template:\n","```text\n","\u003c|scene|\u003e ... \u003c|uncanny|\u003e ... \u003c|caption|\u003e ... \u003c|endofcaption|\u003e\n","```\n","where, `\u003c|scene|\u003e`, `\u003c|uncanny|\u003e`, `\u003c|caption|\u003e`, and `\u003c|endofcaption|\u003e` are special tokens used to facilitate controlled text generation. Don't worry if whatever is noted here seems alien; in simple words, we want the model to associate that the tokens between `\u003c|caption|\u003e ... \u003c|endofcaption|\u003e` correspond to humorous captions, associated with preceding scene tokens and uncanny tokens.\n","\n","Consider the following example (taken from the training data):\n","\u003e __Scene__: An alligator is coming out of the floor. Two people stare at it. A waiter points at it. \u003cbr/\u003e\n","\u003e __Uncanny__: There is an alligator coming out of the floor. \u003cbr/\u003e\n","\u003e __Caption__: It's our way of encouraging social distancing.\n","\n","The corresponding template-filled text is:\n","\n","\u003e `\u003c|scene|\u003e` An alligator is coming out of the floor. Two people stare at it. A waiter points at it. `\u003c|uncanny|\u003e` There is an alligator coming out of the floor. `\u003c|caption|\u003e` It's our way of encouraging social distancing. `\u003c|endofcaption|\u003e`\n","\n","We use the template-filled text to finetune our model on next token prediction. The above template holds for train/val splits where caption text is available. For test split, we use the following template instead:\n","```text\n","\u003c|scene|\u003e ... \u003c|uncanny|\u003e ... \u003c|caption|\u003e\n","```\n","Do you see what we're doing?—we're forcing the model to associate tokens after `\u003c|caption|\u003e` as associated captions, and at test/inference time, we hope that the model will start generating caption-like text after seeing the `\u003c|caption|\u003e` special token.\n","\n","This processing step is already implemented for you and the dataset stored in ``dataset/processed``. Let's inspect a few samples to familiarize ourselves with the template."]},{"cell_type":"code","execution_count":10,"id":"257c5a57","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":8706,"status":"ok","timestamp":1746583324834,"user":{"displayName":"Aileen Huang","userId":"01843079669161655601"},"user_tz":240},"id":"257c5a57","outputId":"5c7e8c51-3ac1-4c41-b953-b8b1e013c074"},"outputs":[{"name":"stdout","output_type":"stream","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['caption', 'text'],\n","        num_rows: 79180\n","    })\n","    val: Dataset({\n","        features: ['caption', 'text'],\n","        num_rows: 10783\n","    })\n","    test: Dataset({\n","        features: ['id', 'text', 'caption'],\n","        num_rows: 986\n","    })\n","}) \n","\n","\u001b[4m\u001b[1msplit='train'\u001b[0m\u001b[0m\u001b[0m\n","text: \u003c|scene|\u003e Two knights are standing guard in a throne room. one is in a hammock put up between two pillars. \u003c|uncanny|\u003e A guard shouldn't have a hammock where he's supposed to be standing guard. \u003c|caption|\u003e Going union was the best decision we ever made. \u003c|endofcaption|\u003e \n","\n","\u001b[4m\u001b[1msplit='val'\u001b[0m\u001b[0m\u001b[0m\n","text: \u003c|scene|\u003e A dolphin is casually riding a skateboard at a skatepark, performing impressive tricks while the crowd cheers. \u003c|uncanny|\u003e Dolphins are marine creatures and cannot ride skateboards due to their lack of limbs. \u003c|caption|\u003e Move aside, Tony Hawk, dolphins are here to show off their fin-tastic skills! \u003c|endofcaption|\u003e \n","\n","\u001b[4m\u001b[1msplit='test'\u001b[0m\u001b[0m\u001b[0m\n","text: \u003c|scene|\u003e A centaur man is looking at a naked man with an upper torso of a horse. The horse man with his butt sticking out looks sad. \u003c|uncanny|\u003e There is a centaur there and a man with a horse head. Both are not real. \u003c|caption|\u003e \n","\n"]}],"source":["processed_dataset = datasets.load_from_disk(\"dataset/processed\")\n","print(processed_dataset, \"\\n\")\n","\n","sample_idx = 10  # change to view other samples\n","for split in [\"train\", \"val\", \"test\"]:\n","    print(colored(f\"{split=}\", attrs=[\"bold\", \"underline\"]))\n","    print(\"text:\", processed_dataset[split][sample_idx][\"text\"], \"\\n\")"]},{"cell_type":"markdown","id":"4bb8529f","metadata":{"id":"4bb8529f"},"source":["Now that we've preprocessed our dataset in a format suited to our task of next token prediction, we can proceed to tokenize the template-filled text. We will use the same Byte-level Byte-Pair Encoding (BBPE) tokenizer  as HW3. Let's load the tokenizer; pay attention to the `special_tokens` in the tokenizer. We've already noted `\u003c|scene|\u003e`, `\u003c|uncanny|\u003e`, `\u003c|caption|\u003e`, `\u003c|endofcaption|\u003e` special tokens; other special tokens include: `\u003c|pad|\u003e` for padding token, `\u003c|unk|\u003e` for unknown token, and `\u003c|endoftext|\u003e` to indicate the end of text, i.e., the model has completed the ongoing chain of thought."]},{"cell_type":"code","execution_count":11,"id":"a88a7651","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":1996,"status":"ok","timestamp":1746583326832,"user":{"displayName":"Aileen Huang","userId":"01843079669161655601"},"user_tz":240},"id":"a88a7651","outputId":"9a34d402-62b9-4aaf-94a9-870da61b13a5"},"outputs":[{"data":{"text/plain":["BBPETokenizer(name=seagull-bbpe, vocab_size=33264, lowercase=False, punct_behavior=contiguous, special_tokens={'\u003c|pad|\u003e': 0, '\u003c|unk|\u003e': 1, '\u003c|endoftext|\u003e': 2, '\u003c|scene|\u003e': 3, '\u003c|uncanny|\u003e': 4, '\u003c|caption|\u003e': 5, '\u003c|endofcaption|\u003e': 6})"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["bbpe_tokenizer = BBPETokenizer()\n","bbpe_tokenizer.from_file(os.path.join(PRETRAINED_ARTEFACTS_DIR, \"tokenizer\"))\n","bbpe_tokenizer"]},{"cell_type":"markdown","id":"Rn0zLw3lNN2H","metadata":{"id":"Rn0zLw3lNN2H"},"source":["---\n","\u003cfont color=\"orange\"\u003e__Q1.__ While masking the attention matrix using the padding mask, we only mask the pad keys and not pad queries; take a moment to convince yourself that this is true. Why is it not needed to mask the pad queries? Would not padding queries have any unintended consequences? Explain in under three sentences; unclear or misleading arguments will receive no credit.\u003cbr/\u003e\n","\n","_Hint. Think of how the cross-entropy loss is backpropagated from outputs to input tokens, specifically the pad token._\n","\u003c/font\u003e"]},{"cell_type":"markdown","id":"dOxzJy5nNOuG","metadata":{"id":"dOxzJy5nNOuG"},"source":["__Answer.__\n","\n","As said in homework 3, assuming we only have padded tokens at the end, we don't need to mask pad queries because during backpropagation, we will ignore the output of padded query tokens with ignore_index (talked about on Ed, linked below).\n","However, it would have consequences. The input embedding to our attention mechanism still contains positional information since we combined both token embedding and positional embedding. If we also masked pad tokens of our queries, we would potentially lose some of this positional information. ED LINK: #634 https://edstem.org/us/courses/73802/discussion/6528460?comment=15133746"]},{"cell_type":"markdown","id":"f7900923","metadata":{"id":"f7900923"},"source":["You can run the `generate_tokenized_dataset.py` command to generate the tokenized dataset (from the processed dataset) using the pretrained tokenizer."]},{"cell_type":"code","execution_count":12,"id":"3ff39aaa","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":43234,"status":"ok","timestamp":1746583370067,"user":{"displayName":"Aileen Huang","userId":"01843079669161655601"},"user_tz":240},"id":"3ff39aaa","outputId":"b4417032-0a20-44d5-c60d-bdb1e501490a"},"outputs":[{"name":"stdout","output_type":"stream","text":["dataset/processed\n","Map: 100% 79180/79180 [00:24\u003c00:00, 3199.18 examples/s]\n","Map: 100% 10783/10783 [00:02\u003c00:00, 3919.54 examples/s]\n","Map: 100% 986/986 [00:00\u003c00:00, 4752.23 examples/s]\n","Saving the dataset (1/1 shards): 100% 79180/79180 [00:00\u003c00:00, 138381.40 examples/s]\n","Saving the dataset (1/1 shards): 100% 10783/10783 [00:01\u003c00:00, 8711.66 examples/s]\n","Saving the dataset (1/1 shards): 100% 986/986 [00:01\u003c00:00, 834.48 examples/s]\n"]}],"source":["!python3 -m scripts.generate_tokenized_dataset \\\n","    --config-path={os.path.join(CONFIGS_DIR, \"generate_tokenized_dataset.yml\")} \\\n","    --basepath-to-processed-dataset=\"dataset/processed\" \\\n","    --tokenizer-basepath={os.path.join(PRETRAINED_ARTEFACTS_DIR, \"tokenizer\")} \\\n","    --path-to-store-tokenized-dataset={os.path.join(ARTEFACTS_DIR, \"dataset/tokenized_dataset\")}"]},{"cell_type":"markdown","id":"c7aac4d9","metadata":{"id":"c7aac4d9"},"source":["Let's inspect the dataset"]},{"cell_type":"code","execution_count":13,"id":"87647ff6","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":68,"status":"ok","timestamp":1746583370139,"user":{"displayName":"Aileen Huang","userId":"01843079669161655601"},"user_tz":240},"id":"87647ff6","outputId":"e9e03c14-2b69-4a45-bd4b-6d1d2859f092"},"outputs":[{"name":"stdout","output_type":"stream","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['caption', 'text', 'input_ids', 'attention_mask'],\n","        num_rows: 79180\n","    })\n","    val: Dataset({\n","        features: ['caption', 'text', 'input_ids', 'attention_mask'],\n","        num_rows: 10783\n","    })\n","    test: Dataset({\n","        features: ['id', 'text', 'caption', 'input_ids', 'attention_mask'],\n","        num_rows: 986\n","    })\n","}) \n","\n","\u001b[4m\u001b[1msplit='train'\u001b[0m\u001b[0m\u001b[0m\n","input_ids: [3, 580, 6089, 319, 611, 2667, 298, 263, 2000, 768, 20, 569, 293, 298, 263, 2431, 1581, 485, 1682, 846, 8250, 20, 227, 4, 306, 2667, 1347, 13, 90, 405, 263, 2431, 1169, 416, 13, 89, 1988, 309, 338, 611, 2667, 20, 227, 5, 10421, 10010, 874, 279, 2136, 9757, 351, 3113, 1736, 20, 227, 6, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] \n","\n","\u001b[4m\u001b[1msplit='val'\u001b[0m\u001b[0m\u001b[0m\n","input_ids: [3, 306, 3003, 293, 1221, 873, 263, 2425, 355, 263, 10654, 18, 1058, 2420, 1970, 499, 279, 1167, 11015, 20, 227, 4, 1850, 319, 6230, 1535, 321, 1652, 1403, 8420, 4115, 309, 371, 1471, 302, 4571, 20, 227, 5, 3059, 7702, 18, 8854, 9578, 18, 1469, 319, 1038, 309, 1034, 559, 371, 1168, 19, 3823, 731, 7, 227, 6, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] \n","\n","\u001b[4m\u001b[1msplit='test'\u001b[0m\u001b[0m\u001b[0m\n","input_ids: [3, 306, 23520, 377, 293, 695, 355, 263, 17526, 377, 354, 304, 11003, 265, 864, 85, 302, 263, 1229, 20, 330, 1229, 377, 354, 547, 7304, 4895, 436, 728, 4811, 20, 227, 4, 479, 293, 263, 23520, 847, 321, 263, 377, 354, 263, 1229, 815, 20, 3436, 319, 367, 1124, 20, 227, 5, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","attention_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] \n","\n"]}],"source":["tokenized_dataset = datasets.load_from_disk(os.path.join(ARTEFACTS_DIR, \"dataset/tokenized_dataset\"))\n","print(tokenized_dataset, \"\\n\")\n","\n","sample_idx = 10  # change to view other samples\n","for split in [\"train\", \"val\", \"test\"]:\n","    print(colored(f\"{split=}\", attrs=[\"bold\", \"underline\"]))\n","    print(\"input_ids:\", tokenized_dataset[split][sample_idx][\"input_ids\"])\n","    print(\"attention_mask:\", tokenized_dataset[split][sample_idx][\"attention_mask\"], \"\\n\")"]},{"cell_type":"markdown","id":"aa3f9bd7","metadata":{"id":"aa3f9bd7"},"source":["Recall that the \\_\\_getitem\\_\\_ function you implemented in `SequenceSamplingDataset` in `seagull/data_processing/sequence_sampler.py` in HW3 takes these as input, and returns the following:\n","\n","\n","```python\n","{\n","  \"input_ids\": tensor([...]),\n","  \"padding_mask\": tensor([...]),\n","  \"labels\": tensor([...])  # = input_ids shifted left\n","}\n","```"]},{"cell_type":"markdown","id":"QI55FbJGaVlw","metadata":{"id":"QI55FbJGaVlw"},"source":["\u003ca name=\"sec4\"\u003e\u003c/a\u003e\n","### [4] Sampling Tokens given Logits \u003csmall\u003e[↩︎](#outline)\u003c/small\u003e\n","\n","\u003e \u003cfont color=\"orange\"\u003eFile to be modified: `seagull/model/heads/seagull_lm.py`.\u003c/font\u003e\n","\n","Inspect the `forward()` function (or just the documentation) of the `seagull/model/heads/seagull_lm.py` file. This function describes all the computation that happens in the transformer model. It corresponds to the seagull transformer figure at the start of the notebook. It takes the input_ids and the padding_mask (see structure above) as inputs. The forward pass through the Seagull transformer finally returns the logits and the output embeddings from the transformer.\n","\n","Let's focus on the logits; particularly, how we can generate text using these logits. Using our autoregressive model, we want to generate the next token using the logits of the current time step. Remember that the logits at the current time step are tensors of size batch_size x vocabulary_size. For each data point in the batch (dim 0), the logits tensor assigns a score to each token in the vocabulary (dim 1); higher the token score means that that token is more likely to be the next token, compared to tokens with lower scores.\n","\n","However, these raw scores need to be converted into a probability distribution that we can sample from. There are many ways of converting these logits into probability distributions and sampling from them, called decoding strategies. We will implement 4 different decoding strategies in this assignment, described below. Recall that we covered these decoding streategies in class (see lecture https://www.cs.cornell.edu/courses/cs4740/2025sp/lectures/Lec18.pdf) and also Jurafsky and Martin, Chapter 10.2 (https://web.stanford.edu/~jurafsky/slp3/10.pdf). We will implement 4 different decoding strategies in ``TODOS 4.1-1 - 4.1-4``.\n","\n","\n","But first, let us understand the overall workflow of generating text from the Seagull transformer. We'd like to draw your attention to the ``talk()`` function in the SeagullLM class. The function allows for our Seagull model to \"speak\". It takes as input the tensor of input_ids, hyperparameter num_samples that specifies how many outputs to generate for the given input (specified by input_ids), hyperparameter max_new_tokens that specifies the max generation length, and the decoding strategy (one of greedy_decode', 'sampling_decode', 'top_p_decode' or 'top_k_decode'.\n","\n","Given these inputs, the talk function uses the Seagull model to produce up to max_new_tokens number of tokens that the model thinks is a likely continuation of the ongoing sequence (based on the context from the input input_ids).\n","\n","Let's dig into the function a bit more. Here, we assume that the input_ids always contain one data sample, i.e. batch_size = 1. Given such a input_ids tensor, we create num_samples copies of this tensor. We do the same operation on position_ids. Therefore, all_input_ids and all_position_ids tensor is of size num_samples x seq_length. We treat each row of this \"batch\" as an independent data point during generation. This allows us to get different generated continuations for the same input.\n","\n","\n","Aside: You will note that if the sequence length (dim 1) of input_ids is greater than Seagull's _max_positions, we truncate the input from the left. Can you think of why we truncate from the left and not right?\n","\n","What should be our stopping_condition for generation? We should stop generating when we either generate the max_new_tokens number of tokens. But also, recall that our processed data examples in training always end with the ``\u003c|endofcaption|\u003e`` token. We should also stop generating if all the data points in the batch have generated the ``\u003c|endofcaption|\u003e`` token.\n","To satisfy these criterion, the for loop iterates from 0 -\u003e max_new_tokens, and the ``ended_mask`` tracks which of the generations in the batch have already generated the ``\u003c|endofcaption|\u003e`` token. ``ended_mask`` is initialized as a tensor of ``None``s of length num_samples."]},{"cell_type":"markdown","id":"1ef118d8","metadata":{"id":"1ef118d8"},"source":["\u003ca name=\"sec41\"\u003e\u003c/a\u003e\n","#### [4.1] Decoding Strategies \u003csmall\u003e[↩︎](#outline)\u003c/small\u003e\n","\n","To get started, we will implement our 4 decoding strategies. Note that you can implement these without worrying about the correctness of your copied code from HW3.\n","\n","First, let's implement greedy decode, corresponding to \u003cfont color=\"orange\"\u003e`TODO-4.1-1`\u003c/font\u003e in `seagull/model/heads/seagull_lm.py`. The ``greedy_decode()`` takes the logits as input. For each datapoint, it selects the most probable token at each step. This is deterministic and always picks the token with the highest score.\n","\n"]},{"cell_type":"markdown","id":"aaefac8f","metadata":{"id":"aaefac8f"},"source":["Once you've implemented the function, you can use the following test case to test your implementation. But first, look at the example and try to convince yourself that the expected output is correct."]},{"cell_type":"code","execution_count":14,"id":"66n8Pf3-XYjR","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":35,"status":"ok","timestamp":1746583370175,"user":{"displayName":"Aileen Huang","userId":"01843079669161655601"},"user_tz":240},"id":"66n8Pf3-XYjR","outputId":"6db7066e-0573-48b7-9737-8f9a531cbc6d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Test passed. Output: tensor([[1],\n","        [2],\n","        [2]])\n"]}],"source":["logits = torch.tensor([\n","    [1.0, 3.2, 0.5],\n","    [0.2, 0.1, 4.5],\n","    [-1.0, -0.5, -0.2]\n","])\n","\n","expected = torch.tensor([\n","    [1],\n","    [2],\n","    [2]\n","])\n","output = greedy_decode(logits)\n","assert torch.equal(output, expected), f\"Test failed. Got {output}, expected {expected}\"\n","\n","print(\"Test passed. Output:\", output)"]},{"cell_type":"markdown","id":"JRC8Ymzicvng","metadata":{"id":"JRC8Ymzicvng"},"source":["Next, we will implement random sampling. Remember that logits provide you will raw scores. To sample, we first need to convert these logits into a probability distribution over the vocabulary tokens and sample from the resulting probability distribution. This allows us to introduce stochasticity; we can get distinct outputs from the model given the same input!\n","\n","In this TODO, we will implement sampling with temperature scaling. Essentially, for temperature scaling, we divide the logits by a given temperature before converting to a probability distribution. Complete \u003cfont color=\"orange\"\u003e`TODO-4.1-2`\u003c/font\u003e, by implementing the  ``sampling_decode`` function that accepts the logits tensor and hyperparameter temperature as inputs.\n","\n","Hint: You should use `torch.multinomial()` to draw a sample from a probability distribution."]},{"cell_type":"code","execution_count":15,"id":"46ac2cdc","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":41,"status":"ok","timestamp":1746583370218,"user":{"displayName":"Aileen Huang","userId":"01843079669161655601"},"user_tz":240},"id":"46ac2cdc","outputId":"da6cdc20-3ace-47e2-998b-6b8b57cb2d03"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[1],\n","        [2],\n","        [2]])\n","tensor([[1],\n","        [2],\n","        [2]])\n","tensor([[2],\n","        [2],\n","        [2]])\n","tensor([[1],\n","        [0],\n","        [0]])\n","tensor([[2],\n","        [2],\n","        [1]])\n"]}],"source":["output = sampling_decode(logits, temperature=0.9)\n","\n","assert output.shape == expected.shape, f\"Output tensor shape as expected. Does not guarantee correctness of code\"\n","\n","temps = [0.1, 0.9, 20, 50, 100]\n","for t in temps:\n","  output = sampling_decode(logits, temperature=t)\n","  print(output)\n"]},{"cell_type":"markdown","id":"9DVD5sP9Nwpd","metadata":{"id":"9DVD5sP9Nwpd"},"source":["Since ``sampling_decode`` is stochastic, the above only checks that your output tensors are the correct shape. Note that the expected shape is same for all decoding strategies."]},{"cell_type":"markdown","id":"RTDRBBICNzDm","metadata":{"id":"RTDRBBICNzDm"},"source":["\u003cfont color=\"orange\"\u003e__Q2.__ Does the model generate less/more diverse samples when the temperature is high (say, 47.40)? What happens when the temperature is low (e.g., 0.1)?\n","\n","\u003c/font\u003e"]},{"cell_type":"markdown","id":"fcVYkqafN7Sh","metadata":{"id":"fcVYkqafN7Sh"},"source":["__Answer.__\n","\n","As shown with the printout above, when temperature is high, because all our logits will become small and also more similar, there's a greater chance for different types tokens to get sampled, but this includes insignificant ones. On the other hand, when temperature is low, temperature scaling will exagerrate logit differences. Therefore, we'll tend to get more greedy tokens."]},{"cell_type":"markdown","id":"aace9702","metadata":{"id":"aace9702"},"source":["\n","\n","\n","Next, we will implement the next decoding strategy, i.e. top-k sampling. The basic idea is as follows: given an integer ``k``, top-k sampling limits the sampling space to the ``k`` most likely tokens, or the ``k`` highest scoring tokens in the vocabulary.\n","\n","consider the example below. For the input \"The sky is\", we show the next token probabilities (not logits!) for 3 tokens (assume the next token probabilities of all others is 0 for this example).\n","The sky is\n","        blue 0.5\n","        yellow 0.2\n","        overcast 0.3\n","\n","Say k=2. This means that we must only sample from the top-2 most likely tokens, i.e. blue and overcast, in our example.\n","\n","Complete \u003cfont color=\"orange\"\u003e`TODO-4.1-3`\u003c/font\u003e by implementing the ``top_k_decode`` function. Note that you must also perform temperature scaling within the top_k_decode function. You can run the check below to ensure that your returned tensors are the right shape."]},{"cell_type":"code","execution_count":16,"id":"c44ded0f","metadata":{"executionInfo":{"elapsed":172,"status":"ok","timestamp":1746583370396,"user":{"displayName":"Aileen Huang","userId":"01843079669161655601"},"user_tz":240},"id":"c44ded0f"},"outputs":[],"source":["output = top_k_decode(logits, temperature=0.9, k=2)\n","\n","assert output.shape == expected.shape, f\"Output tensor shape as expected. Does not guarantee correctness of code\""]},{"cell_type":"markdown","id":"5GVojSGEN_c0","metadata":{"id":"5GVojSGEN_c0"},"source":["---\n","\u003cfont color=\"orange\"\u003e Q3. What unintended consequences can the model's generative abilities have if the chosen k in top-k is too large?\n","\u003c/font\u003e"]},{"cell_type":"markdown","id":"X4_cxE0hOVF4","metadata":{"id":"X4_cxE0hOVF4"},"source":["__Answer.__\n","\n","If the chosen k in top-k is too large, just like with the large temperature scaling issue, because your range of top values to choose from expands, you can riskk choosing a relatively insignificant token from that large range of top k values."]},{"cell_type":"markdown","id":"06464615","metadata":{"id":"06464615"},"source":["Finally, we will implement our last decoding strategy -- top-p sampling. Top-p sampling accepts a hyperparamter ``p``, which is a float between 0 and 1. Instead of choosing a fixed number of top tokens, top-p sampling samples from the smallest number of tokens, ordered from highest to lowest probabolility, whose cumulative probability exceeds this threshold p.  \n","\n","Consider the example from above.\n","The sky is\n","        blue 0.5\n","        yellow 0.2\n","        overcast 0.3\n","\n","Say p=0.6. The smallest set of of tokens that satisfy the above criterion is (blue, overcast); you must only sample from these tokens. Think how you will convert the now un-normalized probabilities (blue: 0.5, overcast: 0.3) to a probability distribution to sample from.\n","\n","Remember that if p=0.2, the smallest set of of tokens that satisfy the above criterion will be (blue).\n","\n","Complete \u003cfont color=\"orange\"\u003e`TODO-4.1-4`\u003c/font\u003e by implementing the ``top_p_decode`` function. Note that you must also perform temperature scaling within the top_k_decode function. You can run the check below to ensure that your returned tensors are the right shape."]},{"cell_type":"code","execution_count":17,"id":"744c9497","metadata":{"executionInfo":{"elapsed":114,"status":"ok","timestamp":1746583370402,"user":{"displayName":"Aileen Huang","userId":"01843079669161655601"},"user_tz":240},"id":"744c9497"},"outputs":[],"source":["output = top_p_decode(logits, temperature=0.9, p = 0.6)\n","\n","assert output.shape == expected.shape, f\"Output tensor shape as expected. Does not guarantee correctness of code\""]},{"cell_type":"markdown","id":"6c1e6f27","metadata":{"id":"6c1e6f27"},"source":["### [3.2] Hey Seagull, tell me a joke! \u003csmall\u003e[↩︎](#outline)\u003c/small\u003e\n","\n","\u003e \u003cfont color=\"orange\"\u003eFile to be modified: `seagull/model/heads/seagull_lm.py`.\u003c/font\u003e\n","\n","Great! Now we have implemented all our decoding functions! We can now proceed to implementing the ``talk`` function of this assignment!\n","\n","Note that we will use the same pre-trained model from HW3 to generate text. At this point, it would be good for you to ensure that all your function implementations from HW3 are correct. Although you can proceed without checking that, it will be easy for you to debug the ``talk()`` function. This is because, if the functions in HW3 are implemented correctly AND your ``talk`` function implementation is correct, the generated text will be reasonable."]},{"cell_type":"markdown","id":"0641f795","metadata":{"id":"0641f795"},"source":["We will use the same pre-trained Seagull transformer as HW3. Remember that the Seagull model's configuration is as follows:\n","\n","* an embedding dimension of 768, divisible by 12, the number of heads,\n","* RoPE rotary encodings, instead of learned (or even absolute) positional embeddings,\n","*pre-normalization using RMS norm, as opposed to post-normalization using nn.LayerNorm, and\n","* the feed-forward net with: swish activation with GLU (SwiGLU) and a linear projection with intermediate dimension of 2,048, instead of two linear layers with intermediate dimension of 3,072.\n","\n","\n","This Seagull transformer, with 110.53 million trainable parameters (for comparison: GPT-2 small has 117 million parameters), was pre-trained on the OpenWebText dataset. First, let's load the model using the code below:"]},{"cell_type":"code","execution_count":21,"id":"228112c3","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":10198,"status":"ok","timestamp":1746583704048,"user":{"displayName":"Aileen Huang","userId":"01843079669161655601"},"user_tz":240},"id":"228112c3","outputId":"0bd636bd-bd84-4817-b8c5-39b1fcb4f439"},"outputs":[{"name":"stdout","output_type":"stream","text":["BBPETokenizer(name=seagull-bbpe, vocab_size=33264, lowercase=False, punct_behavior=contiguous, special_tokens={'\u003c|pad|\u003e': 0, '\u003c|unk|\u003e': 1, '\u003c|endoftext|\u003e': 2, '\u003c|scene|\u003e': 3, '\u003c|uncanny|\u003e': 4, '\u003c|caption|\u003e': 5, '\u003c|endofcaption|\u003e': 6})\n","model weights loaded using: \u001b[94mopenwebtext_epochs=11_model.pt\u001b[0m\n"]}],"source":["bbpe_tokenizer = BBPETokenizer()\n","bbpe_tokenizer.from_file(os.path.join(PRETRAINED_ARTEFACTS_DIR, \"tokenizer\"))\n","print(bbpe_tokenizer)\n","\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","with open(os.path.join(CONFIGS_DIR, \"train_model.yml\"), \"r\") as fp:\n","    config = yaml.safe_load(fp)\n","seagull_lm = SeagullLM(\n","    vocab_size=bbpe_tokenizer.vocab_size,\n","    padding_idx=bbpe_tokenizer.token2id(bbpe_tokenizer.pad_token),\n","    **config[\"model\"],\n",").to(device)\n","\n","pretrained_filename = \"openwebtext_epochs=11_model.pt\"\n","seagull_lm.from_pretrained(model_filepath=os.path.join(PRETRAINED_ARTEFACTS_DIR, f\"model/{pretrained_filename}\"))\n","print(f\"model weights loaded using: {colored(pretrained_filename, 'blue')}\")"]},{"cell_type":"markdown","id":"d7afd001","metadata":{"id":"d7afd001"},"source":["Okay, let's get back to implementing the ``talk`` functionality which will generate text using the pre-trained SeagullLM transformer. First, let us complete the helper function `parse_before_eos`. This helper function is called within the ``talk`` function after the for loop. Remember that the for loop generates next tokens for the given input_ids (num_samples number of inputs) till the stopping criterion is satisfied, i.e. either ``max_new_tokens`` number of tokens are generated, or the ``\u003c|endofcaption|\u003e`` token is generated for each input.\n","\n","The output of the for loop is all_input_ids, a tensor of size batch_size x seq_length. This means that inputs in the batch for which generation ended earlier (i.e. ``\u003c|endofcaption|\u003e`` already generated) will still contain generated tokens upto the length seq_length till ``\u003c|endofcaption|\u003e`` is generated for each input. The helper function `parse_before_eos` removes everything after the first ``\u003c|endofcaption|\u003e`` token in each sequence. It will be used after generation to trim extra tokens that follow the end of a caption.\n","\n","Complete ``TODO-4.2 -1`` by implenting the `parse_before_eos` function. It wil take as input a  tensor of shape `(batch_size, sequence_length)` and the eos_token_id. We will use should be the token_id corresponding to token ``\u003c|endofcaption|\u003e``. Your function should return a `List[List[int]]` where each sequence is truncated after the first `eos_token_id`.\n","\n","Run the following test case to make sure that your implementation is correct."]},{"cell_type":"code","execution_count":22,"id":"c539906f","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":43,"status":"ok","timestamp":1746583706990,"user":{"displayName":"Aileen Huang","userId":"01843079669161655601"},"user_tz":240},"id":"c539906f","outputId":"d3fcd0cc-ee52-4f9f-892e-5b1215eaa351"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[3.0, 4.0, 6.0], [3.0, 4.0, 1.0, 5.0, 6.0], [3.0, 4.0, 1.0, 5.0, 29.0]]\n"]}],"source":["from seagull.model.heads.seagull_lm import parse_before_eos\n","\n","eos_token_id = bbpe_tokenizer.token2id(END_OF_CAPTION_TOKEN)\n","all_input_ids = torch.Tensor([\n","    [3, 4, 6, 1, 5],\n","    [3, 4, 1, 5, 6],\n","    [3, 4, 1, 5, 29],\n","])\n","\n","expected = [[3, 4, 6], [3, 4, 1, 5, 6], [3, 4, 1, 5, 29]]\n","\n","output = parse_before_eos(all_input_ids, eos_token_id)\n","print(output)\n","\n","assert output == expected, \"Output of parse_before_eos as expected\""]},{"cell_type":"markdown","id":"c5f871c7","metadata":{"id":"c5f871c7"},"source":["Finally, we will complete ``TODO 4.2-2`` that implements the for loop within the ``talk`` function in ``seagull_lm.py``. To remind oureselves, this function allows for our Seagull model to \"speak\". It takes as input a tensor of input_ids, hyperparameter num_samples that specifies how many outputs to sample, hyperparameter max_new_tokens, and the decoding strategy (one of greedy_decode', 'sampling_decode', 'top_p_decode' or 'top_k_decode').\n","\n","Given these inputs, the talk function uses the Seagull model to produce up to max_new_tokens number of tokens that the model thinks is a likely continuation of the ongoing sequence (based on the context from the input input_ids).\n","\n","Let's dig into the function a bit more. Here, we assume that the input_ids always contain one data sample, i.e. batch_size = 1. Given such a input_ids tensor, we create num_samples copies of this tensor. We do the same operation on position_ids. Our all_input_ids and all_position_ids tensor is of size num_samples x seq_length. We treat each row of this \"batch\" as an independant data point during generation. This allows us to get different generated continuations for the same input.\n","\n","Within the for loop, the model performs a forward pass to get the logits for the next token prediction. This is already implemented for you. You need to implement the rest of the functionality within this for loop, particularly:\n","\n","* use the given decoding strategy to sample the next token for all inputs in the batch.\n","* append the next token to the all_input_ids tensors to create the input for the next iteration of the loop. Do the same for all_position_ids.\n","* check if all inputs in the batch have already generated the eos_token_id. If yes, break the loop.\n","\n","Once you implement your ``talk`` function, you can use the code below to generate text using the loaded pre-trained model.\n"]},{"cell_type":"code","execution_count":23,"id":"74895d8e","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":4613,"status":"ok","timestamp":1746583713952,"user":{"displayName":"Aileen Huang","userId":"01843079669161655601"},"user_tz":240},"id":"74895d8e","outputId":"e7b442b3-8e31-4b60-ef1a-c087284b4018"},"outputs":[{"data":{"text/plain":["['I was going to the store and I was going to buy my son’s shoes because it was a great idea.”\\n\\nThe company’s CEO, Jason Lewis, said that the company is “still in the early stages of the process” of making a new product.\\n\\nHe said that the company has been working hard to get the shoe ready and that it will be ready for the end of the year.\\n\\n“We’re very excited to be able to bring the shoe to market',\n"," 'I was going to the store and I saw a guy in my room. I was like, \\'What the fuck? I\\'m going to get this guy.\\'\\n\\n\"I was like, \\'You\\'re going to have to take this guy out of this store.\\'\\n\\n\"I said, \\'Oh my god, I can\\'t do this.\\' And then I went to the other room and he said, \\'You can\\'t do this. I can\\'t do this,\\'\"',\n"," 'I was going to the store and I saw a guy who was wearing a black shirt and a black shirt and I said, “You know, I’m not going to do this.” And I went, “No, I’m not going to do this.” I was like, I’m going to go and see what happens. I said, “You know, I’m not going to do this.” I just said, “No, I’m not going to do this.”\\n\\n']"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["prompt = \"I was going to the store and\"\n","\n","max_new_tokens = 100\n","num_samples = 3\n","top_k = 5\n","temperature = 0.7\n","decoding_strategy=\"top_k_decode\" # choose from the keys in DECODING_FUNCS in seagull_lm.py\n","\n","make_seagull_talk(\n","    seagull_lm=seagull_lm,\n","    bbpe_tokenizer=bbpe_tokenizer,\n","    prompt=prompt,\n","    max_new_tokens=max_new_tokens,\n","    num_samples=num_samples,\n","    decoding_strategy=\"top_k_decode\",\n","    decoding_kwargs={\"temperature\":temperature, \"k\":top_k},\n",")\n"]},{"cell_type":"markdown","id":"0f0acd5d","metadata":{"id":"0f0acd5d"},"source":["If everything is implemneted correctly, the model's generations should be grammatical and reasonable. This informs us that our pretrained model is capable of understanding language, but what about humor? Let's choose a random sample from our dataset and see if the model can generate a humorous caption. (Feel free to change the sample index below.)"]},{"cell_type":"code","execution_count":24,"id":"23ddcfa2","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":54,"status":"ok","timestamp":1746583714005,"user":{"displayName":"Aileen Huang","userId":"01843079669161655601"},"user_tz":240},"id":"23ddcfa2","outputId":"2c2952cd-8103-45f0-acdc-3d89a249dd21"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u003c|scene|\u003e A man is shaking violently. A waitress pours him another cup of coffee. A man eating a sandwich stares at them\n","at the diner. \u003c|uncanny|\u003e The man is shaking a lot due to too much coffee. \u003c|caption|\u003e\n"]}],"source":["tokenized_dataset = datasets.load_from_disk(os.path.join(ARTEFACTS_DIR, \"dataset/tokenized_dataset\"))\n","tokenized_dataset\n","\n","sample_idx = 100  # change as needed\n","\n","# Choose from the test set to ensure captions are not included.\n","prompt = tokenized_dataset[\"test\"][sample_idx][\"text\"]\n","print(\"\\n\".join(text_wrapper.wrap(prompt)))\n"]},{"cell_type":"code","execution_count":25,"id":"99dc2964","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":1824,"status":"ok","timestamp":1746583715829,"user":{"displayName":"Aileen Huang","userId":"01843079669161655601"},"user_tz":240},"id":"99dc2964","outputId":"35d4e2f6-3a5b-4d64-840c-3c40833d6870"},"outputs":[{"data":{"text/plain":["['\u003c|scene|\u003e A man is shaking violently. A waitress pours him another cup of coffee. A man eating a sandwich stares at them at the diner. \u003c|uncanny|\u003e The man is shaking a lot due to too much coffee. \u003c|caption|\u003e\\n\\n\\n\\n\\nA man is shaking violently at the diner. A woman looks at him and says, \"Hey, you\\'re a woman.\" .\"\\n\\nThe woman says, \"Hey, you\\'re a woman. I\\'m a woman. I\\'ve had enough. I\\'ve had enough. I\\'ve had enough. I\\'ve had enough.\"\\n\\nThe woman says, \"Hey, \"Hey, you\\'s okay,',\n"," '\u003c|scene|\u003e A man is shaking violently. A waitress pours him another cup of coffee. A man eating a sandwich stares at them at the diner. \u003c|uncanny|\u003e The man is shaking a lot due to too much coffee. \u003c|caption|\u003e A man is shaking violently at the restaurant. A woman walks past. ĐA man is shaking violently at the restaurant. ĐA man is shaking violently at the restaurant. A woman walks past. ĐA man is shaking violently at the restaurant. ĐA man is shaking violently at the restaurant. ĐA man is shaking violently at the restaurant. ĐA man is shĐA man is shaking violently',\n"," '\u003c|scene|\u003e A man is shaking violently. A waitress pours him another cup of coffee. A man eating a sandwich stares at them at the diner. \u003c|uncanny|\u003e The man is shaking a lot due to too much coffee. \u003c|caption|\u003e A woman who is wearing a white jacket is shaking violently. A woman drinking a coffee. ugg The woman is shaking violently. ugg A woman who is wearing a white jacket is shaking violently. ugg A woman who is wearing a white jacket is shaking violently. ugg A woman who is wearing a white jacket is shaking violently. ugg A woman who is wearing a white jacket is shaking violently. ugg A woman is shaking violly. ugg A']"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["make_seagull_talk(\n","    seagull_lm=seagull_lm,\n","    bbpe_tokenizer=bbpe_tokenizer,\n","    prompt=prompt,\n","    max_new_tokens=max_new_tokens,\n","    num_samples=num_samples,\n","    decoding_strategy=\"top_k_decode\",\n","    decoding_kwargs={\"temperature\":temperature, \"k\":top_k},\n",")"]},{"cell_type":"markdown","id":"a8382d6f","metadata":{"id":"a8382d6f"},"source":["As expected, the model isn't picking up on the nature of the task, i.e., the objective of generating humorous captions; this could simply be because it is unlikely that the model is attuned to our template processing using control codes. Other than that, a few quick observations here:\n","\n","* the model doesn't generate \u003c|endoftext|\u003e token anywhere in the 100 new tokens (think why this is unsurprising)—this is a potential problem for us for two reasons: 1) our task is to generate relatively short captions, and 2) our model has a maximum context length of 128;\n","* despite being trained on humor short texts, the model struggles with even understanding humor.\n","In the next subsection, we will proceed to finetune the pretrained model on our captions dataset, and analyze the finetuned model's generative capabilities."]},{"cell_type":"markdown","id":"uhImp1ypehFx","metadata":{"id":"uhImp1ypehFx"},"source":["\u003ca name=\"sec5\"\u003e\u003c/a\u003e\n","### [5] Finetuning the Seagull to \"understand\" humor \u003csmall\u003e[↩︎](#outline)\u003c/small\u003e\n","\n","Let us use ``train_seagull.py`` script to finetune the pretrained model. Given that the model already seems to have adequate language generation abilities, in theory, it should be sufficient to finetune the pretrained model for just 1-2 epochs.\n","\n","Additional notes. The optimizer and learning rate hyper-hyperparameters are in ``scripts/configs/train_model.yml``, see keys: optimizer/finetuning and lr_scheduler/finetuning respectively; the training hyperparameters are in the same config file, see keys: trainer/finetuning and train_and_eval/finetuning. Feel free to experiment with any of these.\n","\n","We provide a default setting of 16 batch size and 1 epoch for finetuning; feel free to adjust these as needed. Under this setting, it takes about ~22mins to finetune the model and ~3mins to run validation on a T4 (Colab-default) GPU."]},{"cell_type":"code","execution_count":26,"id":"_WiHZcWlueZn","metadata":{"executionInfo":{"elapsed":37,"status":"ok","timestamp":1746583715877,"user":{"displayName":"Aileen Huang","userId":"01843079669161655601"},"user_tz":240},"id":"_WiHZcWlueZn"},"outputs":[],"source":["batch_size = 16\n","num_epochs = 2\n","\n","pretrained_filename = \"openwebtext_epochs=11_model.pt\"\n","pretrained_model_filepath = os.path.join(PRETRAINED_ARTEFACTS_DIR, f\"model/{pretrained_filename}\")\n","experiment_name = f\"seagull_captions_batch-size={batch_size}_epochs={num_epochs}\"\n","\n","import os\n","os.environ[\"PYTHONPATH\"] = \"/content/drive/MyDrive/cs4740_programming_assignments/hw4-release\" ##set this to the right path\n"]},{"cell_type":"code","execution_count":27,"id":"81f0f323","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":2535662,"status":"ok","timestamp":1746586251540,"user":{"displayName":"Aileen Huang","userId":"01843079669161655601"},"user_tz":240},"id":"81f0f323","outputId":"39be9213-ef48-4379-9cb6-d4c2a6c7b9f5"},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------------------------------------------------------+------------+---------------+\n","|                           module                          | num_params | requires_grad |\n","+-----------------------------------------------------------+------------+---------------+\n","|          seagull.embedding.token_embedding.weight         |  25546752  |      True     |\n","|   seagull.transformer_layers.0.mha.qkv_transform.weight   |  1769472   |      True     |\n","|    seagull.transformer_layers.0.mha.qkv_transform.bias    |    2304    |      True     |\n","|  seagull.transformer_layers.0.mha.output_transform.weight |   589824   |      True     |\n","|   seagull.transformer_layers.0.mha.output_transform.bias  |    768     |      True     |\n","|        seagull.transformer_layers.0.ffn.glu.weight        |  3145728   |      True     |\n","|       seagull.transformer_layers.0.ffn.linear.weight      |  1572864   |      True     |\n","|        seagull.transformer_layers.0.layer_norm.gain       |    768     |      True     |\n","|   seagull.transformer_layers.1.mha.qkv_transform.weight   |  1769472   |      True     |\n","|    seagull.transformer_layers.1.mha.qkv_transform.bias    |    2304    |      True     |\n","|  seagull.transformer_layers.1.mha.output_transform.weight |   589824   |      True     |\n","|   seagull.transformer_layers.1.mha.output_transform.bias  |    768     |      True     |\n","|        seagull.transformer_layers.1.ffn.glu.weight        |  3145728   |      True     |\n","|       seagull.transformer_layers.1.ffn.linear.weight      |  1572864   |      True     |\n","|        seagull.transformer_layers.1.layer_norm.gain       |    768     |      True     |\n","|   seagull.transformer_layers.2.mha.qkv_transform.weight   |  1769472   |      True     |\n","|    seagull.transformer_layers.2.mha.qkv_transform.bias    |    2304    |      True     |\n","|  seagull.transformer_layers.2.mha.output_transform.weight |   589824   |      True     |\n","|   seagull.transformer_layers.2.mha.output_transform.bias  |    768     |      True     |\n","|        seagull.transformer_layers.2.ffn.glu.weight        |  3145728   |      True     |\n","|       seagull.transformer_layers.2.ffn.linear.weight      |  1572864   |      True     |\n","|        seagull.transformer_layers.2.layer_norm.gain       |    768     |      True     |\n","|   seagull.transformer_layers.3.mha.qkv_transform.weight   |  1769472   |      True     |\n","|    seagull.transformer_layers.3.mha.qkv_transform.bias    |    2304    |      True     |\n","|  seagull.transformer_layers.3.mha.output_transform.weight |   589824   |      True     |\n","|   seagull.transformer_layers.3.mha.output_transform.bias  |    768     |      True     |\n","|        seagull.transformer_layers.3.ffn.glu.weight        |  3145728   |      True     |\n","|       seagull.transformer_layers.3.ffn.linear.weight      |  1572864   |      True     |\n","|        seagull.transformer_layers.3.layer_norm.gain       |    768     |      True     |\n","|   seagull.transformer_layers.4.mha.qkv_transform.weight   |  1769472   |      True     |\n","|    seagull.transformer_layers.4.mha.qkv_transform.bias    |    2304    |      True     |\n","|  seagull.transformer_layers.4.mha.output_transform.weight |   589824   |      True     |\n","|   seagull.transformer_layers.4.mha.output_transform.bias  |    768     |      True     |\n","|        seagull.transformer_layers.4.ffn.glu.weight        |  3145728   |      True     |\n","|       seagull.transformer_layers.4.ffn.linear.weight      |  1572864   |      True     |\n","|        seagull.transformer_layers.4.layer_norm.gain       |    768     |      True     |\n","|   seagull.transformer_layers.5.mha.qkv_transform.weight   |  1769472   |      True     |\n","|    seagull.transformer_layers.5.mha.qkv_transform.bias    |    2304    |      True     |\n","|  seagull.transformer_layers.5.mha.output_transform.weight |   589824   |      True     |\n","|   seagull.transformer_layers.5.mha.output_transform.bias  |    768     |      True     |\n","|        seagull.transformer_layers.5.ffn.glu.weight        |  3145728   |      True     |\n","|       seagull.transformer_layers.5.ffn.linear.weight      |  1572864   |      True     |\n","|        seagull.transformer_layers.5.layer_norm.gain       |    768     |      True     |\n","|   seagull.transformer_layers.6.mha.qkv_transform.weight   |  1769472   |      True     |\n","|    seagull.transformer_layers.6.mha.qkv_transform.bias    |    2304    |      True     |\n","|  seagull.transformer_layers.6.mha.output_transform.weight |   589824   |      True     |\n","|   seagull.transformer_layers.6.mha.output_transform.bias  |    768     |      True     |\n","|        seagull.transformer_layers.6.ffn.glu.weight        |  3145728   |      True     |\n","|       seagull.transformer_layers.6.ffn.linear.weight      |  1572864   |      True     |\n","|        seagull.transformer_layers.6.layer_norm.gain       |    768     |      True     |\n","|   seagull.transformer_layers.7.mha.qkv_transform.weight   |  1769472   |      True     |\n","|    seagull.transformer_layers.7.mha.qkv_transform.bias    |    2304    |      True     |\n","|  seagull.transformer_layers.7.mha.output_transform.weight |   589824   |      True     |\n","|   seagull.transformer_layers.7.mha.output_transform.bias  |    768     |      True     |\n","|        seagull.transformer_layers.7.ffn.glu.weight        |  3145728   |      True     |\n","|       seagull.transformer_layers.7.ffn.linear.weight      |  1572864   |      True     |\n","|        seagull.transformer_layers.7.layer_norm.gain       |    768     |      True     |\n","|   seagull.transformer_layers.8.mha.qkv_transform.weight   |  1769472   |      True     |\n","|    seagull.transformer_layers.8.mha.qkv_transform.bias    |    2304    |      True     |\n","|  seagull.transformer_layers.8.mha.output_transform.weight |   589824   |      True     |\n","|   seagull.transformer_layers.8.mha.output_transform.bias  |    768     |      True     |\n","|        seagull.transformer_layers.8.ffn.glu.weight        |  3145728   |      True     |\n","|       seagull.transformer_layers.8.ffn.linear.weight      |  1572864   |      True     |\n","|        seagull.transformer_layers.8.layer_norm.gain       |    768     |      True     |\n","|   seagull.transformer_layers.9.mha.qkv_transform.weight   |  1769472   |      True     |\n","|    seagull.transformer_layers.9.mha.qkv_transform.bias    |    2304    |      True     |\n","|  seagull.transformer_layers.9.mha.output_transform.weight |   589824   |      True     |\n","|   seagull.transformer_layers.9.mha.output_transform.bias  |    768     |      True     |\n","|        seagull.transformer_layers.9.ffn.glu.weight        |  3145728   |      True     |\n","|       seagull.transformer_layers.9.ffn.linear.weight      |  1572864   |      True     |\n","|        seagull.transformer_layers.9.layer_norm.gain       |    768     |      True     |\n","|   seagull.transformer_layers.10.mha.qkv_transform.weight  |  1769472   |      True     |\n","|    seagull.transformer_layers.10.mha.qkv_transform.bias   |    2304    |      True     |\n","| seagull.transformer_layers.10.mha.output_transform.weight |   589824   |      True     |\n","|  seagull.transformer_layers.10.mha.output_transform.bias  |    768     |      True     |\n","|        seagull.transformer_layers.10.ffn.glu.weight       |  3145728   |      True     |\n","|      seagull.transformer_layers.10.ffn.linear.weight      |  1572864   |      True     |\n","|       seagull.transformer_layers.10.layer_norm.gain       |    768     |      True     |\n","|   seagull.transformer_layers.11.mha.qkv_transform.weight  |  1769472   |      True     |\n","|    seagull.transformer_layers.11.mha.qkv_transform.bias   |    2304    |      True     |\n","| seagull.transformer_layers.11.mha.output_transform.weight |   589824   |      True     |\n","|  seagull.transformer_layers.11.mha.output_transform.bias  |    768     |      True     |\n","|        seagull.transformer_layers.11.ffn.glu.weight       |  3145728   |      True     |\n","|      seagull.transformer_layers.11.ffn.linear.weight      |  1572864   |      True     |\n","|       seagull.transformer_layers.11.layer_norm.gain       |    768     |      True     |\n","|                  seagull.layer_norm.gain                  |    768     |      True     |\n","+-----------------------------------------------------------+------------+---------------+\n","total trainable params: 110.53M\n","/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","\u001b[2K/content/drive/MyDrive/cs4740_programming_assignments/hw4-release/seagull/data_p\n","rocessing/sequence_sampler.py:50: UserWarning: To copy construct from a tensor, \n","it is recommended to use sourceTensor.clone().detach() or \n","sourceTensor.clone().detach().requires_grad_(True), rather than \n","torch.tensor(sourceTensor).\n","  labels = torch.tensor(input_ids[1:])\n","\u001b[2K/content/drive/MyDrive/cs4740_programming_assignments/hw4-release/seagull/data_p\n","rocessing/sequence_sampler.py:51: UserWarning: To copy construct from a tensor, \n","it is recommended to use sourceTensor.clone().detach() or \n","sourceTensor.clone().detach().requires_grad_(True), rather than \n","torch.tensor(sourceTensor).\n","  input_ids = torch.tensor(input_ids[:-1])\n","\u001b[2K/content/drive/MyDrive/cs4740_programming_assignments/hw4-release/seagull/data_p\n","rocessing/sequence_sampler.py:50: UserWarning: To copy construct from a tensor, \n","it is recommended to use sourceTensor.clone().detach() or \n","sourceTensor.clone().detach().requires_grad_(True), rather than \n","torch.tensor(sourceTensor).\n","  labels = torch.tensor(input_ids[1:])\n","\u001b[2K/content/drive/MyDrive/cs4740_programming_assignments/hw4-release/seagull/data_p\n","rocessing/sequence_sampler.py:50: UserWarning: To copy construct from a tensor, \n","it is recommended to use sourceTensor.clone().detach() or \n","sourceTensor.clone().detach().requires_grad_(True), rather than \n","torch.tensor(sourceTensor).\n","  labels = torch.tensor(input_ids[1:])\n","\u001b[2K/content/drive/MyDrive/cs4740_programming_assignments/hw4-release/seagull/data_p\n","rocessing/sequence_sampler.py:50: UserWarning: To copy construct from a tensor, \n","it is recommended to use sourceTensor.clone().detach() or \n","sourceTensor.clone().detach().requires_grad_(True), rather than \n","torch.tensor(sourceTensor).\n","  labels = torch.tensor(input_ids[1:])\n","\u001b[2K/content/drive/MyDrive/cs4740_programming_assignments/hw4-release/seagull/data_p\n","rocessing/sequence_sampler.py:51: UserWarning: To copy construct from a tensor, \n","it is recommended to use sourceTensor.clone().detach() or \n","sourceTensor.clone().detach().requires_grad_(True), rather than \n","torch.tensor(sourceTensor).\n","  input_ids = torch.tensor(input_ids[:-1])\n","\u001b[2K/content/drive/MyDrive/cs4740_programming_assignments/hw4-release/seagull/data_p\n","rocessing/sequence_sampler.py:51: UserWarning: To copy construct from a tensor, \n","it is recommended to use sourceTensor.clone().detach() or \n","sourceTensor.clone().detach().requires_grad_(True), rather than \n","torch.tensor(sourceTensor).\n","  input_ids = torch.tensor(input_ids[:-1])\n","\u001b[2K/content/drive/MyDrive/cs4740_programming_assignments/hw4-release/seagull/data_p\n","rocessing/sequence_sampler.py:51: UserWarning: To copy construct from a tensor, \n","it is recommended to use sourceTensor.clone().detach() or \n","sourceTensor.clone().detach().requires_grad_(True), rather than \n","torch.tensor(sourceTensor).\n","  input_ids = torch.tensor(input_ids[:-1])\n","\u001b[2K/content/drive/MyDrive/cs4740_programming_assignments/hw4-release/seagull/data_p\n","rocessing/sequence_sampler.py:52: UserWarning: To copy construct from a tensor, \n","it is recommended to use sourceTensor.clone().detach() or \n","sourceTensor.clone().detach().requires_grad_(True), rather than \n","torch.tensor(sourceTensor).\n","  padding_mask = torch.tensor(padding_mask[:-1])\n","\u001b[2K/content/drive/MyDrive/cs4740_programming_assignments/hw4-release/seagull/data_p\n","rocessing/sequence_sampler.py:52: UserWarning: To copy construct from a tensor, \n","it is recommended to use sourceTensor.clone().detach() or \n","sourceTensor.clone().detach().requires_grad_(True), rather than \n","torch.tensor(sourceTensor).\n","  padding_mask = torch.tensor(padding_mask[:-1])\n","\u001b[2K/content/drive/MyDrive/cs4740_programming_assignments/hw4-release/seagull/data_p\n","rocessing/sequence_sampler.py:52: UserWarning: To copy construct from a tensor, \n","it is recommended to use sourceTensor.clone().detach() or \n","sourceTensor.clone().detach().requires_grad_(True), rather than \n","torch.tensor(sourceTensor).\n","  padding_mask = torch.tensor(padding_mask[:-1])\n","\u001b[2K/content/drive/MyDrive/cs4740_programming_assignments/hw4-release/seagull/data_p\n","rocessing/sequence_sampler.py:52: UserWarning: To copy construct from a tensor, \n","it is recommended to use sourceTensor.clone().detach() or \n","sourceTensor.clone().detach().requires_grad_(True), rather than \n","torch.tensor(sourceTensor).\n","  padding_mask = torch.tensor(padding_mask[:-1])\n","\u001b[2Ktrain (epoch: 0) \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[33m0:20:00\u001b[0m\n","\u001b[2K/content/drive/MyDrive/cs4740_programming_assignments/hw4-release/seagull/data_p\n","rocessing/sequence_sampler.py:50: UserWarning: To copy construct from a tensor, \n","it is recommended to use sourceTensor.clone().detach() or \n","sourceTensor.clone().detach().requires_grad_(True), rather than \n","torch.tensor(sourceTensor).\n","  labels = torch.tensor(input_ids[1:])\n","\u001b[2K/content/drive/MyDrive/cs4740_programming_assignments/hw4-release/seagull/data_p\n","rocessing/sequence_sampler.py:51: UserWarning: To copy construct from a tensor, \n","it is recommended to use sourceTensor.clone().detach() or \n","sourceTensor.clone().detach().requires_grad_(True), rather than \n","torch.tensor(sourceTensor).\n","  input_ids = torch.tensor(input_ids[:-1])\n","\u001b[2K/content/drive/MyDrive/cs4740_programming_assignments/hw4-release/seagull/data_p\n","rocessing/sequence_sampler.py:50: UserWarning: To copy construct from a tensor, \n","it is recommended to use sourceTensor.clone().detach() or \n","sourceTensor.clone().detach().requires_grad_(True), rather than \n","torch.tensor(sourceTensor).\n","  labels = torch.tensor(input_ids[1:])\n","\u001b[2K/content/drive/MyDrive/cs4740_programming_assignments/hw4-release/seagull/data_p\n","rocessing/sequence_sampler.py:51: UserWarning: To copy construct from a tensor, \n","it is recommended to use sourceTensor.clone().detach() or \n","sourceTensor.clone().detach().requires_grad_(True), rather than \n","torch.tensor(sourceTensor).\n","  input_ids = torch.tensor(input_ids[:-1])\n","\u001b[2K/content/drive/MyDrive/cs4740_programming_assignments/hw4-release/seagull/data_p\n","rocessing/sequence_sampler.py:52: UserWarning: To copy construct from a tensor, \n","it is recommended to use sourceTensor.clone().detach() or \n","sourceTensor.clone().detach().requires_grad_(True), rather than \n","torch.tensor(sourceTensor).\n","  padding_mask = torch.tensor(padding_mask[:-1])\n","\u001b[2K/content/drive/MyDrive/cs4740_programming_assignments/hw4-release/seagull/data_p\n","rocessing/sequence_sampler.py:52: UserWarning: To copy construct from a tensor, \n","it is recommended to use sourceTensor.clone().detach() or \n","sourceTensor.clone().detach().requires_grad_(True), rather than \n","torch.tensor(sourceTensor).\n","  padding_mask = torch.tensor(padding_mask[:-1])\n","\u001b[2K/content/drive/MyDrive/cs4740_programming_assignments/hw4-release/seagull/data_p\n","rocessing/sequence_sampler.py:50: UserWarning: To copy construct from a tensor, \n","it is recommended to use sourceTensor.clone().detach() or \n","sourceTensor.clone().detach().requires_grad_(True), rather than \n","torch.tensor(sourceTensor).\n","  labels = torch.tensor(input_ids[1:])\n","\u001b[2K/content/drive/MyDrive/cs4740_programming_assignments/hw4-release/seagull/data_p\n","rocessing/sequence_sampler.py:50: UserWarning: To copy construct from a tensor, \n","it is recommended to use sourceTensor.clone().detach() or \n","sourceTensor.clone().detach().requires_grad_(True), rather than \n","torch.tensor(sourceTensor).\n","  labels = torch.tensor(input_ids[1:])\n","\u001b[2K/content/drive/MyDrive/cs4740_programming_assignments/hw4-release/seagull/data_p\n","rocessing/sequence_sampler.py:51: UserWarning: To copy construct from a tensor, \n","it is recommended to use sourceTensor.clone().detach() or \n","sourceTensor.clone().detach().requires_grad_(True), rather than \n","torch.tensor(sourceTensor).\n","  input_ids = torch.tensor(input_ids[:-1])\n","\u001b[2K/content/drive/MyDrive/cs4740_programming_assignments/hw4-release/seagull/data_p\n","rocessing/sequence_sampler.py:52: UserWarning: To copy construct from a tensor, \n","it is recommended to use sourceTensor.clone().detach() or \n","sourceTensor.clone().detach().requires_grad_(True), rather than \n","torch.tensor(sourceTensor).\n","  padding_mask = torch.tensor(padding_mask[:-1])\n","\u001b[2K/content/drive/MyDrive/cs4740_programming_assignments/hw4-release/seagull/data_p\n","rocessing/sequence_sampler.py:51: UserWarning: To copy construct from a tensor, \n","it is recommended to use sourceTensor.clone().detach() or \n","sourceTensor.clone().detach().requires_grad_(True), rather than \n","torch.tensor(sourceTensor).\n","  input_ids = torch.tensor(input_ids[:-1])\n","\u001b[2K/content/drive/MyDrive/cs4740_programming_assignments/hw4-release/seagull/data_p\n","rocessing/sequence_sampler.py:52: UserWarning: To copy construct from a tensor, \n","it is recommended to use sourceTensor.clone().detach() or \n","sourceTensor.clone().detach().requires_grad_(True), rather than \n","torch.tensor(sourceTensor).\n","  padding_mask = torch.tensor(padding_mask[:-1])\n","\u001b[2Kval (epoch: 0) \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[33m0:00:45\u001b[0m\n","\u001b[2K/content/drive/MyDrive/cs4740_programming_assignments/hw4-release/seagull/data_p\n","rocessing/sequence_sampler.py:50: UserWarning: To copy construct from a tensor, \n","it is recommended to use sourceTensor.clone().detach() or \n","sourceTensor.clone().detach().requires_grad_(True), rather than \n","torch.tensor(sourceTensor).\n","  labels = torch.tensor(input_ids[1:])\n","\u001b[2K/content/drive/MyDrive/cs4740_programming_assignments/hw4-release/seagull/data_p\n","rocessing/sequence_sampler.py:51: UserWarning: To copy construct from a tensor, \n","it is recommended to use sourceTensor.clone().detach() or \n","sourceTensor.clone().detach().requires_grad_(True), rather than \n","torch.tensor(sourceTensor).\n","  input_ids = torch.tensor(input_ids[:-1])\n","\u001b[2K/content/drive/MyDrive/cs4740_programming_assignments/hw4-release/seagull/data_p\n","rocessing/sequence_sampler.py:50: UserWarning: To copy construct from a tensor, \n","it is recommended to use sourceTensor.clone().detach() or \n","sourceTensor.clone().detach().requires_grad_(True), rather than \n","torch.tensor(sourceTensor).\n","  labels = torch.tensor(input_ids[1:])\n","\u001b[2K/content/drive/MyDrive/cs4740_programming_assignments/hw4-release/seagull/data_p\n","rocessing/sequence_sampler.py:50: UserWarning: To copy construct from a tensor, \n","it is recommended to use sourceTensor.clone().detach() or \n","sourceTensor.clone().detach().requires_grad_(True), rather than \n","torch.tensor(sourceTensor).\n","  labels = torch.tensor(input_ids[1:])\n","\u001b[2K/content/drive/MyDrive/cs4740_programming_assignments/hw4-release/seagull/data_p\n","rocessing/sequence_sampler.py:52: UserWarning: To copy construct from a tensor, \n","it is recommended to use sourceTensor.clone().detach() or \n","sourceTensor.clone().detach().requires_grad_(True), rather than \n","torch.tensor(sourceTensor).\n","  padding_mask = torch.tensor(padding_mask[:-1])\n","\u001b[2K/content/drive/MyDrive/cs4740_programming_assignments/hw4-release/seagull/data_p\n","rocessing/sequence_sampler.py:51: UserWarning: To copy construct from a tensor, \n","it is recommended to use sourceTensor.clone().detach() or \n","sourceTensor.clone().detach().requires_grad_(True), rather than \n","torch.tensor(sourceTensor).\n","  input_ids = torch.tensor(input_ids[:-1])\n","\u001b[2K/content/drive/MyDrive/cs4740_programming_assignments/hw4-release/seagull/data_p\n","rocessing/sequence_sampler.py:51: UserWarning: To copy construct from a tensor, \n","it is recommended to use sourceTensor.clone().detach() or \n","sourceTensor.clone().detach().requires_grad_(True), rather than \n","torch.tensor(sourceTensor).\n","  input_ids = torch.tensor(input_ids[:-1])\n","\u001b[2K/content/drive/MyDrive/cs4740_programming_assignments/hw4-release/seagull/data_p\n","rocessing/sequence_sampler.py:52: UserWarning: To copy construct from a tensor, \n","it is recommended to use sourceTensor.clone().detach() or \n","sourceTensor.clone().detach().requires_grad_(True), rather than \n","torch.tensor(sourceTensor).\n","  padding_mask = torch.tensor(padding_mask[:-1])\n","\u001b[2K/content/drive/MyDrive/cs4740_programming_assignments/hw4-release/seagull/data_p\n","rocessing/sequence_sampler.py:52: UserWarning: To copy construct from a tensor, \n","it is recommended to use sourceTensor.clone().detach() or \n","sourceTensor.clone().detach().requires_grad_(True), rather than \n","torch.tensor(sourceTensor).\n","  padding_mask = torch.tensor(padding_mask[:-1])\n","\u001b[2K/content/drive/MyDrive/cs4740_programming_assignments/hw4-release/seagull/data_p\n","rocessing/sequence_sampler.py:50: UserWarning: To copy construct from a tensor, \n","it is recommended to use sourceTensor.clone().detach() or \n","sourceTensor.clone().detach().requires_grad_(True), rather than \n","torch.tensor(sourceTensor).\n","  labels = torch.tensor(input_ids[1:])\n","\u001b[2K/content/drive/MyDrive/cs4740_programming_assignments/hw4-release/seagull/data_p\n","rocessing/sequence_sampler.py:51: UserWarning: To copy construct from a tensor, \n","it is recommended to use sourceTensor.clone().detach() or \n","sourceTensor.clone().detach().requires_grad_(True), rather than \n","torch.tensor(sourceTensor).\n","  input_ids = torch.tensor(input_ids[:-1])\n","\u001b[2K/content/drive/MyDrive/cs4740_programming_assignments/hw4-release/seagull/data_p\n","rocessing/sequence_sampler.py:52: UserWarning: To copy construct from a tensor, \n","it is recommended to use sourceTensor.clone().detach() or \n","sourceTensor.clone().detach().requires_grad_(True), rather than \n","torch.tensor(sourceTensor).\n","  padding_mask = torch.tensor(padding_mask[:-1])\n","\u001b[2Ktrain (epoch: 1) \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[33m0:20:06\u001b[0m\n","\u001b[2K/content/drive/MyDrive/cs4740_programming_assignments/hw4-release/seagull/data_p\n","rocessing/sequence_sampler.py:50: UserWarning: To copy construct from a tensor, \n","it is recommended to use sourceTensor.clone().detach() or \n","sourceTensor.clone().detach().requires_grad_(True), rather than \n","torch.tensor(sourceTensor).\n","  labels = torch.tensor(input_ids[1:])\n","\u001b[2K/content/drive/MyDrive/cs4740_programming_assignments/hw4-release/seagull/data_p\n","rocessing/sequence_sampler.py:51: UserWarning: To copy construct from a tensor, \n","it is recommended to use sourceTensor.clone().detach() or \n","sourceTensor.clone().detach().requires_grad_(True), rather than \n","torch.tensor(sourceTensor).\n","  input_ids = torch.tensor(input_ids[:-1])\n","\u001b[2K/content/drive/MyDrive/cs4740_programming_assignments/hw4-release/seagull/data_p\n","rocessing/sequence_sampler.py:50: UserWarning: To copy construct from a tensor, \n","it is recommended to use sourceTensor.clone().detach() or \n","sourceTensor.clone().detach().requires_grad_(True), rather than \n","torch.tensor(sourceTensor).\n","  labels = torch.tensor(input_ids[1:])\n","\u001b[2K/content/drive/MyDrive/cs4740_programming_assignments/hw4-release/seagull/data_p\n","rocessing/sequence_sampler.py:52: UserWarning: To copy construct from a tensor, \n","it is recommended to use sourceTensor.clone().detach() or \n","sourceTensor.clone().detach().requires_grad_(True), rather than \n","torch.tensor(sourceTensor).\n","  padding_mask = torch.tensor(padding_mask[:-1])\n","\u001b[2K/content/drive/MyDrive/cs4740_programming_assignments/hw4-release/seagull/data_p\n","rocessing/sequence_sampler.py:51: UserWarning: To copy construct from a tensor, \n","it is recommended to use sourceTensor.clone().detach() or \n","sourceTensor.clone().detach().requires_grad_(True), rather than \n","torch.tensor(sourceTensor).\n","  input_ids = torch.tensor(input_ids[:-1])\n","\u001b[2K/content/drive/MyDrive/cs4740_programming_assignments/hw4-release/seagull/data_p\n","rocessing/sequence_sampler.py:50: UserWarning: To copy construct from a tensor, \n","it is recommended to use sourceTensor.clone().detach() or \n","sourceTensor.clone().detach().requires_grad_(True), rather than \n","torch.tensor(sourceTensor).\n","  labels = torch.tensor(input_ids[1:])\n","\u001b[2K/content/drive/MyDrive/cs4740_programming_assignments/hw4-release/seagull/data_p\n","rocessing/sequence_sampler.py:50: UserWarning: To copy construct from a tensor, \n","it is recommended to use sourceTensor.clone().detach() or \n","sourceTensor.clone().detach().requires_grad_(True), rather than \n","torch.tensor(sourceTensor).\n","  labels = torch.tensor(input_ids[1:])\n","\u001b[2K/content/drive/MyDrive/cs4740_programming_assignments/hw4-release/seagull/data_p\n","rocessing/sequence_sampler.py:51: UserWarning: To copy construct from a tensor, \n","it is recommended to use sourceTensor.clone().detach() or \n","sourceTensor.clone().detach().requires_grad_(True), rather than \n","torch.tensor(sourceTensor).\n","  input_ids = torch.tensor(input_ids[:-1])\n","\u001b[2K/content/drive/MyDrive/cs4740_programming_assignments/hw4-release/seagull/data_p\n","rocessing/sequence_sampler.py:52: UserWarning: To copy construct from a tensor, \n","it is recommended to use sourceTensor.clone().detach() or \n","sourceTensor.clone().detach().requires_grad_(True), rather than \n","torch.tensor(sourceTensor).\n","  padding_mask = torch.tensor(padding_mask[:-1])\n","\u001b[2K/content/drive/MyDrive/cs4740_programming_assignments/hw4-release/seagull/data_p\n","rocessing/sequence_sampler.py:52: UserWarning: To copy construct from a tensor, \n","it is recommended to use sourceTensor.clone().detach() or \n","sourceTensor.clone().detach().requires_grad_(True), rather than \n","torch.tensor(sourceTensor).\n","  padding_mask = torch.tensor(padding_mask[:-1])\n","\u001b[2K/content/drive/MyDrive/cs4740_programming_assignments/hw4-release/seagull/data_p\n","rocessing/sequence_sampler.py:51: UserWarning: To copy construct from a tensor, \n","it is recommended to use sourceTensor.clone().detach() or \n","sourceTensor.clone().detach().requires_grad_(True), rather than \n","torch.tensor(sourceTensor).\n","  input_ids = torch.tensor(input_ids[:-1])\n","\u001b[2K/content/drive/MyDrive/cs4740_programming_assignments/hw4-release/seagull/data_p\n","rocessing/sequence_sampler.py:52: UserWarning: To copy construct from a tensor, \n","it is recommended to use sourceTensor.clone().detach() or \n","sourceTensor.clone().detach().requires_grad_(True), rather than \n","torch.tensor(sourceTensor).\n","  padding_mask = torch.tensor(padding_mask[:-1])\n","\u001b[2Kval (epoch: 1) \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[33m0:00:46\u001b[0m\n","\u001b[?25h"]}],"source":["!python3 scripts/train_model.py \\\n","    --config-path={os.path.join(CONFIGS_DIR, \"train_model.yml\")} \\\n","    --basepath-to-tokenized-dataset={os.path.join(ARTEFACTS_DIR, \"dataset/tokenized_dataset\")} \\\n","    --tokenizer-basepath={os.path.join(PRETRAINED_ARTEFACTS_DIR, \"tokenizer\")} \\\n","    --basepath-to-store-results={os.path.join(ARTEFACTS_DIR, \"experiments\")} \\\n","    --batch-size={batch_size} \\\n","    --num-epochs={num_epochs} \\\n","    --pretrained-checkpoint-or-model-filepath={pretrained_model_filepath} \\\n","    --experiment-name={experiment_name}\n"]},{"cell_type":"markdown","id":"9d9bca8c","metadata":{"id":"9d9bca8c"},"source":["Once the model finetuning completes running, load the best model (or the only model) based on the observed validation performance measured through loss or perplexity. Set the best_epoch below to reflect the epoch (zero-indexed) that resulted in the best model.\n","\n","Tip. Step and epoch-level metrics for both training/finetuning and validation, and model checkpoints are written to artefacts/experiments/\u003cexperiment-name\u003e. Also, if you wish to continue finetuning from an already finetuned checkpoint, modify the value of --pretrained-checkpoint-or-model-filepath in the above command above accordingly."]},{"cell_type":"code","execution_count":28,"id":"6e1a950f","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":9223,"status":"ok","timestamp":1746586260764,"user":{"displayName":"Aileen Huang","userId":"01843079669161655601"},"user_tz":240},"id":"6e1a950f","outputId":"a98276e7-1623-493a-f409-813f2628df70"},"outputs":[{"name":"stdout","output_type":"stream","text":["loading model from: \u001b[94mseagull_captions_batch-size=16_epochs=2/checkpoints/checkpoint_0.ckpt\u001b[0m\n","+-----------------------------------------------------------+------------+---------------+\n","|                           module                          | num_params | requires_grad |\n","+-----------------------------------------------------------+------------+---------------+\n","|          seagull.embedding.token_embedding.weight         |  25546752  |      True     |\n","|   seagull.transformer_layers.0.mha.qkv_transform.weight   |  1769472   |      True     |\n","|    seagull.transformer_layers.0.mha.qkv_transform.bias    |    2304    |      True     |\n","|  seagull.transformer_layers.0.mha.output_transform.weight |   589824   |      True     |\n","|   seagull.transformer_layers.0.mha.output_transform.bias  |    768     |      True     |\n","|        seagull.transformer_layers.0.ffn.glu.weight        |  3145728   |      True     |\n","|       seagull.transformer_layers.0.ffn.linear.weight      |  1572864   |      True     |\n","|        seagull.transformer_layers.0.layer_norm.gain       |    768     |      True     |\n","|   seagull.transformer_layers.1.mha.qkv_transform.weight   |  1769472   |      True     |\n","|    seagull.transformer_layers.1.mha.qkv_transform.bias    |    2304    |      True     |\n","|  seagull.transformer_layers.1.mha.output_transform.weight |   589824   |      True     |\n","|   seagull.transformer_layers.1.mha.output_transform.bias  |    768     |      True     |\n","|        seagull.transformer_layers.1.ffn.glu.weight        |  3145728   |      True     |\n","|       seagull.transformer_layers.1.ffn.linear.weight      |  1572864   |      True     |\n","|        seagull.transformer_layers.1.layer_norm.gain       |    768     |      True     |\n","|   seagull.transformer_layers.2.mha.qkv_transform.weight   |  1769472   |      True     |\n","|    seagull.transformer_layers.2.mha.qkv_transform.bias    |    2304    |      True     |\n","|  seagull.transformer_layers.2.mha.output_transform.weight |   589824   |      True     |\n","|   seagull.transformer_layers.2.mha.output_transform.bias  |    768     |      True     |\n","|        seagull.transformer_layers.2.ffn.glu.weight        |  3145728   |      True     |\n","|       seagull.transformer_layers.2.ffn.linear.weight      |  1572864   |      True     |\n","|        seagull.transformer_layers.2.layer_norm.gain       |    768     |      True     |\n","|   seagull.transformer_layers.3.mha.qkv_transform.weight   |  1769472   |      True     |\n","|    seagull.transformer_layers.3.mha.qkv_transform.bias    |    2304    |      True     |\n","|  seagull.transformer_layers.3.mha.output_transform.weight |   589824   |      True     |\n","|   seagull.transformer_layers.3.mha.output_transform.bias  |    768     |      True     |\n","|        seagull.transformer_layers.3.ffn.glu.weight        |  3145728   |      True     |\n","|       seagull.transformer_layers.3.ffn.linear.weight      |  1572864   |      True     |\n","|        seagull.transformer_layers.3.layer_norm.gain       |    768     |      True     |\n","|   seagull.transformer_layers.4.mha.qkv_transform.weight   |  1769472   |      True     |\n","|    seagull.transformer_layers.4.mha.qkv_transform.bias    |    2304    |      True     |\n","|  seagull.transformer_layers.4.mha.output_transform.weight |   589824   |      True     |\n","|   seagull.transformer_layers.4.mha.output_transform.bias  |    768     |      True     |\n","|        seagull.transformer_layers.4.ffn.glu.weight        |  3145728   |      True     |\n","|       seagull.transformer_layers.4.ffn.linear.weight      |  1572864   |      True     |\n","|        seagull.transformer_layers.4.layer_norm.gain       |    768     |      True     |\n","|   seagull.transformer_layers.5.mha.qkv_transform.weight   |  1769472   |      True     |\n","|    seagull.transformer_layers.5.mha.qkv_transform.bias    |    2304    |      True     |\n","|  seagull.transformer_layers.5.mha.output_transform.weight |   589824   |      True     |\n","|   seagull.transformer_layers.5.mha.output_transform.bias  |    768     |      True     |\n","|        seagull.transformer_layers.5.ffn.glu.weight        |  3145728   |      True     |\n","|       seagull.transformer_layers.5.ffn.linear.weight      |  1572864   |      True     |\n","|        seagull.transformer_layers.5.layer_norm.gain       |    768     |      True     |\n","|   seagull.transformer_layers.6.mha.qkv_transform.weight   |  1769472   |      True     |\n","|    seagull.transformer_layers.6.mha.qkv_transform.bias    |    2304    |      True     |\n","|  seagull.transformer_layers.6.mha.output_transform.weight |   589824   |      True     |\n","|   seagull.transformer_layers.6.mha.output_transform.bias  |    768     |      True     |\n","|        seagull.transformer_layers.6.ffn.glu.weight        |  3145728   |      True     |\n","|       seagull.transformer_layers.6.ffn.linear.weight      |  1572864   |      True     |\n","|        seagull.transformer_layers.6.layer_norm.gain       |    768     |      True     |\n","|   seagull.transformer_layers.7.mha.qkv_transform.weight   |  1769472   |      True     |\n","|    seagull.transformer_layers.7.mha.qkv_transform.bias    |    2304    |      True     |\n","|  seagull.transformer_layers.7.mha.output_transform.weight |   589824   |      True     |\n","|   seagull.transformer_layers.7.mha.output_transform.bias  |    768     |      True     |\n","|        seagull.transformer_layers.7.ffn.glu.weight        |  3145728   |      True     |\n","|       seagull.transformer_layers.7.ffn.linear.weight      |  1572864   |      True     |\n","|        seagull.transformer_layers.7.layer_norm.gain       |    768     |      True     |\n","|   seagull.transformer_layers.8.mha.qkv_transform.weight   |  1769472   |      True     |\n","|    seagull.transformer_layers.8.mha.qkv_transform.bias    |    2304    |      True     |\n","|  seagull.transformer_layers.8.mha.output_transform.weight |   589824   |      True     |\n","|   seagull.transformer_layers.8.mha.output_transform.bias  |    768     |      True     |\n","|        seagull.transformer_layers.8.ffn.glu.weight        |  3145728   |      True     |\n","|       seagull.transformer_layers.8.ffn.linear.weight      |  1572864   |      True     |\n","|        seagull.transformer_layers.8.layer_norm.gain       |    768     |      True     |\n","|   seagull.transformer_layers.9.mha.qkv_transform.weight   |  1769472   |      True     |\n","|    seagull.transformer_layers.9.mha.qkv_transform.bias    |    2304    |      True     |\n","|  seagull.transformer_layers.9.mha.output_transform.weight |   589824   |      True     |\n","|   seagull.transformer_layers.9.mha.output_transform.bias  |    768     |      True     |\n","|        seagull.transformer_layers.9.ffn.glu.weight        |  3145728   |      True     |\n","|       seagull.transformer_layers.9.ffn.linear.weight      |  1572864   |      True     |\n","|        seagull.transformer_layers.9.layer_norm.gain       |    768     |      True     |\n","|   seagull.transformer_layers.10.mha.qkv_transform.weight  |  1769472   |      True     |\n","|    seagull.transformer_layers.10.mha.qkv_transform.bias   |    2304    |      True     |\n","| seagull.transformer_layers.10.mha.output_transform.weight |   589824   |      True     |\n","|  seagull.transformer_layers.10.mha.output_transform.bias  |    768     |      True     |\n","|        seagull.transformer_layers.10.ffn.glu.weight       |  3145728   |      True     |\n","|      seagull.transformer_layers.10.ffn.linear.weight      |  1572864   |      True     |\n","|       seagull.transformer_layers.10.layer_norm.gain       |    768     |      True     |\n","|   seagull.transformer_layers.11.mha.qkv_transform.weight  |  1769472   |      True     |\n","|    seagull.transformer_layers.11.mha.qkv_transform.bias   |    2304    |      True     |\n","| seagull.transformer_layers.11.mha.output_transform.weight |   589824   |      True     |\n","|  seagull.transformer_layers.11.mha.output_transform.bias  |    768     |      True     |\n","|        seagull.transformer_layers.11.ffn.glu.weight       |  3145728   |      True     |\n","|      seagull.transformer_layers.11.ffn.linear.weight      |  1572864   |      True     |\n","|       seagull.transformer_layers.11.layer_norm.gain       |    768     |      True     |\n","|                  seagull.layer_norm.gain                  |    768     |      True     |\n","+-----------------------------------------------------------+------------+---------------+\n","total trainable params: 110.53M\n"]}],"source":["# Change the best epoch value.\n","best_epoch = 0\n","bbpe_tokenizer = BBPETokenizer()\n","bbpe_tokenizer.from_file(os.path.join(PRETRAINED_ARTEFACTS_DIR, \"tokenizer\"))\n","\n","checkpoint_path = f\"seagull_captions_batch-size={batch_size}_epochs={num_epochs}/checkpoints/checkpoint_{best_epoch}.ckpt\"\n","print(f\"loading model from: {colored(checkpoint_path, 'blue')}\")\n","\n","device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n","with open(os.path.join(CONFIGS_DIR, \"train_model.yml\"), \"r\") as fp:\n","    config = yaml.safe_load(fp)\n","seagull_lm = SeagullLM(\n","    vocab_size=bbpe_tokenizer.vocab_size,\n","    padding_idx=bbpe_tokenizer.token2id(bbpe_tokenizer.pad_token),\n","    **config[\"model\"],\n",").to(device)\n","seagull_lm.from_pretrained_or_checkpoint(os.path.join(ARTEFACTS_DIR, f\"experiments/{checkpoint_path}\"))\n","\n","seagull_lm.print_params()\n"]},{"cell_type":"markdown","id":"d8abb408","metadata":{"id":"d8abb408"},"source":["Let's now check if the finetuned model has unlocked the superpower of contextual humor, in that, is the model now capable of generating humorous captions for a given scene and uncanny description.\n","\n","Same as before, let's load a random test sample and see how the model captions it. Change the sample_idx below to load a different sample and the cell below that stores the values for generation hyperparameters (e.g., temperature, num_samples), you're free to change those too.\n","\n"]},{"cell_type":"code","execution_count":29,"id":"d7ec6818","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1746586260770,"user":{"displayName":"Aileen Huang","userId":"01843079669161655601"},"user_tz":240},"id":"d7ec6818","outputId":"b1d68e4a-4761-409e-8a0f-9c74b8b862b6"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u003c|scene|\u003e A centaur man is looking at a naked man with an upper torso of a horse. The horse man with his butt sticking\n","out looks sad. \u003c|uncanny|\u003e There is a centaur there and a man with a horse head. Both are not real. \u003c|caption|\u003e\n"]}],"source":["sample_idx = 10  # change as needed\n","\n","# Choose from the test set to ensure captions are not included.\n","prompt = tokenized_dataset[\"test\"][sample_idx][\"text\"]\n","print(\"\\n\".join(text_wrapper.wrap(prompt)))\n"]},{"cell_type":"code","execution_count":30,"id":"c7b226bf","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":664,"status":"ok","timestamp":1746586261435,"user":{"displayName":"Aileen Huang","userId":"01843079669161655601"},"user_tz":240},"id":"c7b226bf","outputId":"6b2897da-7026-439f-f673-fb79a5e36a53"},"outputs":[{"data":{"text/plain":["[\"\u003c|scene|\u003e A centaur man is looking at a naked man with an upper torso of a horse. The horse man with his butt sticking out looks sad. \u003c|uncanny|\u003e There is a centaur there and a man with a horse head. Both are not real. \u003c|caption|\u003e It's a good idea, I'm just going to have to get the dog out of there, you know? \u003c|endofcaption|\u003e\",\n"," \"\u003c|scene|\u003e A centaur man is looking at a naked man with an upper torso of a horse. The horse man with his butt sticking out looks sad. \u003c|uncanny|\u003e There is a centaur there and a man with a horse head. Both are not real. \u003c|caption|\u003e I'm afraid I'll be in the wrong place at the wrong time. \u003c|endofcaption|\u003e\",\n"," \"\u003c|scene|\u003e A centaur man is looking at a naked man with an upper torso of a horse. The horse man with his butt sticking out looks sad. \u003c|uncanny|\u003e There is a centaur there and a man with a horse head. Both are not real. \u003c|caption|\u003e I'm afraid you're not sure I'm the only one who's still in it. \u003c|endofcaption|\u003e\"]"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["max_new_tokens = 100\n","num_samples = 3\n","top_k = 5\n","temperature = 0.7\n","decoding_strategy=\"top_k_decode\" # choose from the keys in DECODING_FUNCS in seagull_lm.py\n","\n","make_seagull_talk(\n","    seagull_lm=seagull_lm,\n","    bbpe_tokenizer=bbpe_tokenizer,\n","    prompt=prompt,\n","    max_new_tokens=max_new_tokens,\n","    num_samples=num_samples,\n","    decoding_strategy=\"top_k_decode\",\n","    decoding_kwargs={\"temperature\":temperature, \"k\":top_k},\n",")"]},{"cell_type":"code","execution_count":32,"id":"mYVPbT_ayPPi","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":2575,"status":"ok","timestamp":1746586594230,"user":{"displayName":"Aileen Huang","userId":"01843079669161655601"},"user_tz":240},"id":"mYVPbT_ayPPi","outputId":"2e40b725-f4fe-4830-f21b-385ea290fbbb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sample  \u003c|scene|\u003e A centaur man is looking at a naked man with an upper torso of a horse. The horse man with his butt sticking out looks sad. \u003c|uncanny|\u003e There is a centaur there and a man with a horse head. Both are not real. \u003c|caption|\u003e \n","\n","1 \u003c|scene|\u003e A centaur man is looking at a naked man with an upper torso of a horse. The horse man with his butt sticking out looks sad. \u003c|uncanny|\u003e There is a centaur there and a man with a horse head. Both are not real. \u003c|caption|\u003e It's a good idea, I'm just going to have to get the dog out of there, you know? \u003c|endofcaption|\u003e\n","2 \u003c|scene|\u003e A centaur man is looking at a naked man with an upper torso of a horse. The horse man with his butt sticking out looks sad. \u003c|uncanny|\u003e There is a centaur there and a man with a horse head. Both are not real. \u003c|caption|\u003e I'm afraid I'll be in the wrong place at the wrong time. \u003c|endofcaption|\u003e\n","3 \u003c|scene|\u003e A centaur man is looking at a naked man with an upper torso of a horse. The horse man with his butt sticking out looks sad. \u003c|uncanny|\u003e There is a centaur there and a man with a horse head. Both are not real. \u003c|caption|\u003e I'm afraid you're not sure I'm the only one who's still in it. \u003c|endofcaption|\u003e\n","Sample  \u003c|scene|\u003e A couple is at a car lot when a salesman shows them a hideous car. the care has cat arms coming out of the tires and cat eyes for headlights and razor sharp teeth. \u003c|uncanny|\u003e Cars do not have cat body parts coming out of it. \u003c|caption|\u003e \n","\n","1 \u003c|scene|\u003e A couple is at a car lot when a salesman shows them a hideous car. the care has cat arms coming out of the tires and cat eyes for headlights and razor sharp teeth. \u003c|uncanny|\u003e Cars do not have cat body parts coming out of it. \u003c|caption|\u003e It's a good car, but you're going to have to get the engine out. \u003c|endofcaption|\u003e\n","2 \u003c|scene|\u003e A couple is at a car lot when a salesman shows them a hideous car. the care has cat arms coming out of the tires and cat eyes for headlights and razor sharp teeth. \u003c|uncanny|\u003e Cars do not have cat body parts coming out of it. \u003c|caption|\u003e I'm just saying, I'm not afraid of the car. \u003c|endofcaption|\u003e\n","3 \u003c|scene|\u003e A couple is at a car lot when a salesman shows them a hideous car. the care has cat arms coming out of the tires and cat eyes for headlights and razor sharp teeth. \u003c|uncanny|\u003e Cars do not have cat body parts coming out of it. \u003c|caption|\u003e I'm just here for the \"cat\" and the \"cat\" and the \"cat\" and the \"cat\" and the \"cat\" and the \"cat\" and the \"cat\" and the \"cat\" and the \"cat\" and the \"cat\" and the \"cat\" and the \"cat\"cat\"cat\"cat\" and the\" and the cat\" and the\" and the \" and the”\n","Sample  \u003c|scene|\u003e Two men are in a courtroom. A judge is addressing a bailiff. \u003c|uncanny|\u003e Nothing is out of place that I can see. \u003c|caption|\u003e \n","\n","1 \u003c|scene|\u003e Two men are in a courtroom. A judge is addressing a bailiff. \u003c|uncanny|\u003e Nothing is out of place that I can see. \u003c|caption|\u003e It's a good way to settle the case. \u003c|endofcaption|\u003e\n","2 \u003c|scene|\u003e Two men are in a courtroom. A judge is addressing a bailiff. \u003c|uncanny|\u003e Nothing is out of place that I can see. \u003c|caption|\u003e I'm sorry, but I don't want you to believe it. \u003c|endofcaption|\u003e\n","3 \u003c|scene|\u003e Two men are in a courtroom. A judge is addressing a bailiff. \u003c|uncanny|\u003e Nothing is out of place that I can see. \u003c|caption|\u003e Your honor, I will be in court on Thursday. \u003c|endofcaption|\u003e\n"]}],"source":["#USING EXAMPLE CODE\n","sample_idxs = [10, 25, 42]\n","\n","for idx in sample_idxs:\n","    prompt = tokenized_dataset[\"test\"][idx][\"text\"]\n","\n","    max_new_tokens   = 100\n","    num_samples      = 3\n","    temperature      = 0.7\n","    top_k            = 5\n","    decoding_strategy = \"top_k_decode\"\n","\n","    # sample\n","    captions = make_seagull_talk(\n","        seagull_lm=seagull_lm,\n","        bbpe_tokenizer=bbpe_tokenizer,\n","        prompt=prompt,\n","        max_new_tokens=max_new_tokens,\n","        num_samples=num_samples,\n","        decoding_strategy=decoding_strategy,\n","        decoding_kwargs={\"temperature\": temperature, \"k\": top_k},\n","    )\n","\n","    print(\"Sample \", prompt, \"\\n\")\n","    i = 1\n","    for cap in captions:\n","        print(i, cap)\n","        i += 1"]},{"cell_type":"markdown","id":"f38c876e","metadata":{"id":"f38c876e"},"source":["Okay, has the model improved in its humor understanding? Is it able to generate context-specific, meaningful captions? We've also had the problem of the model not generating an \u003c|endofcaption|\u003e or \u003c|endoftext|\u003e token—has this been resolved with finetuning?\n","\n","Note. The function make_seagull_talk() stops generating further after the generation of an \u003c|endofcaption|\u003e token; so if the above captions appear to be shorter than the specified max_new_tokens, then the model has learned to associate captions as being short-length sequences ending in an \u003c|endofcaption|\u003e token.\n","\n"]},{"cell_type":"markdown","id":"9bb9a0ff","metadata":{"id":"9bb9a0ff"},"source":["\u003cfont color=\"orange\"\u003eQ4: Using your finetuned model, generate captions for some random test samples from the captions dataset and note down two/three of the most humorous ones below; format: scene, uncanny description, and the model-generated caption. (Maximum score: 2 points.)\n","\n","This question is intended to have you inspect the generative and humor understanding abilities of the model, and to understand the impact of generation hyperparameters (e.g., temperature, top_k) on the generation diversity/quality.\n","\n","\u003c/font\u003e"]},{"cell_type":"markdown","id":"BwUms54POnqh","metadata":{"id":"BwUms54POnqh"},"source":["__Answer.__\n","\n","Example 1:\n","Scene: A centaur man is looking at a naked man with an upper torso of a horse. The horse man with his butt sticking out looks sad\n","Uncanny description: There is a centaur there and a man with a horse head. Both are not real.\n","Model-generated caption: I'm afraid I'll be in the wrong place at the wrong time.\n","\n","\n","Example 2:\n","Scene:  A couple is at a car lot when a salesman shows them a hideous car. the care has cat arms coming out of the tires and cat eyes for headlights and razor sharp teeth.\n","Uncanny description: Cars do not have cat body parts coming out of it.\n","Model-generated caption: I'm just saying, I'm not afraid of the car.\n","\n","Example 3:\n","Scene: Two men are in a courtroom. A judge is addressing a bailiff.\n","Uncanny description:  Nothing is out of place that I can see.\n","Model-generated caption: It's a good way to settle the case."]},{"cell_type":"markdown","id":"Z7FiI2-2edYt","metadata":{"id":"Z7FiI2-2edYt"},"source":["\u003ca name=\"sec6\"\u003e\u003c/a\u003e\n","### [6] Model evaluation \u003csmall\u003e[↩︎](#outline)\u003c/small\u003e\n","\n","The main motivation and goal of this assignment was to build your own transformer, a humorous one at that!\n","\n","We will evaluate our models using the ROUGE score between the generated captions and the gold captions in the dataset.\n","\n","Use the following script to generate outputs for 100 examples in the validation set.\n"]},{"cell_type":"code","execution_count":33,"id":"E6hTdAWaGVDW","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":21042,"status":"ok","timestamp":1746587065876,"user":{"displayName":"Aileen Huang","userId":"01843079669161655601"},"user_tz":240},"id":"E6hTdAWaGVDW","outputId":"563f1f9b-2d15-42f7-d61f-e601c4a28249"},"outputs":[{"name":"stdout","output_type":"stream","text":[" It'm your Honor, it's mine! \u003c|endofcaption|\u003e\n"," Our estate planner wants to know if you would like your inheritance in a lump sum or spread out over nine lives. \u003c|endofcaption|\u003e\n"]}],"source":["max_new_tokens = 100\n","num_samples = 1\n","top_k = 5\n","temperature = 2\n","decoding_strategy=\"top_k_decode\" # choose from the keys in DECODING_FUNCS in seagull_lm.py\n","\n","\n","generated_outputs = []\n","gold_outputs = []\n","for i in range(100):\n","  text_curr = processed_dataset['val'][i]['text']\n","  input_curr = text_curr.split('\u003c|caption|\u003e')[0] + '\u003c|caption|\u003e'\n","  caption_curr = text_curr.split('\u003c|caption|\u003e')[1]\n","  gold_outputs.append(caption_curr)\n","  generated_output_curr = make_seagull_talk(\n","                    seagull_lm=seagull_lm,\n","                    bbpe_tokenizer=bbpe_tokenizer,\n","                    prompt=prompt,\n","                    max_new_tokens=max_new_tokens,\n","                    num_samples=num_samples,\n","                    decoding_strategy=\"top_k_decode\",\n","                    decoding_kwargs={\"temperature\":temperature, \"k\":top_k},\n","                  )\n","  generated_outputs.append(generated_output_curr[0].split('\u003c|caption|\u003e')[1])\n","\n","print(generated_outputs[0])\n","print(gold_outputs[0])\n"]},{"cell_type":"markdown","id":"wyXs9zrSJOLv","metadata":{"id":"wyXs9zrSJOLv"},"source":["You can quantify how good these generations are using the following code block that computes ROUGE scores."]},{"cell_type":"code","execution_count":40,"id":"mLX_G-kkKi6g","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1746588126796,"user":{"displayName":"Aileen Huang","userId":"01843079669161655601"},"user_tz":240},"id":"mLX_G-kkKi6g","outputId":"59a589a2-ef81-415f-b424-6d4a2ce66865"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'rouge1': 0.1419763668208131, 'rouge2': 0.0030050125313283212}\n"]}],"source":["from rouge_score import rouge_scorer\n","\n","scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2'], use_stemmer=True)\n","\n","scores = {'rouge1': [], 'rouge2': []}\n","for pred, ref in zip(generated_outputs, gold_outputs):\n","    result = scorer.score(ref, pred)\n","    for key in scores:\n","        scores[key].append(result[key].fmeasure)\n","\n","avg_scores = {key: np.mean(scores[key]) for key in scores}\n","print(avg_scores)\n","\n"]},{"cell_type":"code","execution_count":43,"id":"7v7fC6Hru-O2","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":19184,"status":"ok","timestamp":1746588614131,"user":{"displayName":"Aileen Huang","userId":"01843079669161655601"},"user_tz":240},"id":"7v7fC6Hru-O2","outputId":"aa1493d1-3b5e-43b6-fe7f-e5812c08947a"},"outputs":[{"name":"stdout","output_type":"stream","text":[" Let's see how you perform. \u003c|endofcaption|\u003e\n"," Our estate planner wants to know if you would like your inheritance in a lump sum or spread out over nine lives. \u003c|endofcaption|\u003e\n","{'rouge1': 0.14216684301128932, 'rouge2': 0.0030050125313283212}\n"]}],"source":["#USING FORMAT FROM ABOVE\n","max_new_tokens = 100\n","num_samples = 1\n","top_k = 20\n","temperature = 0.7\n","decoding_strategy=\"top_k_decode\" # choose from the keys in DECODING_FUNCS in seagull_lm.py\n","\n","\n","generated_outputs = []\n","gold_outputs = []\n","for i in range(100):\n","  text_curr = processed_dataset['val'][i]['text']\n","  input_curr = text_curr.split('\u003c|caption|\u003e')[0] + '\u003c|caption|\u003e'\n","  caption_curr = text_curr.split('\u003c|caption|\u003e')[1]\n","  gold_outputs.append(caption_curr)\n","  generated_output_curr = make_seagull_talk(\n","                    seagull_lm=seagull_lm,\n","                    bbpe_tokenizer=bbpe_tokenizer,\n","                    prompt=prompt,\n","                    max_new_tokens=max_new_tokens,\n","                    num_samples=num_samples,\n","                    decoding_strategy=\"top_k_decode\",\n","                    decoding_kwargs={\"temperature\":temperature, \"k\":top_k},\n","                  )\n","  generated_outputs.append(generated_output_curr[0].split('\u003c|caption|\u003e')[1])\n","\n","print(generated_outputs[0])\n","print(gold_outputs[0])\n","\n","scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2'], use_stemmer=True)\n","\n","scores = {'rouge1': [], 'rouge2': []}\n","for pred, ref in zip(generated_outputs, gold_outputs):\n","    result = scorer.score(ref, pred)\n","    for key in scores:\n","        scores[key].append(result[key].fmeasure)\n","\n","avg_scores = {key: np.mean(scores[key]) for key in scores}\n","print(avg_scores)\n"]},{"cell_type":"code","execution_count":45,"id":"zbFLbIaYy3sn","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":146524,"status":"ok","timestamp":1746588786705,"user":{"displayName":"Aileen Huang","userId":"01843079669161655601"},"user_tz":240},"id":"zbFLbIaYy3sn","outputId":"9ff982d3-5ed7-4b92-81c1-65ac815e44ba"},"outputs":[{"name":"stdout","output_type":"stream","text":[" Let you look. There were very few men ... but one person (Malinowitz, the real winner at an early court contest), with the full consent on the defense part !! It will have never happen....And this trial. All you see you say will been “No”? And the winner? We are now a part owner of BetoCon and all our stock, our stock has crashed to a tiny 4-bit high !! That’ve never looked\n"," Our estate planner wants to know if you would like your inheritance in a lump sum or spread out over nine lives. \u003c|endofcaption|\u003e\n","{'rouge1': 0.06665912860270261, 'rouge2': 0.001431158071299182}\n"]}],"source":["#USING FORMAT FROM ABOVE\n","max_new_tokens = 100\n","num_samples = 1\n","top_k = 20\n","temperature = 7\n","decoding_strategy=\"top_k_decode\" # choose from the keys in DECODING_FUNCS in seagull_lm.py\n","\n","\n","generated_outputs = []\n","gold_outputs = []\n","for i in range(100):\n","  text_curr = processed_dataset['val'][i]['text']\n","  input_curr = text_curr.split('\u003c|caption|\u003e')[0] + '\u003c|caption|\u003e'\n","  caption_curr = text_curr.split('\u003c|caption|\u003e')[1]\n","  gold_outputs.append(caption_curr)\n","  generated_output_curr = make_seagull_talk(\n","                    seagull_lm=seagull_lm,\n","                    bbpe_tokenizer=bbpe_tokenizer,\n","                    prompt=prompt,\n","                    max_new_tokens=max_new_tokens,\n","                    num_samples=num_samples,\n","                    decoding_strategy=\"top_k_decode\",\n","                    decoding_kwargs={\"temperature\":temperature, \"k\":top_k},\n","                  )\n","  generated_outputs.append(generated_output_curr[0].split('\u003c|caption|\u003e')[1])\n","\n","print(generated_outputs[0])\n","print(gold_outputs[0])\n","\n","scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2'], use_stemmer=True)\n","\n","scores = {'rouge1': [], 'rouge2': []}\n","for pred, ref in zip(generated_outputs, gold_outputs):\n","    result = scorer.score(ref, pred)\n","    for key in scores:\n","        scores[key].append(result[key].fmeasure)\n","\n","avg_scores = {key: np.mean(scores[key]) for key in scores}\n","print(avg_scores)\n"]},{"cell_type":"code","execution_count":48,"id":"TFK_HIspwCgD","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40173,"status":"ok","timestamp":1746589601612,"user":{"displayName":"Aileen Huang","userId":"01843079669161655601"},"user_tz":240},"id":"TFK_HIspwCgD","outputId":"aa3c72a0-708f-4150-ce0e-5a951d23c17f"},"outputs":[{"name":"stdout","output_type":"stream","text":[" I'm sorry, but I'm not sure you're the only one who's been in the wrong place. \u003c|endofcaption|\u003e\n"," Our estate planner wants to know if you would like your inheritance in a lump sum or spread out over nine lives. \u003c|endofcaption|\u003e\n","greedy decode {'rouge1': 0.15717696361710565, 'rouge2': 0.01129175776251948}\n"]}],"source":["#USING FORMAT FROM ABOVE\n","max_new_tokens = 100\n","num_samples = 1\n","decoding_strategy=\"greedy_decode\" # choose from the keys in DECODING_FUNCS in seagull_lm.py\n","\n","\n","generated_outputs = []\n","gold_outputs = []\n","for i in range(100):\n","  text_curr = processed_dataset['val'][i]['text']\n","  input_curr = text_curr.split('\u003c|caption|\u003e')[0] + '\u003c|caption|\u003e'\n","  caption_curr = text_curr.split('\u003c|caption|\u003e')[1]\n","  gold_outputs.append(caption_curr)\n","  generated_output_curr = make_seagull_talk(\n","                    seagull_lm=seagull_lm,\n","                    bbpe_tokenizer=bbpe_tokenizer,\n","                    prompt=prompt,\n","                    max_new_tokens=max_new_tokens,\n","                    num_samples=num_samples,\n","                    decoding_strategy=\"greedy_decode\",\n","                    decoding_kwargs={},\n","                  )\n","  generated_outputs.append(generated_output_curr[0].split('\u003c|caption|\u003e')[1])\n","\n","print(generated_outputs[0])\n","print(gold_outputs[0])\n","\n","scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2'], use_stemmer=True)\n","\n","scores = {'rouge1': [], 'rouge2': []}\n","for pred, ref in zip(generated_outputs, gold_outputs):\n","    result = scorer.score(ref, pred)\n","    for key in scores:\n","        scores[key].append(result[key].fmeasure)\n","\n","avg_scores = {key: np.mean(scores[key]) for key in scores}\n","print(\"greedy decode\", avg_scores)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"zrNzEzgg1zxg","metadata":{"colab":{"background_save":true},"id":"zrNzEzgg1zxg"},"outputs":[{"name":"stdout","output_type":"stream","text":[" Let's see what happens in the case of the defendant. \u003c|endofcaption|\u003e\n"," Our estate planner wants to know if you would like your inheritance in a lump sum or spread out over nine lives. \u003c|endofcaption|\u003e\n"," p decode {'rouge1': 0.16383369258151248, 'rouge2': 0.006172490832081625}\n"]}],"source":["#USING FORMAT FROM ABOVE\n","#TAkING TOO LONG TO RUN SO STOPPED IT -\u003e Decided to no longer test top_p_decode\n","max_new_tokens = 100\n","num_samples = 1\n","p = 0.01\n","temperature = 0.7\n","decoding_strategy=\"top_p_decode\" # choose from the keys in DECODING_FUNCS in seagull_lm.py\n","\n","\n","generated_outputs = []\n","gold_outputs = []\n","for i in range(100):\n","  text_curr = processed_dataset['val'][i]['text']\n","  input_curr = text_curr.split('\u003c|caption|\u003e')[0] + '\u003c|caption|\u003e'\n","  caption_curr = text_curr.split('\u003c|caption|\u003e')[1]\n","  gold_outputs.append(caption_curr)\n","  generated_output_curr = make_seagull_talk(\n","                    seagull_lm=seagull_lm,\n","                    bbpe_tokenizer=bbpe_tokenizer,\n","                    prompt=prompt,\n","                    max_new_tokens=max_new_tokens,\n","                    num_samples=num_samples,\n","                    decoding_strategy=\"top_p_decode\",\n","                    decoding_kwargs={\"temperature\": 0.7, \"p\": 0.8},\n","                  )\n","  generated_outputs.append(generated_output_curr[0].split('\u003c|caption|\u003e')[1])\n","\n","print(generated_outputs[0])\n","print(gold_outputs[0])\n","\n","scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2'], use_stemmer=True)\n","\n","scores = {'rouge1': [], 'rouge2': []}\n","for pred, ref in zip(generated_outputs, gold_outputs):\n","    result = scorer.score(ref, pred)\n","    for key in scores:\n","        scores[key].append(result[key].fmeasure)\n","\n","avg_scores = {key: np.mean(scores[key]) for key in scores}\n","print(\" p decode\", avg_scores)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"La2pDv3Z1jVc","metadata":{"id":"La2pDv3Z1jVc"},"outputs":[],"source":["#USING FORMAT FROM ABOVE\n","#TAkING TOO LONG TO RUN SO STOPPED IT -\u003e Decided to no longer test top_p_decode\n","max_new_tokens = 100\n","num_samples = 1\n","p = 0.1\n","temperature = 0.7\n","decoding_strategy=\"top_p_decode\" # choose from the keys in DECODING_FUNCS in seagull_lm.py\n","\n","\n","generated_outputs = []\n","gold_outputs = []\n","for i in range(100):\n","  text_curr = processed_dataset['val'][i]['text']\n","  input_curr = text_curr.split('\u003c|caption|\u003e')[0] + '\u003c|caption|\u003e'\n","  caption_curr = text_curr.split('\u003c|caption|\u003e')[1]\n","  gold_outputs.append(caption_curr)\n","  generated_output_curr = make_seagull_talk(\n","                    seagull_lm=seagull_lm,\n","                    bbpe_tokenizer=bbpe_tokenizer,\n","                    prompt=prompt,\n","                    max_new_tokens=max_new_tokens,\n","                    num_samples=num_samples,\n","                    decoding_strategy=\"top_p_decode\",\n","                    decoding_kwargs={\"temperature\": 0.7, \"p\": 0.8},\n","                  )\n","  generated_outputs.append(generated_output_curr[0].split('\u003c|caption|\u003e')[1])\n","\n","print(generated_outputs[0])\n","print(gold_outputs[0])\n","\n","scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2'], use_stemmer=True)\n","\n","scores = {'rouge1': [], 'rouge2': []}\n","for pred, ref in zip(generated_outputs, gold_outputs):\n","    result = scorer.score(ref, pred)\n","    for key in scores:\n","        scores[key].append(result[key].fmeasure)\n","\n","avg_scores = {key: np.mean(scores[key]) for key in scores}\n","print(\" p decode\", avg_scores)\n","\n","\n"]},{"cell_type":"code","execution_count":44,"id":"asBxoLnyxKHY","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":23600,"status":"ok","timestamp":1746588638017,"user":{"displayName":"Aileen Huang","userId":"01843079669161655601"},"user_tz":240},"id":"asBxoLnyxKHY","outputId":"c0c9cf52-affd-4187-f025-52f9aa970804"},"outputs":[{"name":"stdout","output_type":"stream","text":[" Let's see how late it's all been, sir. \u003c|endofcaption|\u003e\n"," Our estate planner wants to know if you would like your inheritance in a lump sum or spread out over nine lives. \u003c|endofcaption|\u003e\n","Sampling decode {'rouge1': 0.1250749896073683, 'rouge2': 0.009582989190406071}\n"]}],"source":["#USING FORMAT FROM ABOVE\n","max_new_tokens = 100\n","num_samples = 1\n","temperature = 0.1\n","decoding_strategy=\"sampling_decode\" # choose from the keys in DECODING_FUNCS in seagull_lm.py\n","\n","\n","generated_outputs = []\n","gold_outputs = []\n","for i in range(100):\n","  text_curr = processed_dataset['val'][i]['text']\n","  input_curr = text_curr.split('\u003c|caption|\u003e')[0] + '\u003c|caption|\u003e'\n","  caption_curr = text_curr.split('\u003c|caption|\u003e')[1]\n","  gold_outputs.append(caption_curr)\n","  generated_output_curr = make_seagull_talk(\n","                    seagull_lm=seagull_lm,\n","                    bbpe_tokenizer=bbpe_tokenizer,\n","                    prompt=prompt,\n","                    max_new_tokens=max_new_tokens,\n","                    num_samples=num_samples,\n","                    decoding_strategy=\"sampling_decode\",\n","                    decoding_kwargs={\"temperature\": 0.7},\n","                  )\n","  generated_outputs.append(generated_output_curr[0].split('\u003c|caption|\u003e')[1])\n","\n","print(generated_outputs[0])\n","print(gold_outputs[0])\n","\n","scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2'], use_stemmer=True)\n","\n","scores = {'rouge1': [], 'rouge2': []}\n","for pred, ref in zip(generated_outputs, gold_outputs):\n","    result = scorer.score(ref, pred)\n","    for key in scores:\n","        scores[key].append(result[key].fmeasure)\n","\n","avg_scores = {key: np.mean(scores[key]) for key in scores}\n","print(\"Sampling decode\", avg_scores)\n","\n","\n"]},{"cell_type":"code","execution_count":46,"id":"Dm2lc-XHxgN-","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":31960,"status":"ok","timestamp":1746588818921,"user":{"displayName":"Aileen Huang","userId":"01843079669161655601"},"user_tz":240},"id":"Dm2lc-XHxgN-","outputId":"b6409e15-378a-4211-e75b-270357596189"},"outputs":[{"name":"stdout","output_type":"stream","text":[" Let's settle this because it's all been going on for an additional 10 days! \u003c|endofcaption|\u003e\n"," Our estate planner wants to know if you would like your inheritance in a lump sum or spread out over nine lives. \u003c|endofcaption|\u003e\n","Sampling decode {'rouge1': 0.11787971846292551, 'rouge2': 0.010242632701137318}\n"]}],"source":["#USING FORMAT FROM ABOVE\n","max_new_tokens = 100\n","num_samples = 1\n","temperature = 1\n","decoding_strategy=\"sampling_decode\" # choose from the keys in DECODING_FUNCS in seagull_lm.py\n","\n","\n","generated_outputs = []\n","gold_outputs = []\n","for i in range(100):\n","  text_curr = processed_dataset['val'][i]['text']\n","  input_curr = text_curr.split('\u003c|caption|\u003e')[0] + '\u003c|caption|\u003e'\n","  caption_curr = text_curr.split('\u003c|caption|\u003e')[1]\n","  gold_outputs.append(caption_curr)\n","  generated_output_curr = make_seagull_talk(\n","                    seagull_lm=seagull_lm,\n","                    bbpe_tokenizer=bbpe_tokenizer,\n","                    prompt=prompt,\n","                    max_new_tokens=max_new_tokens,\n","                    num_samples=num_samples,\n","                    decoding_strategy=\"sampling_decode\",\n","                    decoding_kwargs={\"temperature\": 1},\n","                  )\n","  generated_outputs.append(generated_output_curr[0].split('\u003c|caption|\u003e')[1])\n","\n","print(generated_outputs[0])\n","print(gold_outputs[0])\n","\n","scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2'], use_stemmer=True)\n","\n","scores = {'rouge1': [], 'rouge2': []}\n","for pred, ref in zip(generated_outputs, gold_outputs):\n","    result = scorer.score(ref, pred)\n","    for key in scores:\n","        scores[key].append(result[key].fmeasure)\n","\n","avg_scores = {key: np.mean(scores[key]) for key in scores}\n","print(\"Sampling decode\", avg_scores)\n","\n","\n"]},{"cell_type":"markdown","id":"Bhq00g25LlXb","metadata":{"id":"Bhq00g25LlXb"},"source":["You should try to identify the decoding parameter set that gives you the highest rouge score.\n","\n","You can experiment with the following hyperparameters: top_p, temperature if using top_p sampling, top_k, temperature if using top_k sampling, or just the temperature if using random sampling."]},{"cell_type":"markdown","id":"bZe5cg5BPIG5","metadata":{"id":"bZe5cg5BPIG5"},"source":["\u003cfont color=\"orange\"\u003e Q5: Use your best model checkpoint to report ROUGE1 and ROUGE 2 scores for the 100 validation examples. \u003cbr/\u003e\u003cbr/\u003e You should report scores for each of the 4 decoding strategies. For random sampling, report results for 2 different values of temperature.\n","For top_p, and top_k, report results using 2 different the values p and k each. \u003cbr/\u003e\u003cbr/\u003e\n","Which decoding strategy and hyperparameter set gives you the best performance?\n","\n","\u003c/font\u003e"]},{"cell_type":"markdown","id":"J9qWb5HoP5Xw","metadata":{"id":"J9qWb5HoP5Xw"},"source":["__Answer.__\n","\n","The decoding strategy and hyperparameter that gives the strongest performance is greedy decode ran with 1 sample and max_tokens = 100, out of all experiments tested above."]},{"cell_type":"markdown","id":"DN6JISNCP6JI","metadata":{"id":"DN6JISNCP6JI"},"source":["\u003cfont color=\"orange\"\u003e Q6: Comment on whether you believe ROUGE is a good metric for this task. Why or why not? Discuss pros and cons of alaternative text generation evaluation metrics we discussed in class.\n","\n","\u003c/font\u003e"]},{"cell_type":"markdown","id":"mvLaPEhoQOT4","metadata":{"id":"mvLaPEhoQOT4"},"source":["__Answer.__\n","\n","We learned about rouge in context of n-grams, calculating word overlaps, which may not be the most contextually relevant for our goal of generating humorous captions. Also, since we can have different correct captions for our inputs, rouge may rank one correct caption above one another correct caption even though they're both valid. The advantage of ROUGE is that it's quite easy to calculate.\n","\n","It's difficult to account for different \"equally\" correct captions. Even with QA techniques we discussed in class, it still focuses more on factuality assessment than humorous assessment. I think to assess semantic/humor, perhaps we could also build a separate transformer model to classify how humorous our caption is on a scale from 1 to 10, so one model for assessing factuality relevance and one model for assessing humor."]},{"cell_type":"markdown","id":"db6d41a2","metadata":{"id":"db6d41a2"},"source":["\u003ca name=\"sec7\"\u003e\u003c/a\u003e\n","### [7] Final Submission \u003csmall\u003e[↩︎](#outline)\u003c/small\u003e\n","\n","Yay! Now that we've successfully finetuned our Seagull to generate short, humorous captions, let's bundle everything that's needed to make a submission on the submission site(s).\n","\n","For your final submission, you will need to submit the hw4_submission.zip and a .pdf of this notebook file (or a separate file containing answers to your written questions; don't forget to attribute!) on the submission site(s). Note: this notebook will only be used to grade your answers to the written questions; you will not be graded on any code in this notebook file.\n","\n","\n","Running the cell below generates a hw4_submission.zip file in the artefacts folder. Caution: the script will overwrite files with the same name existing in the artefacts folder.\n","\n"]},{"cell_type":"code","execution_count":47,"id":"SOIwfNK9QTc0","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":296},"executionInfo":{"elapsed":4781,"status":"ok","timestamp":1746589409554,"user":{"displayName":"Aileen Huang","userId":"01843079669161655601"},"user_tz":240},"id":"SOIwfNK9QTc0","outputId":"cee398af-aff1-4341-b5f7-d20a7e168e78"},"outputs":[{"name":"stdout","output_type":"stream","text":["submission stored at: /content/drive/MyDrive/cs4740_programming_assignments/hw4-release/artefacts/hw4_submission.zip\n","\u001b[92mSuccess!\n","\u001b[0m"]},{"data":{"text/html":["\u003cvideo alt=\"success, happy puppy!\" width=\"400\" height=\"240\" controls autoplay=1\u003e\n","                \u003csource src=\"https://openpuppies.com/mp4/bRKfspn.mp4\" type=\"video/mp4\"/\u003e \n","            \u003c/video\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"}],"source":["!python3 -m scripts.make_submission \\\n","    --basepath-to-store-submission={os.path.join(ARTEFACTS_DIR, submission_filepath)} \\\n","    --net-ids={net_ids}\n","\n","if os.path.isfile(f\"{os.path.join(ARTEFACTS_DIR, 'hw4_submission.zip')}\"):\n","    display(success())\n","else:\n","    print(colored(\"Oops, something went wrong!\", \"red\"))"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":5}