{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jnrb6Cd8yBvw"
   },
   "source": [
    "<h2>CS 3780/5780 Creative Project: </h2>\n",
    "<h3>Emotion Classification of Natural Language</h3>\n",
    "\n",
    "Names and NetIDs for your group members: Jessica Andrews (jaa375) and Aileen Huang (aeh245)\n",
    "\n",
    "Kaggle Team Name: Jess and Aileen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "POj8xC0EyBvy"
   },
   "source": [
    "<h3>Introduction:</h3>\n",
    "\n",
    "<p> The creative project is about conducting a real-world machine learning project on your own, with everything that is involved. Unlike in the programming projects 1-5, where we gave you all the scaffolding and you just filled in the blanks, you now start from scratch. The past programming projects provide templates for how to do this (and you can reuse part of your code if you wish), and the lectures provide some of the methods you can use. So, this creative project brings realism to how you will use machine learning in the real world.  </p>\n",
    "\n",
    "The task you will work on is classifying texts to human emotions. Through words, humans express feelings, articulate thoughts, and communicate our deepest needs and desires. Language helps us interpret the nuances of joy, sadness, anger, and love, allowing us to connect with others on a deeper level. Are you able to train an ML model that recognizes the human emotions expressed in a piece of text? <b>Please read the project description PDF file carefully and follow the instructions there. Also make sure you write your code and answers to all the questions in this Jupyter Notebook </b> </p>\n",
    "<p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3aLESwEyBvz"
   },
   "source": [
    "<h2>Part 0: Basics</h2><p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0_WkK84yBvz"
   },
   "source": [
    "<h3>0.1 Import:</h3><p>\n",
    "Please import necessary packages to use. Note that learning and using packages are recommended but not required for this project. Some official tutorial for suggested packacges includes:\n",
    "    \n",
    "https://scikit-learn.org/stable/tutorial/basic/tutorial.html\n",
    "    \n",
    "https://pytorch.org/tutorials/\n",
    "    \n",
    "https://pandas.pydata.org/pandas-docs/stable/user_guide/10min.html\n",
    "<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ywPiF0SGyBv0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5hQN8qPyBv0"
   },
   "source": [
    "<h3>0.2 Accuracy and Mean Squared Error:</h3><p>\n",
    "To measure your performance in the Kaggle Competition, we are using accuracy. As a recap, accuracy is the percent of labels you predict correctly. To measure this, you can use library functions from sklearn. A simple example is shown below.\n",
    "<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "eW1W5M28yBv1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42857142857142855"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = [3, 2, 1, 0, 1, 2, 3]\n",
    "y_true = [0, 1, 2, 3, 1, 2, 3]\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m3m00ldByBv1"
   },
   "source": [
    "<h2>Part 1: Basic</h2><p>\n",
    "Note that your code should be commented well and in part 1.4 you can refer to your comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ourlPF8_yBv2"
   },
   "source": [
    "<h3>1.1 Load and preprocess the dataset:</h3><p>\n",
    "We provide how to load the data on Kaggle's Notebook.\n",
    "<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ph033tLAyBv2"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "train_text = train[\"text\"]\n",
    "train_label = train[\"label\"]\n",
    "\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "test_id = test[\"id\"]\n",
    "test_text = test[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6-zAX-OXyBv2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Complete!\n",
      "Train TF-IDF shape: (8000, 2572)\n",
      "Validation TF-IDF shape: (2000, 2572)\n",
      "Test TF-IDF shape: (15000, 2572)\n",
      "Number of features: 2572\n",
      "Sample Feature Names:\n",
      "['20' '30' 'ability' 'able' 'absolute' 'absolutely' 'abuse' 'abused'\n",
      " 'accept' 'acceptable' 'accepted' 'accident' 'accidentally'\n",
      " 'accomplishment' 'account' 'accounts' 'ache' 'aching' 'act' 'acting'\n",
      " 'active' 'activities' 'actor' 'actual' 'actually' 'actually feel' 'add'\n",
      " 'admired' 'admit' 'admit feeling']\n"
     ]
    }
   ],
   "source": [
    "# Make sure you comment your code clearly and you may refer to these comments in the part 1.4\n",
    "\n",
    "# 3. Define a text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Cleans and preprocesses the input text.\n",
    "    \"\"\"\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)  # Remove URLs\n",
    "    return text\n",
    "\n",
    "train_text = train[\"text\"].apply(preprocess_text)\n",
    "test_text = test[\"text\"].apply(preprocess_text)\n",
    "train_label = train[\"label\"]\n",
    "\n",
    "# 5. Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_label_encoded = label_encoder.fit_transform(train_label)\n",
    "\n",
    "# 6. Split the training data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_text, train_label_encoded, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 7. TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=20000,  # Use top 20,000 features for richer representation\n",
    "    ngram_range=(1, 3),  # Include unigrams, bigrams, and trigrams\n",
    "    stop_words=\"english\",  # Remove common stop words\n",
    "    min_df=5,  # Ignore terms appearing in fewer than 5 documents\n",
    "    max_df=0.8,  # Ignore terms appearing in more than 80% of documents\n",
    ")\n",
    "\n",
    "# Fit and transform the train data, transform validation and test data\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_val_tfidf = tfidf_vectorizer.transform(X_val)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_text)\n",
    "\n",
    "# 8. Display preprocessing summary\n",
    "print(\"Preprocessing Complete!\")\n",
    "print(f\"Train TF-IDF shape: {X_train_tfidf.shape}\")\n",
    "print(f\"Validation TF-IDF shape: {X_val_tfidf.shape}\")\n",
    "print(f\"Test TF-IDF shape: {X_test_tfidf.shape}\")\n",
    "\n",
    "# 9. Display sample features from the TF-IDF vectorizer\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "print(f\"Number of features: {len(feature_names)}\")\n",
    "print(\"Sample Feature Names:\")\n",
    "print(feature_names[:30])  # Display the first 30 feature names\n",
    "\n",
    "# Save the processed datasets if needed\n",
    "X_train.to_csv(\"X_train_cleaned.csv\", index=False)\n",
    "X_val.to_csv(\"X_val_cleaned.csv\", index=False)\n",
    "test_text.to_csv(\"test_cleaned.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N0065a0ayBv3"
   },
   "source": [
    "<h3>1.2 Use At Least Two Training Algorithms from class:</h3><p>\n",
    "You need to use at least two training algorithms from class. You can use your code from previous projects or any packages you imported in part 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "e1n6rWw4yBv3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.7891\n",
      "Validation Accuracy: 0.6805\n"
     ]
    }
   ],
   "source": [
    "# Make sure you comment your code clearly and you may refer to these comments in the part 1.4\n",
    "\n",
    "#Model 1: Logistic Regression\n",
    "\n",
    "#Logistic Regression Model: Just showing its original training accuracy\n",
    "\n",
    "#Train Logistic Regression model\n",
    "logreg = LogisticRegression(max_iter=1000, random_state=42)  # Increase max_iter if needed\n",
    "logreg.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# 3. Make predictions\n",
    "y_train_pred = logreg.predict(X_train_tfidf)\n",
    "y_val_pred = logreg.predict(X_val_tfidf)\n",
    "\n",
    "# 4. Evaluate the model\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "RnAW7qvfyBv3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6850\n"
     ]
    }
   ],
   "source": [
    "#Model 2: SVM\n",
    "\n",
    "#SVM Grid Search\n",
    "# svm_params = {\n",
    "#     'kernel': ['rbf'],\n",
    "#     'C': np.arange(1,8)\n",
    "# }\n",
    "\n",
    "# grid = GridSearchCV(estimator = SVC(), param_grid = svm_params, scoring= 'accuracy')\n",
    "# grid.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# print(f\"\\nBest Parameters: {grid_search.best_params_}\")\n",
    "# print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "#Model 2: SVM\n",
    "svm = SVC(kernel = 'rbf', C= 2.0, gamma= 'scale')\n",
    "svm.fit(X_train_tfidf, y_train)\n",
    "#SVM Testing\n",
    "\n",
    "# 3. Make predictions\n",
    "y_val_pred = svm.predict(X_val_tfidf)\n",
    "\n",
    "# 4. Evaluate the model\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N7RLiRpMyBv3"
   },
   "source": [
    "<h3>1.3 Training, Validation and Model Selection:</h3><p>\n",
    "You need to split your data to a training set and validation set or performing a cross-validation for model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "aujPtsS5yBv4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train TF-IDF shape: (10000, 3087)\n",
      "Test TF-IDF shape: (15000, 3087)\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aileenh/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 3 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "/Users/aileenh/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/aileenh/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/aileenh/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/aileenh/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/aileenh/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/aileenh/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/aileenh/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/aileenh/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/aileenh/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Parameters: {'C': 10, 'solver': 'liblinear'}\n",
      "Best Cross-Validation Accuracy: 0.7156\n",
      "Training Accuracy with Best Model: 0.9438\n",
      "Predictions saved to submission_logreg.csv\n"
     ]
    }
   ],
   "source": [
    "#LOGISTIC REGRESSION:\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "#Want to use Entire Training Set for K-fold cross validation!! This line restores the entire training set\n",
    "X_train, y_train = train_text, train_label_encoded\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_text)\n",
    "print(f\"Train TF-IDF shape: {X_train_tfidf.shape}\")\n",
    "print(f\"Test TF-IDF shape: {X_test_tfidf.shape}\")\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],  # Regularization strengths\n",
    "    'solver': ['liblinear', 'saga'],  # Solvers compatible with the penalties\n",
    "}\n",
    "\n",
    "# Set up StratifiedKFold for cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define Logistic Regression model\n",
    "logreg = LogisticRegression(random_state=42)\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=logreg,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=cv,  # Use StratifiedKFold\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    verbose=2   # Show detailed output\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV to the training data\n",
    "grid_search.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Output the best parameters and accuracy\n",
    "print(f\"\\nBest Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Predict on the test set\n",
    "\n",
    "# Evaluate the best model on the training set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_train_pred = best_model.predict(X_train_tfidf)\n",
    "train_accuracy = accuracy_score(train_label_encoded, y_train_pred)\n",
    "print(f\"Training Accuracy with Best Model: {train_accuracy:.4f}\")\n",
    "\n",
    "# Use the best model to make predictions\n",
    "test_predictions = best_model.predict(X_test_tfidf)\n",
    "\n",
    "# Save predictions to a CSV file\n",
    "output = pd.DataFrame({\"id\": test_id, \"label\": test_predictions})\n",
    "output.to_csv(\"submission_logreg.csv\", index=False)\n",
    "print(\"Predictions saved to submission_logreg.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLgCUP6wyBv4"
   },
   "source": [
    "<h3>1.4 Explanation in Words:</h3><p>\n",
    "    You need to answer the following questions in the markdown cell after this cell:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOYIuEQEyBv4"
   },
   "source": [
    "1.4.1 How did you formulate the learning problem?\n",
    "\n",
    "1.4.2 Which two learning methods from class did you choose and why did you made the choices?\n",
    "\n",
    "1.4.3 How did you do the model selection?\n",
    "\n",
    "1.4.4 Does the test performance reach the first baseline \"Tiny Piney\"? (Please include a screenshot of Kaggle Submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gcvQWV2AyBv4"
   },
   "source": [
    "Responses to 1.4\n",
    "\n",
    "1.4.1 How did you formulate the learning problem?\n",
    "\n",
    "We formulated the problem as a multi-class text classification task, where the goal is to predict one of 28 emotion classes (labeled 0 to 27) based on natural language text input. The training data consisted of labeled text samples, while the test data contained unlabeled samples requiring predictions.\n",
    "\n",
    "The problem was addressed using supervised learning.\n",
    "\n",
    "For preprocessing, we converted all words to lowercase and removed punctuation and potential urls. Converting all text to lowercase standardizes the data, helping reduce the feature space by treating variations of the same word equally. Removing punctuation and URLs simplifies the text, eliminating characters and elements that may not contribute to the semantic meaning. These steps ensure that the model focuses on essential textual elements—words and their contexts—thereby facilitating cleaner, more effective training.\n",
    "\n",
    "For feature extraction, we used the TF-IDF vectorizer to transform text into numerical representations. This technique measures the importance of a word in a document in relation to a collection of documents, thereby enabling us to convert text data into a format that is suitable for machine learning algorithms. TF-IDF helps in distinguishing the relevance of words in large texts, enhancing the model's ability to focus on more significant words for classification.\n",
    "\n",
    "For label encoding, the emotional labels were numerically encoded to facilitate processing by machine learning models. This encoding converts categorical labels into a numerical format that models can interpret and work with during training and predictions.\n",
    "\n",
    "By framing the task within a supervised learning context, we prepared a structured approach to train models on labeled data, enabling them to learn the relationship between the textual features and their corresponding emotional classes. This formulation is good for guiding the subsequent steps of model selection, training, and evaluation.\n",
    "\n",
    "1.4.2 Which two learning methods from class did you choose and why did you made the choices?\n",
    "\n",
    "We chose logistic regression model and SVM model.\n",
    "\n",
    "Logistic Regression Reasoning:\n",
    "We selected Logistic Regression as our learning method because it is an interpretable and efficient algorithm that is well-suited for multi-classification problem. Logistic Regression models the probability of class membership using a linear decision boundary, which works effectively in high-dimensional spaces. This makes it an ideal choice for text data, where the TF-IDF representation often results in a very large feature space.\n",
    "\n",
    "Logistic Regression was chosen because it is straightforward to implement and interpret, making it a good baseline for evaluating more complex models. Additionally, its ability to handle high-dimensional feature spaces makes it highly effective for text classification tasks with large vocabularies. By leveraging Logistic Regression, we could establish a solid foundation for tackling the problem while keeping the model relatively simple and computationally efficient. This choice aligns with the learning methods covered in class, reinforcing the foundational concepts of supervised learning.\n",
    "\n",
    "SVM Reasoning:\n",
    "We chose the SVM model because we wanted to try a model that would be effective for modeling data that is not linearly separable. The SVM model allows you to represent the decision boundary through the use of kernels. By using kernels, SVM can efficiently handle complex relationships between features and learn from non-linear patterns, which is useful for text classification tasks where the relationships between words and classes are complex and less straightforward. Additionally, SVM works well in high-dimensional spaces, which is effective when dealing with text data which typically has a large feature space.Its ability to maximize the margin between classes helps improve generalization, making it less likely to overfit compared to other models. Additionally, SVM performs well on multi-class classification tasks which aligns well with the needs of our text classification problem.\n",
    "\n",
    "\n",
    "\n",
    "1.4.3 How did you do the model selection?\n",
    "\n",
    "To determine the best model for our text classification task, we evaluated both Logistic Regression and Support Vector Machines (SVM) using accuracy score as the performance metric. While the validation accuracy for both models was similar, we ultimately chose Logistic Regression due to its practical advantages in efficiency, scalability, and interpretability, which made it a more suitable choice for our specific task.\n",
    "\n",
    "Logistic Regression is computationally faster to train than SVM, especially for datasets with a large number of samples and features, such as our TF-IDF-transformed text data. This efficiency is important when working with high-dimensional feature spaces, as Logistic Regression solves a simpler optimization problem compared to SVM, which requires computing support vectors and kernel transformations (if used). For large-scale tasks like this, Logistic Regression provides a more practical solution with similar performance.\n",
    "\n",
    "Another key factor is scalability. Logistic Regression can handle high-dimensional datasets effectively without requiring additional complexity, such as kernel functions often needed in SVM to handle non-linear decision boundaries. Additionally, Logistic Regression provides probabilistic outputs, which indicate the likelihood of a sample belonging to each class. This is particularly valuable for multi-class classification, as it offers more nuanced predictions and insight into model confidence. SVM, on the other hand, does not natively produce probabilities and requires additional computation to do so.\n",
    "\n",
    "Finally, Logistic Regression’s interpretability gave it an edge over SVM. The coefficients of the Logistic Regression model correspond directly to the importance of features (e.g., specific words or n-grams) in making classification decisions. This transparency allows us to understand how the model arrives at its predictions and provides actionable insights into the relationships between text features and emotion classes. SVM’s decision boundary is less interpretable, as it depends on the support vectors and kernel transformations.\n",
    "\n",
    "Because of these reasons, we chose Logistic Regression over SVM despite their similar validation performance. Its combination of computational efficiency, scalability, probabilistic outputs, and interpretability made it the more practical and stronger choice for this task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "egW18h2W1QPT"
   },
   "source": [
    "1.4.4 Does the test performance reach the first baseline \"Tiny Piney\"? (Please include a screenshot of Kaggle Submission)\n",
    "\n",
    "Our Kaggle Team Name is Jess and Aileen. We beat \"Tiny Piney\".\n",
    "\n",
    "Logistic Regression Screenshot:  ![Screenshot 2024-12-08 at 7.32.50 PM.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWfh2JaZyBv4"
   },
   "source": [
    "<h2>Part 2: Be creative!</h2><p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JTBC95hCyBv5"
   },
   "source": [
    "<h3>2.1 Open-ended Code:</h3><p>\n",
    "You may follow the steps in part 1 again but making innovative changes like using new training algorithms, etc. Make sure you explain everything clearly in part 2.2. Note that beating \"Zero Hero\" is only a small portion of this part. Any creative ideas will receive most points as long as they are reasonable and clearly explained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "f46WwKtqyBv5"
   },
   "outputs": [],
   "source": [
    "#METHOD: BERT Transformer - NOTE RUN ON JUPYTER NOTEBOOK!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "jFOQXLDpyBv5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (4.47.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: filelock in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from transformers) (0.26.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: requests in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Users/aileenh/opt/anaconda3/bin/python -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "UosJTAz1yBv5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (4.47.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from transformers[torch]) (4.64.0)\n",
      "Requirement already satisfied: filelock in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from transformers[torch]) (3.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from transformers[torch]) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from transformers[torch]) (6.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from transformers[torch]) (0.4.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from transformers[torch]) (0.26.5)\n",
      "Requirement already satisfied: requests in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from transformers[torch]) (2.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from transformers[torch]) (1.21.5)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from transformers[torch]) (0.21.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from transformers[torch]) (2022.3.15)\n",
      "Requirement already satisfied: torch in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from transformers[torch]) (2.2.2)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from transformers[torch]) (0.26.0)\n",
      "Requirement already satisfied: psutil in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from accelerate>=0.26.0->transformers[torch]) (5.8.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers[torch]) (4.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers[torch]) (2024.10.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers[torch]) (3.0.4)\n",
      "Requirement already satisfied: sympy in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from torch->transformers[torch]) (1.10.1)\n",
      "Requirement already satisfied: networkx in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from torch->transformers[torch]) (2.7.1)\n",
      "Requirement already satisfied: jinja2 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from torch->transformers[torch]) (2.11.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers[torch]) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers[torch]) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers[torch]) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers[torch]) (2.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from jinja2->torch->transformers[torch]) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from sympy->torch->transformers[torch]) (1.2.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Users/aileenh/opt/anaconda3/bin/python -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "k9kG-dfLyBv5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate==0.26.0 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (0.26.0)\n",
      "Requirement already satisfied: psutil in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from accelerate==0.26.0) (5.8.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from accelerate==0.26.0) (1.21.5)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from accelerate==0.26.0) (0.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from accelerate==0.26.0) (21.3)\n",
      "Requirement already satisfied: huggingface-hub in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from accelerate==0.26.0) (0.26.5)\n",
      "Requirement already satisfied: torch>=1.10.0 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from accelerate==0.26.0) (2.2.2)\n",
      "Requirement already satisfied: pyyaml in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from accelerate==0.26.0) (6.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->accelerate==0.26.0) (3.0.4)\n",
      "Requirement already satisfied: filelock in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->accelerate==0.26.0) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->accelerate==0.26.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->accelerate==0.26.0) (1.10.1)\n",
      "Requirement already satisfied: networkx in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->accelerate==0.26.0) (2.7.1)\n",
      "Requirement already satisfied: jinja2 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->accelerate==0.26.0) (2.11.3)\n",
      "Requirement already satisfied: fsspec in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from torch>=1.10.0->accelerate==0.26.0) (2024.10.0)\n",
      "Requirement already satisfied: requests in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub->accelerate==0.26.0) (2.27.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub->accelerate==0.26.0) (4.64.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from jinja2->torch>=1.10.0->accelerate==0.26.0) (2.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate==0.26.0) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate==0.26.0) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate==0.26.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from requests->huggingface-hub->accelerate==0.26.0) (3.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from sympy->torch>=1.10.0->accelerate==0.26.0) (1.2.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Users/aileenh/opt/anaconda3/bin/python -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install accelerate==0.26.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "LenWU2CUyBv5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (4.47.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from transformers[torch]) (4.64.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from transformers[torch]) (1.21.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from transformers[torch]) (2022.3.15)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from transformers[torch]) (21.3)\n",
      "Requirement already satisfied: filelock in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from transformers[torch]) (3.6.0)\n",
      "Requirement already satisfied: requests in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from transformers[torch]) (2.27.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from transformers[torch]) (0.26.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from transformers[torch]) (6.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from transformers[torch]) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from transformers[torch]) (0.4.5)\n",
      "Requirement already satisfied: torch in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from transformers[torch]) (2.2.2)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from transformers[torch]) (0.26.0)\n",
      "Requirement already satisfied: psutil in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from accelerate>=0.26.0->transformers[torch]) (5.8.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers[torch]) (4.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers[torch]) (2024.10.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers[torch]) (3.0.4)\n",
      "Requirement already satisfied: sympy in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from torch->transformers[torch]) (1.10.1)\n",
      "Requirement already satisfied: networkx in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from torch->transformers[torch]) (2.7.1)\n",
      "Requirement already satisfied: jinja2 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from torch->transformers[torch]) (2.11.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers[torch]) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers[torch]) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers[torch]) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from requests->transformers[torch]) (2021.10.8)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from jinja2->torch->transformers[torch]) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/aileenh/opt/anaconda3/lib/python3.9/site-packages (from sympy->torch->transformers[torch]) (1.2.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Users/aileenh/opt/anaconda3/bin/python -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install 'transformers[torch]'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297,
     "referenced_widgets": [
      "9ab5d73264b94596aeb6efbf35928ee4",
      "b3af94b3472c4a2984138f2a538de324",
      "5dd7e201a6fd4cc9a14394bc6ce3b02f",
      "738795dbc5e04f2ea52ead0a103db4e0",
      "0e389ef1a2074a4aa375845b5cd06109"
     ]
    },
    "id": "YO6FrkotyBv5",
    "outputId": "953fa13a-6549-4a3e-f2fa-f3afb5a7360f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1689' max='1689' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1689/1689 19:55, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.363800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.360900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.341900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.315000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.282100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.197800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.111200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.971200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.893900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.800200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.606100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.470900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.692400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.294900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.453000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.359300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>2.426400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.273300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>2.209000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.078200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>2.047200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.011500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>1.931600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.945700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.793800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.816200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.744400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.706800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.712900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.524700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>1.503600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.572400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.595300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.392100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.236100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>1.498800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>1.392100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>1.332400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.174500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>1.202800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>1.132500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>1.106600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>1.201800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.236800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>1.206000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.985100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>1.186100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>1.058000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.003400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>1.095300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>1.102000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.993700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.936800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.931300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.942100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.863600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.849300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>1.066900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.876300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>1.136300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.847800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.930600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.829700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.886700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>1.016000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.824600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.900100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.920100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.867300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.846100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.833300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.995700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.955200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.787900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.969300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.924100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.811500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.860300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.802700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.919800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.964800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.933200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.692500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.714400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.804300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.913200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>0.739800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.732100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.803900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>0.737200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.771600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.954500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.843500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>1.012500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.802300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>0.843800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.971000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>0.876600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>0.684100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>0.842000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.728000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.681800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>0.738300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>0.639300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.759400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>0.758700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.874000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>0.678300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.517400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>0.607700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>0.677500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.581400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>0.456200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>0.664500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>0.834900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>0.625900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.429500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>0.558000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>0.642500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>0.650100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>0.642400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.650300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>0.473200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>0.689600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>0.585400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1290</td>\n",
       "      <td>0.628200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.466700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1310</td>\n",
       "      <td>0.550600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>0.744800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1330</td>\n",
       "      <td>0.794700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>0.738600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.698200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>0.548300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1370</td>\n",
       "      <td>0.565200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>0.705000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390</td>\n",
       "      <td>0.735400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.635300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1410</td>\n",
       "      <td>0.484700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1430</td>\n",
       "      <td>0.650300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>0.666300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.579000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>0.543700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1470</td>\n",
       "      <td>0.450700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>0.612100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1490</td>\n",
       "      <td>0.601100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.494400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1510</td>\n",
       "      <td>0.575500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>0.460800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1530</td>\n",
       "      <td>0.553900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>0.647800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.522300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>0.529400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1570</td>\n",
       "      <td>0.674300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580</td>\n",
       "      <td>0.718700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1590</td>\n",
       "      <td>0.533800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.531900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1610</td>\n",
       "      <td>0.590400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1620</td>\n",
       "      <td>0.693900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1630</td>\n",
       "      <td>0.577800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>0.672500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.698900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1660</td>\n",
       "      <td>0.554400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1670</td>\n",
       "      <td>0.693800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>0.517700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ................................C=0.01, solver=saga; total time=   0.8s\n",
      "[CV] END .................................C=0.1, solver=saga; total time=   0.7s\n",
      "[CV] END ..............................C=1, solver=liblinear; total time=   0.8s\n",
      "[CV] END .............................C=10, solver=liblinear; total time=   1.2s\n",
      "[CV] END ..................................C=10, solver=saga; total time=   3.7s\n",
      "[CV] END ...........................C=0.01, solver=liblinear; total time=   0.5s\n",
      "[CV] END ............................C=0.1, solver=liblinear; total time=   0.5s\n",
      "[CV] END .................................C=0.1, solver=saga; total time=   0.7s\n",
      "[CV] END ...................................C=1, solver=saga; total time=   0.8s\n",
      "[CV] END .............................C=10, solver=liblinear; total time=   1.2s\n",
      "[CV] END ............................C=100, solver=liblinear; total time=   1.8s\n",
      "[CV] END ............................C=100, solver=liblinear; total time=   1.8s\n",
      "[CV] END ...........................C=0.01, solver=liblinear; total time=   0.5s\n",
      "[CV] END ............................C=0.1, solver=liblinear; total time=   0.6s\n",
      "[CV] END .................................C=0.1, solver=saga; total time=   0.7s\n",
      "[CV] END ..............................C=1, solver=liblinear; total time=   0.8s\n",
      "[CV] END .............................C=10, solver=liblinear; total time=   1.2s\n",
      "[CV] END ............................C=100, solver=liblinear; total time=   1.7s\n",
      "[CV] END ............................C=100, solver=liblinear; total time=   1.9s\n",
      "[CV] END ...........................C=0.01, solver=liblinear; total time=   0.5s\n",
      "[CV] END ............................C=0.1, solver=liblinear; total time=   0.6s\n",
      "[CV] END .................................C=0.1, solver=saga; total time=   0.7s\n",
      "[CV] END ...................................C=1, solver=saga; total time=   0.8s\n",
      "[CV] END .............................C=10, solver=liblinear; total time=   1.2s\n",
      "[CV] END ............................C=100, solver=liblinear; total time=   1.8s\n",
      "[CV] END .................................C=100, solver=saga; total time=   3.3s\n",
      "[CV] END ...........................C=0.01, solver=liblinear; total time=   0.5s\n",
      "[CV] END ................................C=0.01, solver=saga; total time=   0.8s\n",
      "[CV] END ..............................C=1, solver=liblinear; total time=   0.8s\n",
      "[CV] END ...................................C=1, solver=saga; total time=   0.7s\n",
      "[CV] END ..................................C=10, solver=saga; total time=   3.1s\n",
      "[CV] END .................................C=100, solver=saga; total time=   3.1s\n",
      "[CV] END ...........................C=0.01, solver=liblinear; total time=   0.5s\n",
      "[CV] END ................................C=0.01, solver=saga; total time=   0.7s\n",
      "[CV] END .................................C=0.1, solver=saga; total time=   0.7s\n",
      "[CV] END ...................................C=1, solver=saga; total time=   0.8s\n",
      "[CV] END ..................................C=10, solver=saga; total time=   3.5s\n",
      "[CV] END .................................C=100, solver=saga; total time=   3.0s\n",
      "[CV] END ................................C=0.01, solver=saga; total time=   0.7s\n",
      "[CV] END ............................C=0.1, solver=liblinear; total time=   0.5s\n",
      "[CV] END ..............................C=1, solver=liblinear; total time=   0.8s\n",
      "[CV] END ...................................C=1, solver=saga; total time=   0.7s\n",
      "[CV] END ..................................C=10, solver=saga; total time=   3.5s\n",
      "[CV] END .................................C=100, solver=saga; total time=   3.0s\n",
      "[CV] END ................................C=0.01, solver=saga; total time=   0.8s\n",
      "[CV] END ............................C=0.1, solver=liblinear; total time=   0.5s\n",
      "[CV] END ..............................C=1, solver=liblinear; total time=   0.7s\n",
      "[CV] END .............................C=10, solver=liblinear; total time=   1.2s\n",
      "[CV] END ..................................C=10, solver=saga; total time=   3.7s\n",
      "[CV] END .................................C=100, solver=saga; total time=   2.5s\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.8573\n",
      "Validation Accuracy: 0.7730\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('train.csv')\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(data['text'], data['label'], test_size=0.1)\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True, max_length=128)\n",
    "val_encodings = tokenizer(val_texts.tolist(), truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Dataset class\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = EmotionDataset(train_encodings, train_labels.tolist())\n",
    "val_dataset = EmotionDataset(val_encodings, val_labels.tolist())\n",
    "\n",
    "# Model initialization\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=28)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Define metric computation\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'accuracy': acc}\n",
    "\n",
    "# Trainer - Built in trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate and print accuracy for both training and validation datasets\n",
    "train_results = trainer.evaluate(train_dataset)\n",
    "val_results = trainer.evaluate(val_dataset)\n",
    "print(f\"Training Accuracy: {train_results['eval_accuracy']:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_results['eval_accuracy']:.4f}\")\n",
    "\n",
    "# Load the test dataset for final predictions\n",
    "test_data = pd.read_csv('test.csv')\n",
    "test_texts = test_data['text'].tolist()\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128)\n",
    "test_dataset = EmotionDataset(test_encodings, [0]*len(test_texts))  # Dummy labels\n",
    "\n",
    "# Predict on test data\n",
    "predictions = trainer.predict(test_dataset)\n",
    "predicted_labels = predictions.predictions.argmax(-1)\n",
    "\n",
    "# Saving predictions to CSV for submission\n",
    "submission = pd.DataFrame({'id': range(len(predicted_labels)), 'label': predicted_labels})\n",
    "submission.to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qr7yswqtyBv6"
   },
   "source": [
    "<h3>2.2 Explanation in Words:</h3><p>\n",
    "You need to answer the following questions in a markdown cell after this cell:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S2PwiqyqyBv6"
   },
   "source": [
    "2.2.1 How much did you manage to improve performance on the test set? Did you beat \"Zero Hero\" in Kaggle? (Please include a screenshot of Kaggle Submission)\n",
    "\n",
    "2.2.2 Please explain in detail how you achieved this and what you did specifically and why you tried this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRJzKSbmyBv6"
   },
   "source": [
    "Responses to cell above:  \n",
    "\n",
    "2.2.1: We did improve performance on the test set by around 0.6% compared to logistic regression, and beat Zero Hero on Kaggle (screenshot below).\n",
    "    \n",
    "2.2.2:\n",
    "\n",
    "Reference to articles that influenced our choice to use DistilledBERT and Trainer.\n",
    "\n",
    "https://medium.com/@kiddojazz/distilbert-for-multiclass-text-classification-using-transformers-d6374e6678ba\n",
    "\n",
    "https://www.sunnyville.ai/fine-tuning-distilbert-multi-class-text-classification-using-transformers-and-tensorflow/\n",
    "\n",
    "To achieve an improvement in performance on the test set using a BERT-based approach, we transitioned from the baseline logistic regression model (our best result), which initially provided us with an accuracy of 70.43%. The primary rationale behind this switch was BERT's ability to understand and leverage the contextual nuances of language, which is essential in emotion classification tasks.\n",
    "\n",
    "Implementation Details and Strategy:\n",
    "DistilBERT leverages the full textual context, including punctuation and capitalization, which can convey meaningful signals about sentence boundaries, emotions, or emphases (like exclamation marks for excitement), so unlike our approach for Part 1, we decided to keep punctuation and capitalization. Removing these elements might strip away subtle contextual clues that the model can use to make more informed predictions.\n",
    "\n",
    "We chose DistilBERT for our model due to its balance between efficiency and effectiveness. DistilBERT is a streamlined version of the more cumbersome BERT model that retains most of the original model's strengths but is more resource-efficient. This choice was ideal given our constraints on training time and computational resources (HuggingFace source).\n",
    "\n",
    "For tokenization, we employed DistilBertTokenizerFast, which adapts the text for BERT’s requirements, adding necessary special tokens and managing sentence length through padding and truncation. The model was fine-tuned using the Hugging Face’s Trainer API, which simplified the training process and allowed for easy integration of training arguments like the number of epochs, batch size, and learning rate adjustments ((Sunnyville Article)).\n",
    "\n",
    "Fine-Tuning Process (Sunnyville Article): We utilized the Hugging Face’s Trainer API for fine-tuning DistilBERT, which streamlined the training and evaluation process. The Trainer API is particularly advantageous because it abstracts away much of the boilerplate code associated with training loops (as used in past class projects), allowing us to focus on tuning the model’s performance.\n",
    "\n",
    "Tokenization (Temidayo Omoniyi - Medium Article): Using DistilBertTokenizerFast, we ensured that our input data was correctly formatted for the model, including the addition of special tokens CLS and SEP, necessary for BERT architectures to function correctly.               \n",
    "\n",
    "Hyperparameter Adjustments: In our implementation, we carefully fine-tuned several key hyperparameters to optimize the performance of the DistilBERT model for our specific task. We set the number of training epochs to three. This choice was aimed at providing the model with sufficient exposure to the training data while minimizing the risk of overfitting. Overfitting could diminish the model's ability to generalize well to unseen data.\n",
    "\n",
    "Additionally, we selected an appropriate batch size that balances between efficient memory usage and the granularity of the gradient updates during the training process. A well-chosen batch size ensures that the model updates are smooth and stable, contributing to a more robust learning trajectory.\n",
    "\n",
    "We also included warm-up steps in our training configuration. These steps are designed to gradually increase the learning rate from a lower starting point at the beginning of training. This approach helps in stabilizing the training process by preventing large, destabilizing updates to the model's pre-trained weights, which can be crucial for leveraging the already learned features while allowing the model to adapt effectively to the nuances of our new data.\n",
    "\n",
    "Evaluation and Results:\n",
    "The compute_metrics function was defined to calculate the accuracy of the model, ensuring we could quantitatively measure its performance against our logistic regression baseline. After training, we conducted evaluations on both a training set and a validation set to confirm the model's ability to learn effectively and generalize well beyond the training data.\n",
    "\n",
    "The final step involved making predictions on the test dataset, which had been similarly preprocessed and tokenized. These predictions were formatted according to Kaggle's submission requirements and uploaded to the platform. Our submission achieved around a 0.6% improvement over our best baseline model, outperforming the \"Zero Hero\" benchmark on Kaggle. This improvement was a direct result of leveraging BERT's advanced capabilities to interpret the complex and varied semantic structures within the text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Sh_PEUv00iL"
   },
   "source": [
    "Screenshot of beating Zero Hero:![Screenshot 2024-12-08 at 7.31.26 PM.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffJWTCBVyBv6"
   },
   "source": [
    "<h2>Part 3: Kaggle Submission</h2><p>\n",
    "You need to generate a prediction CSV using the following cell from your trained model and submit the direct output of your code to Kaggle. The results should be presented in two columns in csv format: the first column is the data id (0-14999) and the second column includes the predictions for the test set. The first column must be named id and the second column must be named label (otherwise your submission will fail). A sample predication file can be downloaded from Kaggle for each problem.\n",
    "We provide how to save a csv file if you are running Notebook on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "3CynWX_qyBv6"
   },
   "outputs": [],
   "source": [
    "id = range(15000)\n",
    "prediction = range(15000)\n",
    "submission = pd.DataFrame({'id': id, 'label': prediction})\n",
    "submission.to_csv('/kaggle/working/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j7_NFmGtyBv7"
   },
   "outputs": [],
   "source": [
    "# We created submission csv's in our code above! And have submitted these on Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gb9TtLSKyBv7"
   },
   "source": [
    "<h2>Part 4: Resources and Literature Used</h2><p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rQ9cNazayBv7"
   },
   "source": [
    "Please cite the papers and open resources you used.\n",
    "\n",
    "Part 1: Two Learning Algorithms Learned from Class\n",
    "\n",
    "Lecture 5 from class on k-fold cross validation,  Lecture 8 on SVM, and Lecture 10 on Regularized Linear Models\n",
    "\n",
    "Learned TD-IDF Preprocessing from course CS 4300 last spring! https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html, https://scikit-learn.org/dev/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
    "\n",
    "Sklearn library (Link to GridSearchCV and CrossValidation): https://scikit-learn.org/stable/\n",
    "\n",
    "https://scikit-learn.org/1.5/modules/generated/sklearn.model_selection.KFold.html\n",
    "\n",
    "LogReg on Hyperparameters and Multi-classification: https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "SVM: https://scikit-learn.org/1.5/modules/generated/sklearn.svm.SVC.html\n",
    "\n",
    "Part 2:\n",
    "\n",
    "Lecture 15 on attention and transformers\n",
    "\n",
    "GenAI citation for create Emotion Dataset for Preprocessing BERT: https://chatgpt.com/share/67563f8b-813c-8012-ad17-27277bbf06c9\n",
    "\n",
    "Info on DistilBERT:\n",
    "\n",
    "https://huggingface.co/docs/transformers/en/model_doc/distilbert\n",
    "\n",
    "\n",
    "https://medium.com/@kiddojazz/distilbert-for-multiclass-text-classification-using-transformers-d6374e6678ba\n",
    "\n",
    "https://www.sunnyville.ai/fine-tuning-distilbert-multi-class-text-classification-using-transformers-and-tensorflow/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVdUNLe8yBv7"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
