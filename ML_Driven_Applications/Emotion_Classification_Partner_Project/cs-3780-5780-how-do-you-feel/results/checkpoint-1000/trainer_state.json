{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.7761989342806395,
  "eval_steps": 500,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.017761989342806393,
      "grad_norm": 2.8358917236328125,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 3.3638,
      "step": 10
    },
    {
      "epoch": 0.035523978685612786,
      "grad_norm": 2.785738229751587,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 3.3609,
      "step": 20
    },
    {
      "epoch": 0.05328596802841918,
      "grad_norm": 2.3931219577789307,
      "learning_rate": 3e-06,
      "loss": 3.3419,
      "step": 30
    },
    {
      "epoch": 0.07104795737122557,
      "grad_norm": 2.4643747806549072,
      "learning_rate": 4.000000000000001e-06,
      "loss": 3.315,
      "step": 40
    },
    {
      "epoch": 0.08880994671403197,
      "grad_norm": 2.4723434448242188,
      "learning_rate": 5e-06,
      "loss": 3.2821,
      "step": 50
    },
    {
      "epoch": 0.10657193605683836,
      "grad_norm": 3.6589529514312744,
      "learning_rate": 6e-06,
      "loss": 3.1978,
      "step": 60
    },
    {
      "epoch": 0.12433392539964476,
      "grad_norm": 3.1119384765625,
      "learning_rate": 7.000000000000001e-06,
      "loss": 3.1112,
      "step": 70
    },
    {
      "epoch": 0.14209591474245115,
      "grad_norm": 3.358727216720581,
      "learning_rate": 8.000000000000001e-06,
      "loss": 2.9712,
      "step": 80
    },
    {
      "epoch": 0.15985790408525755,
      "grad_norm": 3.065464735031128,
      "learning_rate": 9e-06,
      "loss": 2.8939,
      "step": 90
    },
    {
      "epoch": 0.17761989342806395,
      "grad_norm": 3.1591718196868896,
      "learning_rate": 1e-05,
      "loss": 2.8002,
      "step": 100
    },
    {
      "epoch": 0.19538188277087035,
      "grad_norm": 4.008114337921143,
      "learning_rate": 1.1000000000000001e-05,
      "loss": 2.6061,
      "step": 110
    },
    {
      "epoch": 0.21314387211367672,
      "grad_norm": 3.473119020462036,
      "learning_rate": 1.2e-05,
      "loss": 2.4709,
      "step": 120
    },
    {
      "epoch": 0.23090586145648312,
      "grad_norm": 3.7786190509796143,
      "learning_rate": 1.3000000000000001e-05,
      "loss": 2.6924,
      "step": 130
    },
    {
      "epoch": 0.24866785079928952,
      "grad_norm": 3.9858078956604004,
      "learning_rate": 1.4000000000000001e-05,
      "loss": 2.2949,
      "step": 140
    },
    {
      "epoch": 0.2664298401420959,
      "grad_norm": 3.2470765113830566,
      "learning_rate": 1.5e-05,
      "loss": 2.453,
      "step": 150
    },
    {
      "epoch": 0.2841918294849023,
      "grad_norm": 5.571349620819092,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 2.3593,
      "step": 160
    },
    {
      "epoch": 0.3019538188277087,
      "grad_norm": 4.5926713943481445,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 2.4264,
      "step": 170
    },
    {
      "epoch": 0.3197158081705151,
      "grad_norm": 3.3515923023223877,
      "learning_rate": 1.8e-05,
      "loss": 2.2733,
      "step": 180
    },
    {
      "epoch": 0.33747779751332146,
      "grad_norm": 6.012602806091309,
      "learning_rate": 1.9e-05,
      "loss": 2.209,
      "step": 190
    },
    {
      "epoch": 0.3552397868561279,
      "grad_norm": 5.898227214813232,
      "learning_rate": 2e-05,
      "loss": 2.0782,
      "step": 200
    },
    {
      "epoch": 0.37300177619893427,
      "grad_norm": 4.002500057220459,
      "learning_rate": 2.1e-05,
      "loss": 2.0472,
      "step": 210
    },
    {
      "epoch": 0.3907637655417407,
      "grad_norm": 4.66304874420166,
      "learning_rate": 2.2000000000000003e-05,
      "loss": 2.0115,
      "step": 220
    },
    {
      "epoch": 0.40852575488454707,
      "grad_norm": 5.881225109100342,
      "learning_rate": 2.3000000000000003e-05,
      "loss": 1.9316,
      "step": 230
    },
    {
      "epoch": 0.42628774422735344,
      "grad_norm": 8.163241386413574,
      "learning_rate": 2.4e-05,
      "loss": 1.9457,
      "step": 240
    },
    {
      "epoch": 0.44404973357015987,
      "grad_norm": 6.864853382110596,
      "learning_rate": 2.5e-05,
      "loss": 1.7938,
      "step": 250
    },
    {
      "epoch": 0.46181172291296624,
      "grad_norm": 6.02701473236084,
      "learning_rate": 2.6000000000000002e-05,
      "loss": 1.8162,
      "step": 260
    },
    {
      "epoch": 0.47957371225577267,
      "grad_norm": 5.542759895324707,
      "learning_rate": 2.7000000000000002e-05,
      "loss": 1.7444,
      "step": 270
    },
    {
      "epoch": 0.49733570159857904,
      "grad_norm": 6.277385711669922,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 1.7068,
      "step": 280
    },
    {
      "epoch": 0.5150976909413855,
      "grad_norm": 6.992009162902832,
      "learning_rate": 2.9e-05,
      "loss": 1.7129,
      "step": 290
    },
    {
      "epoch": 0.5328596802841918,
      "grad_norm": 6.30530309677124,
      "learning_rate": 3e-05,
      "loss": 1.5247,
      "step": 300
    },
    {
      "epoch": 0.5506216696269982,
      "grad_norm": 8.87588119506836,
      "learning_rate": 3.1e-05,
      "loss": 1.5036,
      "step": 310
    },
    {
      "epoch": 0.5683836589698046,
      "grad_norm": 11.885651588439941,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 1.5724,
      "step": 320
    },
    {
      "epoch": 0.5861456483126111,
      "grad_norm": 4.405028343200684,
      "learning_rate": 3.3e-05,
      "loss": 1.5953,
      "step": 330
    },
    {
      "epoch": 0.6039076376554174,
      "grad_norm": 4.797493934631348,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 1.55,
      "step": 340
    },
    {
      "epoch": 0.6216696269982238,
      "grad_norm": 6.567544460296631,
      "learning_rate": 3.5e-05,
      "loss": 1.3921,
      "step": 350
    },
    {
      "epoch": 0.6394316163410302,
      "grad_norm": 4.617964267730713,
      "learning_rate": 3.6e-05,
      "loss": 1.2361,
      "step": 360
    },
    {
      "epoch": 0.6571936056838366,
      "grad_norm": 10.678743362426758,
      "learning_rate": 3.7e-05,
      "loss": 1.4988,
      "step": 370
    },
    {
      "epoch": 0.6749555950266429,
      "grad_norm": 9.952666282653809,
      "learning_rate": 3.8e-05,
      "loss": 1.3921,
      "step": 380
    },
    {
      "epoch": 0.6927175843694494,
      "grad_norm": 8.60153579711914,
      "learning_rate": 3.9000000000000006e-05,
      "loss": 1.3324,
      "step": 390
    },
    {
      "epoch": 0.7104795737122558,
      "grad_norm": 6.027344703674316,
      "learning_rate": 4e-05,
      "loss": 1.1745,
      "step": 400
    },
    {
      "epoch": 0.7282415630550622,
      "grad_norm": 16.72760581970215,
      "learning_rate": 4.1e-05,
      "loss": 1.2028,
      "step": 410
    },
    {
      "epoch": 0.7460035523978685,
      "grad_norm": 7.116293907165527,
      "learning_rate": 4.2e-05,
      "loss": 1.1325,
      "step": 420
    },
    {
      "epoch": 0.7637655417406749,
      "grad_norm": 6.3977532386779785,
      "learning_rate": 4.3e-05,
      "loss": 1.1066,
      "step": 430
    },
    {
      "epoch": 0.7815275310834814,
      "grad_norm": 6.7352447509765625,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 1.2018,
      "step": 440
    },
    {
      "epoch": 0.7992895204262878,
      "grad_norm": 10.674076080322266,
      "learning_rate": 4.5e-05,
      "loss": 1.2368,
      "step": 450
    },
    {
      "epoch": 0.8170515097690941,
      "grad_norm": 10.352594375610352,
      "learning_rate": 4.600000000000001e-05,
      "loss": 1.206,
      "step": 460
    },
    {
      "epoch": 0.8348134991119005,
      "grad_norm": 7.1709699630737305,
      "learning_rate": 4.7e-05,
      "loss": 0.9851,
      "step": 470
    },
    {
      "epoch": 0.8525754884547069,
      "grad_norm": 5.184566497802734,
      "learning_rate": 4.8e-05,
      "loss": 1.1861,
      "step": 480
    },
    {
      "epoch": 0.8703374777975134,
      "grad_norm": 5.512131690979004,
      "learning_rate": 4.9e-05,
      "loss": 1.058,
      "step": 490
    },
    {
      "epoch": 0.8880994671403197,
      "grad_norm": 6.4800124168396,
      "learning_rate": 5e-05,
      "loss": 1.0034,
      "step": 500
    },
    {
      "epoch": 0.9058614564831261,
      "grad_norm": 4.833974361419678,
      "learning_rate": 4.957947855340623e-05,
      "loss": 1.0953,
      "step": 510
    },
    {
      "epoch": 0.9236234458259325,
      "grad_norm": 5.160473823547363,
      "learning_rate": 4.915895710681245e-05,
      "loss": 1.102,
      "step": 520
    },
    {
      "epoch": 0.9413854351687388,
      "grad_norm": 5.20122766494751,
      "learning_rate": 4.8738435660218675e-05,
      "loss": 0.9937,
      "step": 530
    },
    {
      "epoch": 0.9591474245115453,
      "grad_norm": 5.10511589050293,
      "learning_rate": 4.83179142136249e-05,
      "loss": 0.9368,
      "step": 540
    },
    {
      "epoch": 0.9769094138543517,
      "grad_norm": 12.348362922668457,
      "learning_rate": 4.789739276703112e-05,
      "loss": 0.9313,
      "step": 550
    },
    {
      "epoch": 0.9946714031971581,
      "grad_norm": 3.259582996368408,
      "learning_rate": 4.747687132043734e-05,
      "loss": 0.9421,
      "step": 560
    },
    {
      "epoch": 1.0124333925399644,
      "grad_norm": 14.23579216003418,
      "learning_rate": 4.705634987384357e-05,
      "loss": 0.8636,
      "step": 570
    },
    {
      "epoch": 1.030195381882771,
      "grad_norm": 12.13515567779541,
      "learning_rate": 4.66358284272498e-05,
      "loss": 0.8493,
      "step": 580
    },
    {
      "epoch": 1.0479573712255772,
      "grad_norm": 5.559974193572998,
      "learning_rate": 4.6215306980656014e-05,
      "loss": 1.0669,
      "step": 590
    },
    {
      "epoch": 1.0657193605683837,
      "grad_norm": 7.030978679656982,
      "learning_rate": 4.579478553406224e-05,
      "loss": 0.8125,
      "step": 600
    },
    {
      "epoch": 1.0834813499111902,
      "grad_norm": 3.144197702407837,
      "learning_rate": 4.537426408746846e-05,
      "loss": 0.8763,
      "step": 610
    },
    {
      "epoch": 1.1012433392539964,
      "grad_norm": 8.019453048706055,
      "learning_rate": 4.495374264087469e-05,
      "loss": 1.1363,
      "step": 620
    },
    {
      "epoch": 1.119005328596803,
      "grad_norm": 25.932207107543945,
      "learning_rate": 4.453322119428091e-05,
      "loss": 0.8478,
      "step": 630
    },
    {
      "epoch": 1.1367673179396092,
      "grad_norm": 6.305386066436768,
      "learning_rate": 4.4112699747687136e-05,
      "loss": 0.9306,
      "step": 640
    },
    {
      "epoch": 1.1545293072824157,
      "grad_norm": 14.159951210021973,
      "learning_rate": 4.369217830109335e-05,
      "loss": 0.8297,
      "step": 650
    },
    {
      "epoch": 1.1722912966252221,
      "grad_norm": 11.460890769958496,
      "learning_rate": 4.3271656854499584e-05,
      "loss": 0.8867,
      "step": 660
    },
    {
      "epoch": 1.1900532859680284,
      "grad_norm": 5.411353588104248,
      "learning_rate": 4.285113540790581e-05,
      "loss": 1.016,
      "step": 670
    },
    {
      "epoch": 1.2078152753108349,
      "grad_norm": 4.966886043548584,
      "learning_rate": 4.2430613961312026e-05,
      "loss": 0.8246,
      "step": 680
    },
    {
      "epoch": 1.2255772646536411,
      "grad_norm": 11.931076049804688,
      "learning_rate": 4.201009251471825e-05,
      "loss": 0.9001,
      "step": 690
    },
    {
      "epoch": 1.2433392539964476,
      "grad_norm": 8.22316837310791,
      "learning_rate": 4.1589571068124474e-05,
      "loss": 0.9201,
      "step": 700
    },
    {
      "epoch": 1.261101243339254,
      "grad_norm": 4.599932670593262,
      "learning_rate": 4.11690496215307e-05,
      "loss": 0.8673,
      "step": 710
    },
    {
      "epoch": 1.2788632326820604,
      "grad_norm": 6.584653377532959,
      "learning_rate": 4.074852817493692e-05,
      "loss": 0.8461,
      "step": 720
    },
    {
      "epoch": 1.2966252220248669,
      "grad_norm": 4.00999116897583,
      "learning_rate": 4.032800672834315e-05,
      "loss": 0.8333,
      "step": 730
    },
    {
      "epoch": 1.3143872113676731,
      "grad_norm": 4.675708293914795,
      "learning_rate": 3.990748528174937e-05,
      "loss": 0.9957,
      "step": 740
    },
    {
      "epoch": 1.3321492007104796,
      "grad_norm": 28.962923049926758,
      "learning_rate": 3.9486963835155596e-05,
      "loss": 0.9552,
      "step": 750
    },
    {
      "epoch": 1.349911190053286,
      "grad_norm": 6.800754547119141,
      "learning_rate": 3.906644238856182e-05,
      "loss": 0.7879,
      "step": 760
    },
    {
      "epoch": 1.3676731793960923,
      "grad_norm": 14.251548767089844,
      "learning_rate": 3.8645920941968044e-05,
      "loss": 0.9693,
      "step": 770
    },
    {
      "epoch": 1.3854351687388988,
      "grad_norm": 12.466997146606445,
      "learning_rate": 3.822539949537426e-05,
      "loss": 0.9241,
      "step": 780
    },
    {
      "epoch": 1.403197158081705,
      "grad_norm": 4.944668769836426,
      "learning_rate": 3.780487804878049e-05,
      "loss": 0.8115,
      "step": 790
    },
    {
      "epoch": 1.4209591474245116,
      "grad_norm": 10.059270858764648,
      "learning_rate": 3.738435660218672e-05,
      "loss": 0.8603,
      "step": 800
    },
    {
      "epoch": 1.438721136767318,
      "grad_norm": 3.3789262771606445,
      "learning_rate": 3.6963835155592935e-05,
      "loss": 0.8027,
      "step": 810
    },
    {
      "epoch": 1.4564831261101243,
      "grad_norm": 6.536566734313965,
      "learning_rate": 3.654331370899916e-05,
      "loss": 0.9198,
      "step": 820
    },
    {
      "epoch": 1.4742451154529308,
      "grad_norm": 8.571167945861816,
      "learning_rate": 3.612279226240538e-05,
      "loss": 0.9648,
      "step": 830
    },
    {
      "epoch": 1.492007104795737,
      "grad_norm": 6.115657806396484,
      "learning_rate": 3.570227081581161e-05,
      "loss": 0.9332,
      "step": 840
    },
    {
      "epoch": 1.5097690941385435,
      "grad_norm": 4.854413032531738,
      "learning_rate": 3.528174936921783e-05,
      "loss": 0.6925,
      "step": 850
    },
    {
      "epoch": 1.52753108348135,
      "grad_norm": 9.667940139770508,
      "learning_rate": 3.4861227922624056e-05,
      "loss": 0.7144,
      "step": 860
    },
    {
      "epoch": 1.5452930728241563,
      "grad_norm": 5.266278266906738,
      "learning_rate": 3.4440706476030274e-05,
      "loss": 0.8043,
      "step": 870
    },
    {
      "epoch": 1.5630550621669625,
      "grad_norm": 6.3296427726745605,
      "learning_rate": 3.4020185029436505e-05,
      "loss": 0.9132,
      "step": 880
    },
    {
      "epoch": 1.580817051509769,
      "grad_norm": 7.131582736968994,
      "learning_rate": 3.359966358284273e-05,
      "loss": 0.7398,
      "step": 890
    },
    {
      "epoch": 1.5985790408525755,
      "grad_norm": 5.82141637802124,
      "learning_rate": 3.317914213624895e-05,
      "loss": 0.7321,
      "step": 900
    },
    {
      "epoch": 1.616341030195382,
      "grad_norm": 8.882863998413086,
      "learning_rate": 3.275862068965517e-05,
      "loss": 0.6,
      "step": 910
    },
    {
      "epoch": 1.6341030195381883,
      "grad_norm": 3.478675127029419,
      "learning_rate": 3.2338099243061395e-05,
      "loss": 0.8039,
      "step": 920
    },
    {
      "epoch": 1.6518650088809945,
      "grad_norm": 6.948004722595215,
      "learning_rate": 3.1917577796467626e-05,
      "loss": 0.7372,
      "step": 930
    },
    {
      "epoch": 1.669626998223801,
      "grad_norm": 4.096661567687988,
      "learning_rate": 3.1497056349873844e-05,
      "loss": 0.7716,
      "step": 940
    },
    {
      "epoch": 1.6873889875666075,
      "grad_norm": 8.930024147033691,
      "learning_rate": 3.107653490328007e-05,
      "loss": 0.9545,
      "step": 950
    },
    {
      "epoch": 1.705150976909414,
      "grad_norm": 7.3193817138671875,
      "learning_rate": 3.065601345668629e-05,
      "loss": 0.8435,
      "step": 960
    },
    {
      "epoch": 1.7229129662522202,
      "grad_norm": 5.765203952789307,
      "learning_rate": 3.0235492010092513e-05,
      "loss": 1.0125,
      "step": 970
    },
    {
      "epoch": 1.7406749555950265,
      "grad_norm": 20.384252548217773,
      "learning_rate": 2.981497056349874e-05,
      "loss": 0.8023,
      "step": 980
    },
    {
      "epoch": 1.758436944937833,
      "grad_norm": 6.549558639526367,
      "learning_rate": 2.9394449116904965e-05,
      "loss": 0.8438,
      "step": 990
    },
    {
      "epoch": 1.7761989342806395,
      "grad_norm": 9.565075874328613,
      "learning_rate": 2.8973927670311186e-05,
      "loss": 0.971,
      "step": 1000
    }
  ],
  "logging_steps": 10,
  "max_steps": 1689,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 285622385973696.0,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
